{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5952fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all\n",
    "from torch import nn\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from collections import deque \n",
    "import itertools \n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f892ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values to be used \n",
    "\n",
    "gamma                   = 0.99   # discount rate \n",
    "batch_size              = 32\n",
    "buffer_size             = 50000\n",
    "replay_size             = 1000\n",
    "epsilon_start           = 1.0\n",
    "epsilon_end             = 0.02\n",
    "epsilon_decay           = 10000\n",
    "target_update_frequency = 1000\n",
    "learning_rate           = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b975766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment \n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2fc94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen = buffer_size)\n",
    "reward_buffer = deque([0.0], maxlen = 100)\n",
    "\n",
    "curr_reward   = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef990b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network class \n",
    "# nn.Sequential creates the required hidden layers in the order provided \n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self,env):\n",
    "        super().__init__()\n",
    "        \n",
    "        input_features = int(np.prod(env.observation_space.shape))\n",
    "        self.net = nn.Sequential( nn.Linear(input_features , 64) \n",
    "                                 , nn.Tanh() \n",
    "                                 , nn.Linear( 64 , env.action_space.n))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def act(self,curr_state):\n",
    "        \n",
    "        #convert the current state to a pytorch tensor \n",
    "        curr_state_t = torch.as_tensor(curr_state , dtype = torch.float32)\n",
    "        q_values     = self(curr_state_t.unsqueeze(0))\n",
    "        \n",
    "        max_q        = torch.argmax(q_values , dim=1)[0]\n",
    "        action       = max_q.detach().item()\n",
    "        \n",
    "        return action \n",
    "    \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b6054e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create networks \n",
    "\n",
    "online_network = Network(env)\n",
    "target_network = Network(env)\n",
    "\n",
    "# copy one network to another \n",
    "# load_state_dict is used to copy one model parameters to another \n",
    "\n",
    "target_network.load_state_dict(online_network.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(online_network.parameters() , lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73ae8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the replay buffer \n",
    "\n",
    "curr_state , info = env.reset()\n",
    "\n",
    "for _ in range(replay_size):\n",
    "    \n",
    "    # take a random action\n",
    "    action = env.action_space.sample()\n",
    "    new_state , reward , terminated , truncated, info = env.step(action)\n",
    "    \n",
    "    # collect the experiences\n",
    "    experience_tuple = (curr_state , action , reward , terminated ,truncated, new_state)\n",
    "    replay_buffer.append(experience_tuple)\n",
    "    \n",
    "    curr_state = new_state \n",
    "    \n",
    "    if terminated or truncated :\n",
    "        curr_state , info = env.reset()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a42ef35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps 0\n",
      "Average Reward 0.0\n",
      "\n",
      "Steps 1000\n",
      "Average Reward 19.86\n",
      "\n",
      "Steps 2000\n",
      "Average Reward 21.322580645161292\n",
      "\n",
      "Steps 3000\n",
      "Average Reward 26.58\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m error \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msmooth_l1_loss(action_q_values , targets)  \u001b[38;5;66;03m# Huber loss\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Gradient Descent \u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m error\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     80\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:803\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    801\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_profile_name):\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    805\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/profiler.py:631\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 631\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_ops.py:692\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the agent \n",
    "\n",
    "curr_state , info = env.reset()\n",
    "\n",
    "for i in itertools.count():\n",
    "    \n",
    "    # Epsilon Greedy Strategy to explore \n",
    "    epsilon = np.interp( i , [0,epsilon_decay] ,[ epsilon_start,epsilon_end ] )\n",
    "    random_value = random.random()\n",
    "    \n",
    "    if random_value <= epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else :\n",
    "        action = online_network.act(curr_state)\n",
    "    \n",
    "    new_state , reward , terminated , truncated , info = env.step(action)\n",
    "    \n",
    "    # collect the experiences\n",
    "    experience_tuple = (curr_state , action , reward , terminated , truncated , new_state)\n",
    "    replay_buffer.append(experience_tuple)\n",
    "    \n",
    "    curr_state = new_state \n",
    "    curr_reward += reward \n",
    "    \n",
    "    if terminated or truncated :\n",
    "        curr_state , info = env.reset()\n",
    "        \n",
    "        reward_buffer.append(curr_reward)\n",
    "        curr_reward = 0.0\n",
    "    \n",
    "    # For rendering the code , beware of decommenting as your kernel can crash\n",
    "#     if len(reward_buffer) >=100:\n",
    "#         if np.mean(reward_buffer)>=200:\n",
    "#             while True :\n",
    "#                 action = online_network.act(curr_state)\n",
    "                \n",
    "#                 curr_state , reward , terminated , truncated , info = env.step(action)\n",
    "#                 env.render()\n",
    "                \n",
    "#                 if terminated or truncated:\n",
    "#                     env.reset\n",
    "    \n",
    "    \n",
    "    # Extract parameters for Gradient Descent \n",
    "    experiences  = random.sample( replay_buffer , batch_size )\n",
    "    \n",
    "    \n",
    "    curr_states  = np.asarray([e[0] for e in experiences])\n",
    "    actions      = np.asarray([e[1] for e in experiences])\n",
    "    rewards      = np.asarray([e[2] for e in experiences])\n",
    "    terminated_s = np.asarray([e[3] for e in experiences])\n",
    "    truncated_s  = np.asarray([e[4] for e in experiences])\n",
    "    new_states   = np.asarray([e[5] for e in experiences])\n",
    "\n",
    "    \n",
    "    curr_states_t   = torch.as_tensor(curr_states, dtype=torch.float32)\n",
    "    actions_t       = torch.as_tensor(actions,dtype=torch.int64).unsqueeze(-1)\n",
    "    rewards_t       = torch.as_tensor(rewards,dtype=torch.float32).unsqueeze(-1)\n",
    "    terminated_s_t  = torch.as_tensor(terminated_s,dtype=torch.float32).unsqueeze(-1)\n",
    "    truncated_s_t   = torch.as_tensor(truncated_s,dtype=torch.float32).unsqueeze(-1)\n",
    "    new_states_t    = torch.as_tensor(new_states,dtype=torch.float32)\n",
    "    \n",
    "    # Compute Targets \n",
    "    target_q_values      = target_network(new_states_t)\n",
    "    max_target_q_values  = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "    \n",
    "    \n",
    "    temp = torch.logical_or(terminated_s_t,truncated_s_t)\n",
    "    targets = rewards_t + gamma * (torch.logical_not(temp)) * max_target_q_values\n",
    "    \n",
    "    # Compute TD errors\n",
    "    q_values        = online_network(curr_states_t)\n",
    "    action_q_values = torch.gather(input=q_values , dim=1 , index=actions_t)\n",
    "    \n",
    "    error = nn.functional.smooth_l1_loss(action_q_values , targets)  # Huber loss\n",
    "    \n",
    "    # Gradient Descent \n",
    "    optimizer.zero_grad()\n",
    "    error.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update Target network \n",
    "    if i%target_update_frequency==0:\n",
    "        target_network.load_state_dict(online_network.state_dict())\n",
    "        \n",
    "    # Logging \n",
    "    if i%1000==0:\n",
    "        print()\n",
    "        print(\"Steps\" , i)\n",
    "        print(\"Average Reward\" , np.mean(reward_buffer))\n",
    "        \n",
    "    if i==100000:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b329b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c85fec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2bea98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49570480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
