{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "313224b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all\n",
    "from torch import nn\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from collections import deque \n",
    "import itertools \n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dbc88ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values to be used \n",
    "\n",
    "gamma                   = 0.99   # discount rate \n",
    "batch_size              = 32\n",
    "buffer_size             = 50000\n",
    "replay_size             = 1000\n",
    "epsilon_start           = 1.0\n",
    "epsilon_end             = 0.02\n",
    "epsilon_decay           = 10000\n",
    "target_update_frequency = 1000\n",
    "learning_rate           = 5e-4\n",
    "icm_learning_rate       = 1e-4\n",
    "\n",
    "# Whether to use ICM or not \n",
    "is_icm = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c8e7b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment \n",
    "env = gym.make(\"CartPole-v1\")\n",
    "input_shape = int(np.prod(env.observation_space.shape))\n",
    "output_shape = env.action_space.n\n",
    "# print(input_shape)\n",
    "# print(output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "35fc0f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen = buffer_size)\n",
    "reward_buffer = deque([0.0], maxlen = 100)\n",
    "\n",
    "curr_reward   = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "292ba01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network class \n",
    "# nn.Sequential creates the required hidden layers in the order provided \n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self,env):\n",
    "        super().__init__()\n",
    "        \n",
    "        input_features = int(np.prod(env.observation_space.shape))\n",
    "        self.net = nn.Sequential( nn.Linear(input_features , 64) \n",
    "                                 , nn.Tanh() \n",
    "                                 , nn.Linear( 64 , env.action_space.n))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def act(self,curr_state):\n",
    "        \n",
    "        #convert the current state to a pytorch tensor \n",
    "        curr_state_t = torch.as_tensor(curr_state , dtype = torch.float32)\n",
    "        q_values     = self(curr_state_t.unsqueeze(0))\n",
    "        \n",
    "        max_q        = torch.argmax(q_values , dim=1)[0]\n",
    "        action       = max_q.detach().item()\n",
    "        \n",
    "        return action \n",
    "    \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "93517fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICM(nn.Module):\n",
    "    def __init__(self , input_dim , output_dim ,alpha=1 , beta=0.2  ):\n",
    "        \n",
    "        super(ICM,self).__init__()\n",
    "        self.alpha = alpha \n",
    "        self.beta = beta \n",
    "        \n",
    "        #input_dim = input_dim[0]\n",
    "        self.inverse = nn.Linear(input_dim * 2 , 256)\n",
    "        self.pi_logits = nn.Linear(256 , output_dim)\n",
    "        \n",
    "        self.dense1 = nn.Linear(input_dim + 1 , 256)\n",
    "        self.new_state = nn.Linear(256, input_dim)\n",
    "        \n",
    "    def forward(self , state , new_state , action ):\n",
    "        \n",
    "        inverse = F.elu(self.inverse(torch.cat([state, new_state], dim=1)))\n",
    "        pi_logits = self.pi_logits(inverse)\n",
    "\n",
    "        # from [T] to [T,1]\n",
    "        action = action.reshape((action.size()[0], 1))\n",
    "        forward_input = torch.cat([state, action], dim=1)\n",
    "        dense = F.elu(self.dense1(forward_input))\n",
    "        state_ = self.new_state(dense)\n",
    "\n",
    "        return pi_logits, state_\n",
    "\n",
    "    def calc_loss(self, state, new_state, action):\n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        action = torch.tensor(action, dtype=torch.float)\n",
    "        new_state = torch.tensor(new_state, dtype=torch.float)\n",
    "\n",
    "        pi_logits, state_ = self.forward(state, new_state, action)\n",
    "\n",
    "        inverse_loss = nn.MSELoss()\n",
    "        L_I = (1-self.beta)*inverse_loss(pi_logits, action)\n",
    "\n",
    "        forward_loss = nn.MSELoss()\n",
    "        L_F = self.beta*forward_loss(state_, new_state)\n",
    "\n",
    "        intrinsic_reward = self.alpha*((state_ - new_state).pow(2)).mean(dim=1)\n",
    "        return intrinsic_reward, L_I, L_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "99a20521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create networks \n",
    "\n",
    "online_network = Network(env)\n",
    "target_network = Network(env)\n",
    "\n",
    "icm_network    = ICM(input_shape , output_shape) ######\n",
    "\n",
    "# copy one network to another \n",
    "# load_state_dict is used to copy one model parameters to another \n",
    "target_network.load_state_dict(online_network.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(online_network.parameters() , lr=learning_rate)\n",
    "icm_optimizer = torch.optim.Adam(icm_network.parameters() , lr = icm_learning_rate  ) ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3d4b274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the replay buffer \n",
    "\n",
    "curr_state , info = env.reset()\n",
    "\n",
    "for _ in range(replay_size):\n",
    "    \n",
    "    # take a random action\n",
    "    action = env.action_space.sample()\n",
    "    new_state , reward , terminated , truncated, info = env.step(action)\n",
    "    \n",
    "    # collect the experiences\n",
    "    experience_tuple = (curr_state , action , reward , terminated ,truncated, new_state)\n",
    "    replay_buffer.append(experience_tuple)\n",
    "    \n",
    "    curr_state = new_state \n",
    "    \n",
    "    if terminated or truncated :\n",
    "        curr_state , info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1a5136fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sc/63b24d_x7_q7jtckgsd8mx000000gn/T/ipykernel_31622/108214652.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float)\n",
      "/var/folders/sc/63b24d_x7_q7jtckgsd8mx000000gn/T/ipykernel_31622/108214652.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action = torch.tensor(action, dtype=torch.float)\n",
      "/var/folders/sc/63b24d_x7_q7jtckgsd8mx000000gn/T/ipykernel_31622/108214652.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_state = torch.tensor(new_state, dtype=torch.float)\n",
      "/var/folders/sc/63b24d_x7_q7jtckgsd8mx000000gn/T/ipykernel_31622/2153060262.py:78: UserWarning: Using a target size (torch.Size([32, 32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  error = nn.functional.smooth_l1_loss(action_q_values , targets)  # Huber loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps 0\n",
      "Average Reward 72.69\n",
      "\n",
      "Steps 1000\n",
      "Average Reward 71.9\n",
      "\n",
      "Steps 2000\n",
      "Average Reward 64.82\n",
      "\n",
      "Steps 3000\n",
      "Average Reward 47.7\n",
      "\n",
      "Steps 4000\n",
      "Average Reward 37.74\n",
      "\n",
      "Steps 5000\n",
      "Average Reward 44.07\n",
      "\n",
      "Steps 6000\n",
      "Average Reward 52.36\n",
      "\n",
      "Steps 7000\n",
      "Average Reward 60.22\n",
      "\n",
      "Steps 8000\n",
      "Average Reward 71.5\n",
      "\n",
      "Steps 9000\n",
      "Average Reward 77.33\n",
      "\n",
      "Steps 10000\n",
      "Average Reward 87.3\n"
     ]
    }
   ],
   "source": [
    "# Training the agent \n",
    "\n",
    "curr_state , info = env.reset()\n",
    "\n",
    "for i in itertools.count():\n",
    "    \n",
    "    # Epsilon Greedy Strategy to explore \n",
    "    epsilon = np.interp( i , [0,epsilon_decay] ,[ epsilon_start,epsilon_end ] )\n",
    "    random_value = random.random()\n",
    "    \n",
    "    if random_value <= epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else :\n",
    "        action = online_network.act(curr_state)\n",
    "    \n",
    "    new_state , reward , terminated , truncated , info = env.step(action)\n",
    "    \n",
    "    \n",
    "    # collect the experiences\n",
    "    experience_tuple = (curr_state , action , reward , terminated , truncated , new_state)\n",
    "    replay_buffer.append(experience_tuple)\n",
    "    \n",
    "    curr_state = new_state \n",
    "    curr_reward += reward \n",
    "    \n",
    "    if terminated or truncated :\n",
    "        curr_state , info = env.reset()\n",
    "        \n",
    "        reward_buffer.append(curr_reward)\n",
    "        curr_reward = 0.0\n",
    "    \n",
    "    # For rendering the code , beware of decommenting as your kernel can crash\n",
    "#     if len(reward_buffer) >=100:\n",
    "#         if np.mean(reward_buffer)>=200:\n",
    "#             while True :\n",
    "#                 action = online_network.act(curr_state)\n",
    "                \n",
    "#                 curr_state , reward , terminated , truncated , info = env.step(action)\n",
    "#                 env.render()\n",
    "                \n",
    "#                 if terminated or truncated:\n",
    "#                     env.reset\n",
    "    \n",
    "    \n",
    "    # Extract parameters for Gradient Descent \n",
    "    experiences  = random.sample( replay_buffer , batch_size )\n",
    "    \n",
    "    \n",
    "    curr_states  = np.asarray([e[0] for e in experiences])\n",
    "    actions      = np.asarray([e[1] for e in experiences])\n",
    "    rewards      = np.asarray([e[2] for e in experiences])\n",
    "    terminated_s = np.asarray([e[3] for e in experiences])\n",
    "    truncated_s  = np.asarray([e[4] for e in experiences])\n",
    "    new_states   = np.asarray([e[5] for e in experiences])\n",
    "    \n",
    "    curr_states_t   = torch.as_tensor(curr_states, dtype=torch.float32)\n",
    "    actions_t       = torch.as_tensor(actions,dtype=torch.int64).unsqueeze(-1)\n",
    "    rewards_t       = torch.as_tensor(rewards,dtype=torch.float32).unsqueeze(-1)\n",
    "    terminated_s_t  = torch.as_tensor(terminated_s,dtype=torch.float32).unsqueeze(-1)\n",
    "    truncated_s_t   = torch.as_tensor(truncated_s,dtype=torch.float32).unsqueeze(-1)\n",
    "    new_states_t    = torch.as_tensor(new_states,dtype=torch.float32)\n",
    "    \n",
    "    intrinsic_reward, L_I, L_F = icm_network.calc_loss(curr_states_t, new_states_t, actions_t)\n",
    "    rewards_t += intrinsic_reward.detach().numpy()\n",
    "    \n",
    "    # Compute Targets \n",
    "    target_q_values      = target_network(new_states_t)\n",
    "    max_target_q_values  = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "    \n",
    "    \n",
    "    temp = torch.logical_or(terminated_s_t,truncated_s_t)\n",
    "    targets = rewards_t + gamma * (torch.logical_not(temp)) * max_target_q_values\n",
    "    \n",
    "    # Compute TD errors\n",
    "    q_values        = online_network(curr_states_t)\n",
    "    action_q_values = torch.gather(input=q_values , dim=1 , index=actions_t)\n",
    "    \n",
    "    error = nn.functional.smooth_l1_loss(action_q_values , targets)  # Huber loss\n",
    "    \n",
    "    # Gradient Descent \n",
    "    icm_optimizer.zero_grad()\n",
    "    (L_I + L_F).backward()\n",
    "    icm_optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    error.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update Target network \n",
    "    if i%target_update_frequency==0:\n",
    "        target_network.load_state_dict(online_network.state_dict())\n",
    "        \n",
    "    # Logging \n",
    "    if i%1000==0:\n",
    "        print()\n",
    "        print(\"Steps\" , i)\n",
    "        print(\"Average Reward\" , np.mean(reward_buffer))\n",
    "        \n",
    "    if i==10000:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1087cd26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdaa2de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5eec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b368df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6090fcf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cae7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e05f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a2fc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
