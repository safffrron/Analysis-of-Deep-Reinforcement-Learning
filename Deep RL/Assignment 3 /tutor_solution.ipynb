{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J09JCeBfgpyg"
   },
   "source": [
    "# ASSIGNMENT 3\n",
    "# Submission Deadline: 20/03/2024 at 10 AM\n",
    "# Submission Link: https://forms.gle/b8s6xYHUYqTJtSNeA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_xJEq7agpyi"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Provide Information](#Provide-Information)\n",
    "2. [Instructions](#Instructions)\n",
    "3. [Environment](#Environment)\n",
    "4. [Hyperparameters](#Hyperparameters)\n",
    "5. [Helper Functions](#helper)\n",
    "6. [Deep Value Based RL Agents](#deep-value-based)\n",
    "7. [Deep Policy Based RL Agents](#deep-policy-based)\n",
    "8. [Experiments to Run](#experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhHZByQrgpyi"
   },
   "source": [
    "# Provide Information\n",
    "<a id=\"Provide-Information\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jinCpSnJgpyi"
   },
   "source": [
    "Name: **TA**\n",
    "\n",
    "Roll No.: **-**\n",
    "\n",
    "IITK EMail: **-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gclJJ5h_gpyi"
   },
   "source": [
    "# Instructions\n",
    "<a id=\"Instructions\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWO1JQCDgpyi"
   },
   "source": [
    "**Read all the instructions below carefully before you start working on the assignment.**\n",
    "- The purpose of this course is that you learn RL and the best way to do that is by implementation and experimentation.\n",
    "- The assignment requires your to implement some algorithms and you are required report your findings after experimenting with those algorithms.\n",
    "- **You are required to submit ZIP file containing a Jupyter notebook (.ipynb), and an image folder. The notebook would include the code, graphs/plots of the experiments you run and your findings/observations. Image folder is the folder having plots, images, etc.**\n",
    "- In case you use any maths in your explanations, render it using latex in the Jupyter notebook.\n",
    "- You are expected to implement algorithms on your own and not copy it from other sources/class mates. Of course, you can refer to lecture slides.\n",
    "- If you use any reference or material (including code), please cite the source, else it will be considered plagiarism. But referring to other sources that directly solve the problems given in the assignment is not allowed. There is a limit to which you can refer to outside material.\n",
    "- This is an individual assignment.\n",
    "- In case your solution is found to have an overlap with solution by someone else (including external sources), all the parties involved will get zero in this and all future assignments plus further more penalties in the overall grade. We will check not just for lexical but also semantic overlap. Same applies for the code as well. Even an iota of cheating would NOT be tolerated. If you cheat one line or cheat one page the penalty would be same.\n",
    "- Be a smart agent, think long term, if you cheat we will discover it somehow, the price you would be paying is not worth it.\n",
    "- In case you are struggling with the assignment, seek help from TAs. Cheating is not an option! I respect honesty and would be lenient if you are not able to solve some questions due to difficulty in understanding. Remember we are there to help you out, seek help if something is difficult to understand.\n",
    "- The deadline for the submission is given above. Submit at least 30 minutes before the deadline, lot can happen at the last moment, your internet can fail, there can be a power failure, you can be abducted by aliens, etc.\n",
    "- You have to submit your assignment via the Google Form (link above)\n",
    "- The form would close after the deadline and we will not accept any solution. No reason what-so-ever would be accepted for not being able to submit before the deadline.\n",
    "- Since the assignment involves experimentation, reporting your results and observations, there is a lot of scope for creativity and innovation and presenting new perspectives. Such efforts would be highly appreciated and accordingly well rewarded. Be an exploratory agent!\n",
    "- Your code should be very well documented, there are marks for that.\n",
    "- In your plots, have a clear legend and clear lines, etc. Of course you would generating the plots in your code but you must also put these plots in your notebook. Generate high resolution pdf/svg version of the plots so that it doesn't pixilate on zooming.\n",
    "- For all experiments, report about the seed used in the code documentation, write about the seed used.\n",
    "- In your notebook write about all things that are not obvious from the code e.g., if you have made any assumptions, references/sources, running time, etc.\n",
    "-  **DO NOT Forget to write name, roll no and email details above**\n",
    "- **In addition to checking your code, we will be conducting one-on-one viva for the evaluation. So please make sure that you do not cheat!**\n",
    "- **Use of LLMs based tools or AI-based code tools is strictly prohibited! Use of ChatGPT, VS Code, Gemini, CO-Pilot, etc. is not allowed. NOTE VS code is also not allowed. Even in Colab disable the AI assistant. If you use it, we will know it very easily. Use of any of the tools would be counted as cheating and would be given a ZERO, with no questions asked.**\n",
    "- For each of the sub-part in the question create a new cell below the question and put your answer in there. This includes the plots as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiFbz0bkgpyj"
   },
   "source": [
    "# OpenAI Gym Environments\n",
    "<a id=\"Environment\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Ba7SmhPgpyj"
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhBe7r4sh7VG",
    "outputId": "a2752458-d8f2-4dd0-9dbc-b48baf29c003"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLAnSVk20cVh"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37zNScFPgpyl"
   },
   "outputs": [],
   "source": [
    "# all imports go in here\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from itertools import cycle, count\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fH7nOy18gpyl"
   },
   "source": [
    "In this assignment we will be exploring Deep RL algorithms and for this we will be using environmentd provided by OpenAI Gym. In particualr we will be exploring \"CartPole-v0\" and \"MountainCar-v0\" environments (https://gymnasium.farama.org/environments/classic_control/ ). The code to instantiate the environments are given in the cells below. Run these cells and play with the environments to learn more details about the environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0AHrU8Cgpyl",
    "outputId": "e73ea0be-4015-44e3-e51b-47625d91afbe"
   },
   "outputs": [],
   "source": [
    "# Create CartPole environment\n",
    "#https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "#env.seed(34)\n",
    "s = env.reset(seed=344)\n",
    "print(\"Observation Space = \")\n",
    "print(env.observation_space)\n",
    "print(\"Action Space = \")\n",
    "print(env.action_space)\n",
    "done = False\n",
    "for episode in range(20):\n",
    "    print(\"In episode {}\".format(episode))\n",
    "    for i in range(100):\n",
    "        env.render()\n",
    "        print(s)\n",
    "        a = env.action_space.sample()\n",
    "        s, r, done, truncated,_ = env.step(a)\n",
    "        if done:\n",
    "            print(\"Finished after {} timestep\".format(i+1))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhUnZT1ugpyl"
   },
   "outputs": [],
   "source": [
    "# Create MountainCar environment:\n",
    "# https://gymnasium.farama.org/environments/classic_control/mountain_car/\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode=\"rgb_array\")\n",
    "#env.seed(45)\n",
    "s = env.reset(seed=455)\n",
    "print(\"Observation Space = \")\n",
    "print(env.observation_space)\n",
    "print(\"Action Space = \")\n",
    "print(env.action_space)\n",
    "done = False\n",
    "for episode in range(20):\n",
    "    print(\"In episode {}\".format(episode))\n",
    "    for i in range(100):\n",
    "        env.render()\n",
    "        print(s)\n",
    "        a = env.action_space.sample()\n",
    "        s, r, done, truncated, _ = env.step(a)\n",
    "        if done:\n",
    "            print(\"Finished after {} timestep\".format(i+1))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SeIO5kngpyl"
   },
   "source": [
    "# Hyperparameters\n",
    "<a id=\"Hyperparameters\"></a>\n",
    "\n",
    "All your hyperparameters should be stated here. We will change their value here and your code should work  accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peeBMkrPgpym"
   },
   "outputs": [],
   "source": [
    "# mention the values of all the hyperparameters (you can add more hyper-paramters as well) to be used in the entire notebook, put the values that gave the best\n",
    "# performance and were finally used for the agent\n",
    "\n",
    "gamma =  #gamma\n",
    "epsilon = #epsolon greedy strategy\n",
    "temp = #softmax strategy\n",
    "delta = #huber loss\n",
    "tau = #D3QN\n",
    "alpha = #D3QN-PER\n",
    "beta = #D3QN-PER\n",
    "beta_rate = #D3QN-PER\n",
    "MAX_TRAIN_EPISODES = 500\n",
    "MAX_EVAL_EPISODES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqn3RJ5tgpym"
   },
   "source": [
    "# Helper Functions\n",
    "<a id=\"helper\"></a>\n",
    "\n",
    "Write all the helper functions that will be used for value-based and policy based algorithms below. In case you want to add more helper functions, please feel free to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VpjDxtQgpym"
   },
   "outputs": [],
   "source": [
    "def selectGreedyAction(net, state):\n",
    "    #this function gets q-values via the network and selects greedy action from q-values and returns it\n",
    "\n",
    "    #Your code goes in here\n",
    "    with torch.no_grad():\n",
    "        # Assuming state is already a PyTorch tensor and appropriately preprocessed\n",
    "        q_values = net(state).cpu().detach().numpy().squeeze()\n",
    "        # Select the action with the highest Q-value\n",
    "        return np.argmax(q_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1i4KJEggpym"
   },
   "outputs": [],
   "source": [
    "def selectEpsilonGreedyAction(net,  state, epsilon):\n",
    "    #this function gets q-values via the network and selects an action from q-values using epsilon greedy strategy\n",
    "    #and returns it\n",
    "    #note this function can be used for decaying epsilon greedy strategy,\n",
    "    #you would need to create a wrapper function that will handle decaying epsilon\n",
    "    #you can create this wrapper in this helper function section\n",
    "    #for the agents you would be implementing it would be nice to play with decaying parameter to get optimal results\n",
    "\n",
    "    #Your code goes in here\n",
    "    exploration = False\n",
    "    with torch.no_grad():\n",
    "        # Assuming state is already a PyTorch tensor and appropriately preprocessed\n",
    "        q_values = net(state).cpu().detach().numpy().squeeze()\n",
    "\n",
    "    if np.random.rand() > epsilon:\n",
    "        action = np.argmax(q_values)\n",
    "    else:\n",
    "        action = np.random.randint(len(q_values))\n",
    "        exploration = True\n",
    "\n",
    "\n",
    "    return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY2XM_vSjxtL"
   },
   "outputs": [],
   "source": [
    "def selectEpsilonExpGreedyAction(model, state, init=False, init_epsilon=1.0, min_epsilon=0.3, decay_steps=20000):\n",
    "    if init or not hasattr(selectEpsilonExpGreedyAction, 'initialized'):\n",
    "        # Initialization\n",
    "        selectEpsilonExpGreedyAction.epsilon = init_epsilon\n",
    "        selectEpsilonExpGreedyAction.init_epsilon = init_epsilon\n",
    "        selectEpsilonExpGreedyAction.decay_steps = decay_steps\n",
    "        selectEpsilonExpGreedyAction.min_epsilon = min_epsilon\n",
    "        selectEpsilonExpGreedyAction.epsilons = 0.01 / np.logspace(-2, 0, decay_steps, endpoint=False) - 0.01\n",
    "        selectEpsilonExpGreedyAction.epsilons = selectEpsilonExpGreedyAction.epsilons * (init_epsilon - min_epsilon) + min_epsilon\n",
    "        selectEpsilonExpGreedyAction.t = 0\n",
    "        selectEpsilonExpGreedyAction.initialized = True\n",
    "        return -1\n",
    "\n",
    "    # Epsilon update\n",
    "    selectEpsilonExpGreedyAction.epsilon = selectEpsilonExpGreedyAction.min_epsilon if selectEpsilonExpGreedyAction.t >= selectEpsilonExpGreedyAction.decay_steps else selectEpsilonExpGreedyAction.epsilons[selectEpsilonExpGreedyAction.t]\n",
    "    selectEpsilonExpGreedyAction.t += 1\n",
    "\n",
    "    # Action selection\n",
    "    with torch.no_grad():\n",
    "        q_values = model(state).detach().cpu().data.numpy().squeeze()\n",
    "\n",
    "    if np.random.rand() > selectEpsilonExpGreedyAction.epsilon:\n",
    "        action = np.argmax(q_values)\n",
    "    else:\n",
    "        action = np.random.randint(len(q_values))\n",
    "\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOEBNiPZgpym"
   },
   "outputs": [],
   "source": [
    "def selectSoftMaxAction(net, state, temp):\n",
    "    #this function gets q-values via the network and selects an action from q-values using softmax strategy\n",
    "    #and returns it\n",
    "    #note this function can be used for decaying temperature softmax strategy,\n",
    "    #you would need to create a wrapper function that will handle decaying temperature\n",
    "    #you can create this wrapper in this helper function section\n",
    "    #for the agents you would be implementing it would be nice to play with decaying parameter to get optimal results\n",
    "\n",
    "    #Your code goes in here\n",
    "    with torch.no_grad():\n",
    "        # Assuming state is already a PyTorch tensor and appropriately preprocessed\n",
    "        q_values = net(state).cpu().detach().numpy().squeeze()\n",
    "        # Compute softmax probabilities with temperature temp\n",
    "        # Adjust q_values by temp and avoid overflow with max subtraction\n",
    "        q_values_adj = q_values / temp - np.max(q_values / temp)\n",
    "        probabilities = np.exp(q_values_adj) / np.sum(np.exp(q_values_adj))\n",
    "        # Select an action based on the probabilities\n",
    "        action = np.random.choice(len(q_values), p=probabilities)\n",
    "        return action\n",
    "\n",
    "    return softAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAtg5QSugpym"
   },
   "outputs": [],
   "source": [
    "#Value Network\n",
    "\n",
    "def createValueNetwork(inDim, outDim, hDim=[32, 32], activation=F.relu):\n",
    "    #this creates a Feed Forward Neural Network class and instantiates it and returns the class\n",
    "    #the class should be derived from torch nn.Module and it should have init and forward method at the very least\n",
    "    #the forward function should return q-value for each possible action\n",
    "    class MyValueNetwork(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, hidden_dims, activation_fc):\n",
    "            super(MyValueNetwork, self).__init__()\n",
    "            self.activation_fc = activation_fc\n",
    "\n",
    "            # Device management\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            # Network layers\n",
    "            self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "            self.hidden_layers = nn.ModuleList()\n",
    "            for i in range(len(hidden_dims) - 1):\n",
    "                self.hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))\n",
    "            self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "            self.to(self.device)\n",
    "\n",
    "        def _format(self, state):\n",
    "            if not isinstance(state, torch.Tensor):\n",
    "                state = torch.tensor(state, device=self.device, dtype=torch.float32)\n",
    "            if state.dim() == 1:\n",
    "                state = state.unsqueeze(0)\n",
    "            return state\n",
    "\n",
    "        def forward(self, state):\n",
    "            x = self._format(state)\n",
    "            x = self.activation_fc(self.input_layer(x))\n",
    "            for hidden_layer in self.hidden_layers:\n",
    "                x = self.activation_fc(hidden_layer(x))\n",
    "            x = self.output_layer(x)\n",
    "            return x\n",
    "\n",
    "        def load(self, experiences):\n",
    "            states, actions, rewards, new_states, is_terminals = experiences\n",
    "            states = torch.from_numpy(states).float().to(self.device)\n",
    "            actions = torch.from_numpy(actions).long().to(self.device)\n",
    "            new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "            rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "            is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "            return states, actions, rewards, new_states, is_terminals\n",
    "\n",
    "    return MyValueNetwork(inDim, outDim, hDim, activation)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyhlTqSXgpym"
   },
   "outputs": [],
   "source": [
    "#Dueling Network\n",
    "def createDuelingNetwork(input_dim, output_dim, hidden_dims=[32, 32], activation_fn=F.relu):\n",
    "    class DDQNetwork(nn.Module):\n",
    "        def __init__(self, in_features, out_features, hidden_layers=(32, 32), activation=activation_fn):\n",
    "            super(DDQNetwork, self).__init__()\n",
    "            self.activate = activation\n",
    "\n",
    "            # Device management\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            self.initial_layer = nn.Linear(in_features, hidden_layers[0])\n",
    "            self.hidden_layers = nn.ModuleList()\n",
    "            for i in range(len(hidden_layers) - 1):\n",
    "                layer = nn.Linear(hidden_layers[i], hidden_layers[i + 1])\n",
    "                self.hidden_layers.append(layer)\n",
    "            self.value_stream = nn.Linear(hidden_layers[-1], 1)\n",
    "            self.advantage_stream = nn.Linear(hidden_layers[-1], out_features)\n",
    "\n",
    "            self.computing_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            self.to(self.computing_device)\n",
    "\n",
    "        def _format(self, state):\n",
    "            if not isinstance(state, torch.Tensor):\n",
    "                state = torch.tensor(state, dtype=torch.float32, device=self.computing_device)\n",
    "                state = state.unsqueeze(0)\n",
    "            return state\n",
    "\n",
    "        def forward(self, state):\n",
    "            state = self._format(state)\n",
    "            state = self.activate(self.initial_layer(state))\n",
    "            for layer in self.hidden_layers:\n",
    "                state = self.activate(layer(state))\n",
    "            value = self.value_stream(state)\n",
    "            advantage = self.advantage_stream(state)\n",
    "            q_value = value + advantage - advantage.mean(dim=1, keepdim=True).expand_as(advantage)\n",
    "            return q_value\n",
    "\n",
    "        def numpy_float_to_device(self, data):\n",
    "            return torch.from_numpy(data).float().to(self.computing_device)\n",
    "\n",
    "        def load(self, experiences):\n",
    "            states, actions, new_states, rewards, is_terminals = experiences\n",
    "            states = torch.from_numpy(states).float().to(self.device)\n",
    "            actions = torch.from_numpy(actions).long().to(self.device)\n",
    "            new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "            rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "            is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "            return states, actions, new_states, rewards, is_terminals\n",
    "            \n",
    "    return DDQNetwork(input_dim, output_dim, hidden_dims, activation_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQVMuhccgpyn"
   },
   "outputs": [],
   "source": [
    "#Policy Network\n",
    "def createPolicyNetworkREINFORCE(inDim, outDim, hDim = [32,32], activation = F.relu):\n",
    "    #this creates a Feed Forward Neural Network class and instantiates it and returns the class\n",
    "    #the class should be derived from torch nn.Module and it should have init and forward method at the very least\n",
    "    #the forward function should return action logit vector\n",
    "    #Your code goes in here\n",
    "\n",
    "    class policyNetwork(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, hidden_dims, activation_fn):\n",
    "            super(policyNetwork, self).__init__()\n",
    "            self.activation_fn = activation_fn\n",
    "\n",
    "            # Device management\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            # Initialize layers\n",
    "            self.layers = nn.ModuleList()\n",
    "            self.layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "            for i in range(len(hidden_dims)-1):\n",
    "                self.layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            self.layers.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "\n",
    "        def _format(self, x):\n",
    "            if not isinstance(x, torch.Tensor):\n",
    "                x = torch.tensor(x, dtype=torch.float32)\n",
    "                x = x.unsqueeze(0)  # Add batch dimension if missing\n",
    "            return x\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self._format(x)\n",
    "            for layer in self.layers[:-1]:\n",
    "                x = self.activation_fn(layer(x))\n",
    "            return self.layers[-1](x)  # No activation on the output layer\n",
    "\n",
    "        def full_pass(self, x):\n",
    "            logits = self.forward(x)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            log_prob_action = dist.log_prob(action).unsqueeze(-1)\n",
    "            entropy = dist.entropy().unsqueeze(-1)\n",
    "            is_exploratory = action != np.argmax(logits.detach().numpy())\n",
    "            return action.item(), is_exploratory.item(), log_prob_action, entropy\n",
    "\n",
    "        def select_action(self, x):\n",
    "            logits = self.forward(x)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            return action.item()\n",
    "\n",
    "        def select_greedy_action(self, x):\n",
    "            logits = self.forward(x)\n",
    "            return np.argmax(logits.detach().numpy())\n",
    "\n",
    "    return policyNetwork(inDim, outDim, hDim, activation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npbtas4_4X1k"
   },
   "outputs": [],
   "source": [
    "#Policy network for VPG\n",
    "def createPolicyNetworkVPG(inDim, outDim, hDim = [32,32], activation = F.relu):\n",
    "\n",
    "  class policyNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, layer_sizes=(32, 32), activation_function=F.relu):\n",
    "        \"\"\"\n",
    "        Initialize the policy network with specified input size, hidden layer sizes,\n",
    "        and activation function. The network architecture includes a sequence of\n",
    "        fully connected layers.\n",
    "\n",
    "        Parameters:\n",
    "        - input_size: Integer, size of the input layer.\n",
    "        - layer_sizes: Tuple of integers, sizes of the hidden layers.\n",
    "        - activation_function: Callable, the activation function to use in the network.\n",
    "        \"\"\"\n",
    "        super(policyNetwork, self).__init__()\n",
    "        self.activate = activation_function\n",
    "\n",
    "        self.first_layer = nn.Linear(input_size, layer_sizes[0])\n",
    "        self.middle_layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layer = nn.Linear(layer_sizes[i], layer_sizes[i + 1])\n",
    "            self.middle_layers.append(layer)\n",
    "        self.final_layer = nn.Linear(layer_sizes[-1], 1)\n",
    "\n",
    "    def _format(self, input_state):\n",
    "        \"\"\"\n",
    "        Ensures the input is in the correct torch.Tensor format.\n",
    "\n",
    "        Parameters:\n",
    "        - input_state: The state input to the network, can be a list or a numpy array.\n",
    "\n",
    "        Returns:\n",
    "        - A torch.Tensor of the state, ready for processing by the network.\n",
    "        \"\"\"\n",
    "        tensor_input = input_state\n",
    "        if not isinstance(tensor_input, torch.Tensor):\n",
    "            tensor_input = torch.tensor(tensor_input, dtype=torch.float32)\n",
    "            tensor_input = tensor_input.unsqueeze(0)\n",
    "        return tensor_input\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Parameters:\n",
    "        - state: The input state to the network.\n",
    "\n",
    "        Returns:\n",
    "        - The output from the network.\n",
    "        \"\"\"\n",
    "        state = self._format(state)\n",
    "        state = self.activate(self.first_layer(state))\n",
    "        for layer in self.middle_layers:\n",
    "            state = self.activate(layer(state))\n",
    "        return self.final_layer(state)\n",
    "\n",
    "  return policyNetwork(inDim, hDim, activation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0BRe6vsgpyn"
   },
   "outputs": [],
   "source": [
    "def plotQuantity(quantityListDict, totalEpisodeCount, descriptionList):\n",
    "    #this function takes in the quantityListDict and plots quantity vs episodes.\n",
    "    #quantityListListDict = {envInstanceCount: quantityList}\n",
    "    #quantityList is list of the qunatity per episode,\n",
    "    #for example it could be mean reward per episode, traintime per episode, etc.\n",
    "    #\n",
    "    #NOTE: len(quantityList) == totalEpisodeCount\n",
    "    #\n",
    "    #Since we run multiple instances of the environment, there will be variance across environments\n",
    "    #so in the plot, you will plot per episode maximum, minimum and average value across all env instances\n",
    "    #Basically, you need to envelop (e.g., via color) the quantity between max and min with mean value in between\n",
    "    #\n",
    "    #use the descriptionList parameter to put legends, title, etc.\n",
    "    #For each of the plot, create the legend on the left/right side so that it doesn't overlay on the plot lines/envelop.\n",
    "    #\n",
    "    #this is a generic function and can be used to plot any of the quantity of interest\n",
    "    #In particular we will be using this function to plot:\n",
    "    #        mean train rewards vs episodes\n",
    "    #        mean evaluation rewards vs episodes\n",
    "    #        total steps vs episode\n",
    "    #        train time vs episode\n",
    "    #        wall clock time vs episode\n",
    "    #\n",
    "    #\n",
    "    #this function doesn't return anything\n",
    "\n",
    "    #Your code goes in here\n",
    "\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bF1x-ZYigpyn"
   },
   "outputs": [],
   "source": [
    "def huberLoss(error, delta):\n",
    "    #this function calculates the huber loss for the error using the delta parameter\n",
    "\n",
    "    #Your code goes in here\n",
    "\n",
    "    return hLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UfNomG6gpyn"
   },
   "outputs": [],
   "source": [
    "#in case you want to create any other helper function, the code goes in here\n",
    "def reset_env(env, seed=None, options=None):\n",
    "    # Reset the environment with seed and options, handling new seeding mechanism\n",
    "    initial_observation, info = env.reset(seed=seed, options=options)\n",
    "    return initial_observation, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GhVe7cEgpyn"
   },
   "outputs": [],
   "source": [
    "#in case you want to create any other helper function, the code goes in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnRVJGG5gpyn"
   },
   "source": [
    "# Deep Value Based RL agents.\n",
    "<a id=\"deep-value-based\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kydy6fphgpyn"
   },
   "source": [
    "### The purpose of this part is to learn about different Deep Value Based RL agents.\n",
    "\n",
    "In this part of the assignment you will be implementing Deep RL algorithms we learnt in Lectures. Namely, we will be implementing NFQ, DQN, Double DQN (DDQN), Duelling Double DQN (D3QN), and Duelling Double DQN with Prioritized Experience Replay (D3QN-PER). For all the algorithms below, this time we will not be specifying the hyper-parameters, please play with the hyper-params to come up with the best values. This way you will learn to tune the model. Some of the values were specified in the lecture, that would be a good starting point. Your aim is to develop the best NFQ/DQN/DDQN/D3QN/D3QN-PER agent for each of the setting.  \n",
    "\n",
    "For those of you who follow TEDEd, here is an interesting video by TED on DQN and Atari Games: https://www.youtube.com/watch?v=PP8Zc778B8s\n",
    "\n",
    "Also since these environments are available in Gymanasium, there are public leaderboards (https://github.com/openai/gym/wiki/Leaderboard) for each of these environments. Compare where does your agent stand on these leaderboard for each of these environments, try to tune your agents so that it is on the top of the leaderboard. In fact, if your agent performs well on these environments, you can alse make your entry on the leaderboard.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a0lXEqkgpyn"
   },
   "source": [
    "## <font color='green'> Do not change any Class/Methods definition. We have split the class methods across cells for code readibility purposes. This requires to inherit the same class, please do not change it. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2hTOERUgpyn"
   },
   "source": [
    "## ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySpheftNgpyo"
   },
   "source": [
    "In next few cells, you will implement replaybuffer class.\n",
    "\n",
    "This class creates a buffer for storing and retrieving experiences. This is a generic class and can be used\n",
    "for different agents like NFQ, DQN, DDQN, PER_DDQN, etc.\n",
    "Following are the methods for this class which are implemented in subsequent cells\n",
    "\n",
    "```\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, bufferSize, **kwargs)\n",
    "    def store(self, experience)\n",
    "    def update(self, indices, priorities)\n",
    "    def collectExperiences(env, state, explorationStrategy, net = None)\n",
    "    def sample(self, batchSize, **kwargs)\n",
    "    def splitExperiences(self, experiences)\n",
    "    def length(self)\n",
    "```   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXL1HSFmgpyo"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, bufferSize, bufferType = 'DQN', **kwargs):\n",
    "        # this function creates the relevant data-structures, and intializes all relevant variables\n",
    "        # it can take variable number of parameters like alpha, beta, beta_rate (required for PER)\n",
    "        # here the bufferType variable can be used to maintain one class for all types of agents\n",
    "        # using the bufferType parameter in the methods below, you can implement all possible functionalities\n",
    "        # that could be used for different types of agents\n",
    "        # permissible values for bufferType = NFQ, DQN, DDQN, D3QN and PER-D3QN\n",
    "\n",
    "        #Your code goes in here\n",
    "        self.ss_mem = np.empty(shape=(bufferSize), dtype=np.ndarray)\n",
    "        self.as_mem = np.empty(shape=(bufferSize), dtype=np.ndarray)\n",
    "        self.rs_mem = np.empty(shape=(bufferSize), dtype=np.ndarray)\n",
    "        self.ps_mem = np.empty(shape=(bufferSize), dtype=np.ndarray)\n",
    "        self.ds_mem = np.empty(shape=(bufferSize), dtype=np.ndarray)\n",
    "\n",
    "        self.max_size = bufferSize\n",
    "        self.bufferType = bufferType\n",
    "        self._idx = 0\n",
    "        self.size = 0\n",
    "        # Assume default batch size for sampling if not provided\n",
    "        self.default_batch_size = 64\n",
    "\n",
    "        #kwargs\n",
    "        if(self.bufferType == \"D3QN-PER\"):\n",
    "            \n",
    "            self.max_samples = kwargs.get('max_samples', 10000)\n",
    "            self.batch_size = kwargs.get('batch_size', 64)\n",
    "            self.rank_based = kwargs.get('rank_based', False)\n",
    "            self.alpha = kwargs.get('alpha', 0.6)  \n",
    "            self.beta = kwargs.get('beta', 0.1)  \n",
    "            self.beta_rate = kwargs.get('beta_rate', 0.9999)\n",
    "            \n",
    "            # Other initializations that depend on the parameters above\n",
    "            self.memory = np.empty(shape=(self.max_samples, 2), dtype=np.ndarray)\n",
    "            self.n_entries = 0\n",
    "            self.next_index = 0\n",
    "            self.td_error_index = 0\n",
    "            self.sample_index = 1\n",
    "\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGQdfC05gpyo"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "    def store(self, experience):\n",
    "        #stores the experiences, based on parameters in init it can assign priorities, etc.\n",
    "        #\n",
    "        #this function does not return anything\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        if(self.bufferType == \"D3QN-PER\"):\n",
    "            priority = 1.0\n",
    "            if self.n_entries > 0:\n",
    "                priority = self.memory[\n",
    "                    :self.n_entries, \n",
    "                    self.td_error_index].max()\n",
    "            self.memory[self.next_index, \n",
    "                        self.td_error_index] = priority\n",
    "            self.memory[self.next_index, \n",
    "                        self.sample_index] = np.array(experience, dtype = 'object')\n",
    "            self.n_entries = min(self.n_entries + 1, self.max_samples)\n",
    "            self.next_index += 1\n",
    "            self.next_index = self.next_index % self.max_samples\n",
    "\n",
    "        else:\n",
    "            \n",
    "            s, a, r, p, d = experience\n",
    "            self.ss_mem[self._idx] = s\n",
    "            self.as_mem[self._idx] = a\n",
    "            self.rs_mem[self._idx] = r\n",
    "            self.ps_mem[self._idx] = p\n",
    "            self.ds_mem[self._idx] = d\n",
    "    \n",
    "            self._idx = (self._idx + 1) % self.max_size\n",
    "            self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gygLd3SVgpyo"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "    def update(self, idxs, td_errors):\n",
    "        self.memory[idxs, self.td_error_index] = np.abs(td_errors)\n",
    "        if self.rank_based:\n",
    "            sorted_arg = self.memory[:self.n_entries, self.td_error_index].argsort()[::-1]\n",
    "            self.memory[:self.n_entries] = self.memory[sorted_arg]\n",
    "\n",
    "    def _update_beta(self):\n",
    "        self.beta = min(1.0, self.beta * self.beta_rate**-1)\n",
    "        return self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpLQpOC_gpyo"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "    def collectExperiences(env, state, explorationStrategy, countExperiences, net = None):\n",
    "        #this method allows the agent to interact with the environment starting from a state and it collects\n",
    "        #experiences during the interaction, it uses network to get the value function and uses exploration strategy\n",
    "        #to select action. It collects countExperiences and in case the environment terminates before that it returns\n",
    "        #the function calling this method needs to handle early termination accordingly.\n",
    "        #\n",
    "        #this function does not return anything\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        experiences = []\n",
    "        for _ in range(countExperiences):\n",
    "            action = explorationStrategy(state, net)  # Assuming explorationStrategy uses net\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            experiences.append((state, action, reward, next_state, done))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        return experiences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnKZQHEcgpyo"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "    def sample(self, batchSize=None, **kwargs):\n",
    "        # this method returns batchSize number of experiences\n",
    "        # based on extra arguments, it could do sampling or it could return the latest batchSize experiences or\n",
    "        # via some other strategy\n",
    "        #\n",
    "        # in the case of Prioritized Experience Replay (PER) the sampling needs to take into account the priorities\n",
    "        #\n",
    "        # this function returns experiences samples\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        if(self.bufferType == \"D3QN-PER\"):\n",
    "            batchSize = self.batch_size if batchSize == None else batchSize\n",
    "            self._update_beta()\n",
    "            entries = self.memory[:self.n_entries]\n",
    "    \n",
    "            if self.rank_based:\n",
    "                priorities = 1/(np.arange(self.n_entries) + 1)\n",
    "            else: # proportional\n",
    "                priorities = entries[:, self.td_error_index] \n",
    "            scaled_priorities = priorities**self.alpha        \n",
    "            probs = np.array(scaled_priorities/np.sum(scaled_priorities), dtype=np.float64)\n",
    "    \n",
    "            weights = (self.n_entries * probs)**-self.beta\n",
    "            normalized_weights = weights/weights.max()\n",
    "            idxs = np.random.choice(self.n_entries, batchSize, replace=False, p=probs)\n",
    "            samples = np.array([entries[idx] for idx in idxs])\n",
    "            \n",
    "            samples_stacks = [np.vstack(batch_type) for batch_type in np.vstack(samples[:, self.sample_index]).T]\n",
    "            idxs_stack = np.vstack(idxs)\n",
    "            weights_stack = np.vstack(normalized_weights[idxs])\n",
    "            return idxs_stack, weights_stack, samples_stacks   \n",
    "\n",
    "        else:\n",
    "        \n",
    "            if batchSize is None:\n",
    "                batchSize = self.default_batch_size\n",
    "            idxs = np.random.choice(self.size, batchSize, replace=False)\n",
    "            experiences = (np.vstack(self.ss_mem[idxs]), \\\n",
    "                           np.vstack(self.as_mem[idxs]), \\\n",
    "                           np.vstack(self.rs_mem[idxs]), \\\n",
    "                           np.vstack(self.ps_mem[idxs]), \\\n",
    "                           np.vstack(self.ds_mem[idxs]))\n",
    "            return experiences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZ7-xVbbgpyo"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "    def splitExperiences(self, experiences):\n",
    "        #it takes in experiences and gives the following:\n",
    "        #states, actions, rewards, nextStates, dones\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        states, actions, rewards, nextStates, dones = experiences\n",
    "        return states, actions, rewards, nextStates, dones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-6ecPNIgpyo"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "    def length(self):\n",
    "        #tells the number of experiences stored in the internal buffer\n",
    "        if(self.bufferType == \"D3QN-PER\"):\n",
    "            return self.n_entries\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChPr8Frngpyo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC-jDRvugpyp"
   },
   "source": [
    "## Neural Fitted Q (NFQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-4sVOgagpyp"
   },
   "source": [
    "Implement the Neural Fitted Q algorithm. We have studied about NFQ algorithm in the Lecture. Use the function definitions (given below).\n",
    "\n",
    "This class implements the NFQ Agent, you are required to implement the various methods of this class\n",
    "as outlined below. Note this class is generic and should work with any permissible Gym environment.\n",
    "Also please feel free to play with different exploration strategies with decaying paramters (epsilon/temperature)\n",
    "\n",
    "```\n",
    "class NFQ():\n",
    "    def __init__(env, seed, gamma, epochs,\n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn,\n",
    "                 explorationStrategyEvalFn)\n",
    "    def initBookKeeping(self)\n",
    "    def performBookKeeping(self, train = True)\n",
    "    def runNFQ(self)\n",
    "    def trainAgent(self)\n",
    "    def trainNetwork(self, experiences, epochs)\n",
    "    def evaluateAgent(self)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VQU3cEkgpyp"
   },
   "outputs": [],
   "source": [
    "class NFQ():\n",
    "    def __init__(self, env_id, seed, gamma, epochs,\n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn,\n",
    "                 explorationStrategyEvalFn):\n",
    "\n",
    "        #this NFQ method\n",
    "        # 1. creates and initializes (with seed) the environment, train/eval episodes, gamma, etc.\n",
    "        # 2. creates and intializes all the variables required for book-keeping values via the initBookKeeping method\n",
    "        # 3. creates Q-network using the createValueNetwork above\n",
    "        # 4. creates and initializes (with network params) the optimizer function\n",
    "        # 5. sets the explorationStartegy variables/functions for train and evaluation\n",
    "        # 6. sets the batchSize for the number of experiences\n",
    "        # 7. Creates the replayBuffer\n",
    "\n",
    "        # 1. Environment initialization using env_id\n",
    "        self.env = gym.make(env_id)\n",
    "        self.seed = seed\n",
    "        reset_env(self.env, seed=seed)  # Seed the environment (affects environment operations)\n",
    "\n",
    "        # Note on seeding:\n",
    "        # Seeding affects global state for NumPy and PyTorch, which might impact other parts of an application.\n",
    "        # It's set here for reproducibility within the scope of this NFQ instance's operations.\n",
    "        torch.manual_seed(seed)  # Seed PyTorch (global effect)\n",
    "        np.random.seed(seed)  # Seed NumPy (global effect)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # 2. Book-keeping initialization\n",
    "        #self.initBookKeeping()\n",
    "\n",
    "        # 3. Creating Q-network\n",
    "        inDim = self.env.observation_space.shape[0]\n",
    "        outDim = self.env.action_space.n\n",
    "        hDim = [512,128]  # Example hidden layer dimensions, adjust as needed\n",
    "        activation = torch.nn.functional.relu  # Specify the activation function\n",
    "        self.q_network = createValueNetwork(inDim, outDim, hDim, activation)\n",
    "\n",
    "        # 4. Optimizer initialization\n",
    "        self.optimizer = optimizerFn(self.q_network, optimizerLR)\n",
    "\n",
    "        # 5. Exploration strategy setup\n",
    "        self.explorationStrategyTrain = explorationStrategyTrainFn\n",
    "        self.explorationStrategyEval = explorationStrategyEvalFn\n",
    "\n",
    "        # 6. Other variables\n",
    "        self.gamma = gamma\n",
    "        self.epochs = epochs\n",
    "        self.batchSize = batchSize\n",
    "        self.MAX_TRAIN_EPISODES = MAX_TRAIN_EPISODES\n",
    "        self.MAX_EVAL_EPISODES = MAX_EVAL_EPISODES\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0m8LFGDgpyq"
   },
   "outputs": [],
   "source": [
    "class NFQ(NFQ):\n",
    "    def initBookKeeping(self):\n",
    "        #this method creates and intializes all the variables required for book-keeping values and it is called by\n",
    "        #init method\n",
    "        pass\n",
    "\n",
    "    #Your code goes in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEWENknegpyq"
   },
   "outputs": [],
   "source": [
    "class NFQ(NFQ):\n",
    "    def performBookKeeping(self, train = True):\n",
    "        #this method updates relevant variables for the bookKeeping, this can be called\n",
    "        #multiple times during training\n",
    "        #if you want you can print information using this, so it may help to monitor progress and also help to debug\n",
    "        pass\n",
    "\n",
    "    #Your code goes in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRQentgTgpyq"
   },
   "outputs": [],
   "source": [
    "class NFQ(NFQ):\n",
    "        def stepint(self, state, env):\n",
    "          action = self.explorationStrategyTrain(self.online_model, state, 0.5) #0.5 is the epsilon value\n",
    "          new_state, reward, is_terminal, is_truncated, info = env.step(action)\n",
    "          is_failure = is_terminal and not is_truncated\n",
    "          experience = (state, action, reward, new_state, float(is_failure))\n",
    "\n",
    "          self.experiences.append(experience)\n",
    "          self.episode_reward[-1] += reward\n",
    "          self.episode_timestep[-1] += 1\n",
    "          #self.episode_exploration[-1] += int(self.training_strategy.exploratory_action_taken)\n",
    "          return new_state, is_terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGykxazHgpyq"
   },
   "outputs": [],
   "source": [
    "class NFQ(NFQ):\n",
    "    def runNFQ(self):\n",
    "        #this is the main method, it trains the agent, performs bookkeeping while training and finally evaluates\n",
    "        #the agent and returns the following quantities:\n",
    "        #1. episode wise mean train rewards\n",
    "        #2. epsiode wise mean eval rewards\n",
    "        #2. episode wise trainTime (in seconds): time elapsed during training since the start of the first episode\n",
    "        #3. episode wise wallClockTime (in seconds): actual time elapsed since the start of training,\n",
    "        #                               note this will include time for BookKeeping and evaluation\n",
    "        # Note both trainTime and wallClockTime get accumulated as episodes proceed.\n",
    "\n",
    "\n",
    "        #Your code goes in here\n",
    "        resultList, trainTimeList, evalRewardsList, wallClockTimeList = self.trainAgent()\n",
    "        resultEval = self.evaluateAgent()\n",
    "        finalEvalReward  = np.mean(resultEval)\n",
    "\n",
    "\n",
    "        return resultList, trainTimeList, evalRewardsList, wallClockTimeList, finalEvalReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWEYb4uIgpyq"
   },
   "outputs": [],
   "source": [
    "class NFQ(NFQ):\n",
    "    def trainAgent(self):\n",
    "\n",
    "\n",
    "        #this method collects experiences and trains the NFQ agent and does BookKeeping while training.\n",
    "        #this calls the trainNetwork() method internally, it also evaluates the agent per episode\n",
    "        #it trains the agent for MAX_TRAIN_EPISODES\n",
    "\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.evaluation_scores = []\n",
    "        self.episode_exploration = []\n",
    "        training_start = time.time()\n",
    "        global train_count\n",
    "        train_count +=1\n",
    "        self.online_model = self.q_network\n",
    "        self.value_optimizer = self.optimizer\n",
    "        self.experiences = []\n",
    "        max_episodes = self.MAX_TRAIN_EPISODES\n",
    "        result = np.empty((2000, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            print(f'Episode {episode} and train called {train_count} times')\n",
    "            episode_start = time.time()\n",
    "\n",
    "            state, _ = self.env.reset(seed=self.seed)\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            for step in count():\n",
    "                state, is_terminal = self.stepint(state, self.env)\n",
    "\n",
    "                if len(self.experiences) >= self.batchSize:\n",
    "\n",
    "                  '''\n",
    "                  #debug going on\n",
    "                    #print(self.experiences.shape)\n",
    "                    #for element in self.experiences:\n",
    "                      #print(element.shape)\n",
    "                    print(self.experiences)\n",
    "                    states, actions, rewards, next_states, done_flags = [], [], [], [], []\n",
    "                    for state, action, reward, next_state, done in self.experiences:\n",
    "                      states.append(state)\n",
    "                      actions.append(action)\n",
    "                      rewards.append(reward)\n",
    "                      next_states.append(next_state)\n",
    "                      done_flags.append(done)\n",
    "\n",
    "                    experiences = np.array([states, actions, rewards, next_states, done_flags])\n",
    "                  #debug went on\n",
    "                  '''\n",
    "                  experiences = np.array(self.experiences, dtype=\"object\")\n",
    "                  batches = [np.vstack(sars) for sars in experiences.T]\n",
    "                  experiences = self.online_model.load(batches)\n",
    "                  self.trainNetwork(experiences,self.epochs)\n",
    "                  self.experiences.clear()\n",
    "\n",
    "                if is_terminal:\n",
    "                    #gc.collect()\n",
    "                    break\n",
    "\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            self.episode_seconds.append(episode_elapsed)\n",
    "            training_time += episode_elapsed\n",
    "            evaluation_score= np.mean(self.evaluateAgent())\n",
    "\n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "\n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            #lst_100_exp_rat = np.array(self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
    "            #mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
    "            #std_100_exp_rat = np.std(lst_100_exp_rat)\n",
    "\n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode-1] = total_step, mean_100_reward, mean_100_eval_score, training_time, wallclock_elapsed\n",
    "            reached_max_minutes = wallclock_elapsed >= 10 * 60\n",
    "            reached_max_episodes = episode >= 10000\n",
    "            #reached_goal_mean_reward = mean_100_eval_score >= 475\n",
    "            reached_goal_mean_reward = False\n",
    "            training_is_over = reached_max_minutes or reached_max_episodes or reached_goal_mean_reward\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "\n",
    "        final_eval_rwd_list = self.evaluateAgent()\n",
    "        mean_eval_rwd = np.mean(final_eval_rwd_list)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        self.env.close()\n",
    "\n",
    "        return result, training_time, final_eval_rwd_list, wallclock_time\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GosBlj66gpyq"
   },
   "outputs": [],
   "source": [
    "class NFQ(NFQ):\n",
    "    def trainNetwork(self, experiences, epochs):\n",
    "\n",
    "        for _ in range(epochs):\n",
    "\n",
    "            states, actions, rewards, next_states, is_terminals = experiences\n",
    "            batch_size = len(is_terminals)\n",
    "\n",
    "            max_a_q_sp = self.online_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "            target_q_s = rewards + self.gamma * max_a_q_sp * (1 - is_terminals)\n",
    "            q_sa = self.online_model(states).gather(1, actions)\n",
    "\n",
    "            td_errors = q_sa - target_q_s\n",
    "            value_loss = td_errors.pow(2).mul(0.5).mean()\n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.value_optimizer.step()\n",
    "\n",
    "\n",
    "        # this method trains the value network epoch number of times and is called by the trainAgent function\n",
    "        # it essentially uses the experiences to calculate target, using the targets it calculates the error, which\n",
    "        # is further used for calulating the loss. It then uses the optimizer over the loss\n",
    "        # to update the params of the network by backpropagating through the network\n",
    "        # this function does not return anything\n",
    "        # you can try out other loss functions other than MSE like Huber loss, MAE, etc.\n",
    "\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XiWAj5kgpyr"
   },
   "outputs": [],
   "source": [
    "class NFQ(NFQ):\n",
    "    def evaluateAgent(self):\n",
    "        #this function evaluates the agent using the value network, it evaluates agent for MAX_EVAL_EPISODES\n",
    "        #typcially MAX_EVAL_EPISODES = 1\n",
    "        rwd_list = []\n",
    "        for _ in range(self.MAX_EVAL_EPISODES):\n",
    "            s, _ = self.env.reset(seed=self.seed)\n",
    "            rwd_list.append(0)\n",
    "            for _ in count():\n",
    "                a = self.explorationStrategyEval(self.online_model, s)\n",
    "                s, rwd, done, truncated,_ = self.env.step(a)\n",
    "                rwd_list[-1] += rwd\n",
    "                if done or truncated: break\n",
    "        return rwd_list\n",
    "        #return np.mean(rwd_list), np.std(rwd_list)\n",
    "        #return finalEvalRewardsList  can we return rs from here and do np.mean and std later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aa8RUBSPG7OZ",
    "outputId": "63851a5e-9d96-4862-b86e-b1f9ebfa010d"
   },
   "outputs": [],
   "source": [
    "train_count = 0\n",
    "nfq_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "seed_list = [420,133,74,317,233]\n",
    "for myseed in seed_list:\n",
    "\n",
    "    batch_size = 1024\n",
    "    epochs = 40\n",
    "    val_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    gamma = 0.99\n",
    "    max_episodes = 10000\n",
    "    goal_mean_100_reward = 475\n",
    "\n",
    "    nfq_instance = NFQ(\n",
    "    env_id='CartPole-v1',\n",
    "    seed=myseed,\n",
    "    gamma=0.99,\n",
    "    epochs=30,\n",
    "    bufferSize=10000,\n",
    "    batchSize=1024,\n",
    "    optimizerFn=val_optimizer_fn,\n",
    "    optimizerLR=1e-3,\n",
    "    MAX_TRAIN_EPISODES=2000,\n",
    "    MAX_EVAL_EPISODES=1,\n",
    "    explorationStrategyTrainFn=selectEpsilonGreedyAction,\n",
    "    explorationStrategyEvalFn=selectGreedyAction)\n",
    "\n",
    "    trainRewardsList, trainTimeList, evalRewardsList, wallClockTimeList, final_eval_score = nfq_instance.runNFQ()\n",
    "    nfq_results.append(trainRewardsList)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = nfq_instance\n",
    "\n",
    "nfq_results = np.array(nfq_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_max, steps_max, eval_score_max, train_time_max, wall_clock_max = np.max(nfq_results, axis=0).T\n",
    "reward_min, steps_min, eval_score_min, train_time_min, wall_clock_min = np.min(nfq_results, axis=0).T\n",
    "reward_avg, steps_avg, eval_score_avg, train_time_avg, wall_clock_avg = np.mean(nfq_results, axis=0).T\n",
    "episode_indices = np.arange(len(reward_avg))\n",
    "\n",
    "plt.style.use('ggplot')  \n",
    "fig, plot_areas = plt.subplots(5, 1, figsize=(12, 25), sharex='col')\n",
    "fig.subplots_adjust(hspace=0.5) \n",
    "\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "titles = ['Total Steps','Training Reward', 'Evaluation reward',  'Training Duration', 'Wall-clock Time']\n",
    "y_labels = ['Steps', 'Reward', 'Reward', 'Seconds', 'Seconds']\n",
    "data_max = [reward_max, steps_max, eval_score_max, train_time_max, wall_clock_max]\n",
    "data_min = [reward_min, steps_min, eval_score_min, train_time_min, wall_clock_min]\n",
    "data_avg = [reward_avg, steps_avg, eval_score_avg, train_time_avg, wall_clock_avg]\n",
    "\n",
    "# Generate plots\n",
    "for ax, title, color, max_data, min_data, avg_data, y_label in zip(plot_areas, titles, colors, data_max, data_min, data_avg, y_labels):\n",
    "    ax.plot(max_data, linestyle='--', color=color, alpha=0.75)\n",
    "    ax.plot(min_data, linestyle='--', color=color, alpha=0.75)\n",
    "    ax.plot(avg_data, label='NFQ', color=color, linewidth=2)\n",
    "    ax.fill_between(episode_indices, min_data, max_data, color=color, alpha=0.2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.legend()\n",
    "\n",
    "plot_areas[-1].set_xlabel('Episodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCF1OdOkT_rV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnqMi0ycgpyr"
   },
   "source": [
    "## Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LikfGG-Pgpyr"
   },
   "source": [
    "Implement the Deep Q algorithm. We have studied about DQN algorithm in the Lecture. Use the function definitions (given below).\n",
    "\n",
    "This class implements the DQN Agent, you are required to implement the various methods of this class\n",
    "as outlined below. Note this class is generic and should work with any permissible Gym environment\n",
    "\n",
    "```\n",
    "class DQN():\n",
    "    def __init__(env, seed, gamma,\n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn,\n",
    "                 explorationStrategyEvalFn,\n",
    "                 updateFrequency)\n",
    "    def initBookKeeping(self)\n",
    "    def performBookKeeping(self, train = True)\n",
    "    def runDQN(self)\n",
    "    def trainAgent(self)\n",
    "    def trainNetwork(self, experiences)\n",
    "    def updateNetwork(self, onlineNet, targetNet)\n",
    "    def evaluateAgent(self)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8z8Y0G4pgpyr"
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, env_id, seed, gamma,epochs,\n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn,\n",
    "                 explorationStrategyEvalFn,\n",
    "                 updateFrequency):\n",
    "        #this DQN method\n",
    "        # 1. creates and initializes (with seed) the environment, train/eval episodes, gamma, etc.\n",
    "        # 2. creates and intializes all the variables required for book-keeping values via the initBookKeeping method\n",
    "        # 3. creates traget and online Q-networks using the createValueNetwork above\n",
    "        # 4. creates and initializes (with network params) the optimizer function\n",
    "        # 5. sets the explorationStartegy variables/functions for train and evaluation\n",
    "        # 6. sets the batchSize for the number of experiences\n",
    "        # 7. Creates the replayBuffer\n",
    "\n",
    "        # 1. Environment initialization using env_id\n",
    "\n",
    "        self.env = gym.make(env_id)\n",
    "        self.seed = seed\n",
    "        reset_env(self.env, seed=seed)  # Seed the environment (affects environment operations)\n",
    "\n",
    "        # Note on seeding:\n",
    "        # Seeding affects global state for NumPy and PyTorch, which might impact other parts of an application.\n",
    "        # It's set here for reproducibility within the scope of this NFQ instance's operations.\n",
    "        torch.manual_seed(seed)  # Seed PyTorch (global effect)\n",
    "        np.random.seed(seed)  # Seed NumPy (global effect)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # 2. Book-keeping initialization\n",
    "        #self.initBookKeeping()\n",
    "\n",
    "        # 3. Creating online Q-network\n",
    "        inDim = self.env.observation_space.shape[0]\n",
    "        outDim = self.env.action_space.n\n",
    "        hDim = [512,128]  # Example hidden layer dimensions, adjust as needed\n",
    "        activation = torch.nn.functional.relu  # Specify the activation function\n",
    "        self.target_model = createValueNetwork(inDim, outDim, hDim, activation)\n",
    "        self.online_model = createValueNetwork(inDim, outDim, hDim, activation)\n",
    "\n",
    "        # 4. Optimizer initialization\n",
    "        self.optimizer = optimizerFn(self.online_model, optimizerLR)\n",
    "\n",
    "        # 5. Exploration strategy setup\n",
    "        self.explorationStrategyTrain = explorationStrategyTrainFn\n",
    "        self.explorationStrategyEval = explorationStrategyEvalFn\n",
    "\n",
    "        # 6. Other variables\n",
    "        self.gamma = gamma\n",
    "        self.epochs = epochs\n",
    "        self.batchSize = batchSize\n",
    "        self.MAX_TRAIN_EPISODES = MAX_TRAIN_EPISODES\n",
    "        self.MAX_EVAL_EPISODES = MAX_EVAL_EPISODES\n",
    "        self.updateFrequency = updateFrequency\n",
    "\n",
    "        # 7. Replay buffer\n",
    "        self.replay_buffer_fn = lambda: ReplayBuffer(bufferSize=50000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTOF0frKgpyr"
   },
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def initBookKeeping(self):\n",
    "        #this method creates and intializes all the variables required for book-keeping values and it is called\n",
    "        #init method\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        #\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHmgdSDDgpyr"
   },
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def performBookKeeping(self, train = True):\n",
    "        #this method updates relevant variables for the bookKeeping, this can be called\n",
    "        #multiple times during training\n",
    "        #if you want you can print information using this, so it may help to monitor progress and also help to debug\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        #\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLWmiKtzjxtb"
   },
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "        def stepint(self, state, env):\n",
    "          action = self.explorationStrategyTrain(self.online_model, state)\n",
    "          new_state, reward, is_terminal, is_truncated, info = env.step(action)\n",
    "          is_failure = is_terminal and not is_truncated\n",
    "          experience = (state, action, reward, new_state, float(is_failure))\n",
    "\n",
    "          self.replay_buffer.store(experience)\n",
    "          self.episode_reward[-1] += reward\n",
    "          self.episode_timestep[-1] += 1\n",
    "          return new_state, is_terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6a3xO7Vpgpyr"
   },
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def runDQN(self):\n",
    "        #this is the main method, it trains the agent, performs bookkeeping while training and finally evaluates\n",
    "        #the agent and returns the following quantities:\n",
    "        #1. episode wise mean train rewards\n",
    "        #2. epsiode wise mean eval rewards\n",
    "        #2. episode wise trainTime (in seconds): time elapsed during training since the start of the first episode\n",
    "        #3. episode wise wallClockTime (in seconds): actual time elapsed since the start of training,\n",
    "        #                               note this will include time for BookKeeping and evaluation\n",
    "        # Note both trainTime and wallClockTime get accumulated as episodes proceed.\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        resultList, trainTimeList, evalRewardsList, wallClockTimeList = self.trainAgent()\n",
    "        resultEval = self.evaluateAgent()\n",
    "        finalEvalReward  = np.mean(resultEval)\n",
    "\n",
    "\n",
    "        return resultList, trainTimeList, evalRewardsList, wallClockTimeList, finalEvalReward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNmD2LtNgpyr"
   },
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def trainAgent(self):\n",
    "        # this method collects experiences and trains the agent and does BookKeeping while training.\n",
    "        # this calls the trainNetwork() method internally, it also evaluates the agent per episode\n",
    "        # it trains the agent for MAX_TRAIN_EPISODES\n",
    "        training_start = time.time()\n",
    "        torch.manual_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.evaluation_scores = []\n",
    "        self.episode_exploration = []\n",
    "        global train_count\n",
    "        train_count +=1\n",
    "\n",
    "        self.updateNetwork()\n",
    "        self.value_optimizer = self.optimizer\n",
    "\n",
    "        self.replay_buffer = self.replay_buffer_fn()\n",
    "        tempstate, _ = self.env.reset(seed=self.seed)\n",
    "        self.explorationStrategyTrain(self.online_model, tempstate, init=True) #IMPORTANT\n",
    "\n",
    "        max_episodes = self.MAX_TRAIN_EPISODES\n",
    "        result = np.empty((2000, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            print(f'Episode {episode} and train called {train_count} times')\n",
    "            state, _ = self.env.reset(seed=self.seed)\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            for step in count():\n",
    "                state, is_terminal = self.stepint(state, self.env)\n",
    "                min_samples = self.replay_buffer.default_batch_size * self.batchSize\n",
    "                if self.replay_buffer.length() > min_samples:\n",
    "                    experiences = self.replay_buffer.sample()\n",
    "                    experiences = self.online_model.load(experiences)\n",
    "                    self.trainNetwork(experiences, self.epochs)\n",
    "\n",
    "                if np.sum(self.episode_timestep) % self.updateFrequency == 0:\n",
    "                    self.updateNetwork()\n",
    "\n",
    "                if is_terminal:\n",
    "                    break\n",
    "\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            self.episode_seconds.append(episode_elapsed)\n",
    "            training_time += episode_elapsed\n",
    "            evaluation_score = np.mean(self.evaluateAgent())\n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "\n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "\n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode - 1] = total_step, mean_100_reward, mean_100_eval_score, training_time, wallclock_elapsed\n",
    "            reached_max_minutes = wallclock_elapsed >= 10 * 60\n",
    "            reached_max_episodes = episode >= 10000\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= 475\n",
    "            #reached_goal_mean_reward=  False\n",
    "            training_is_over = reached_max_minutes or reached_max_episodes or reached_goal_mean_reward\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes:\n",
    "                    print(u'--> reached_max_minutes ✕')\n",
    "                if reached_max_episodes:\n",
    "                    print(u'--> reached_max_episodes ✕')\n",
    "                if reached_goal_mean_reward:\n",
    "                    print(u'--> reached_goal_mean_reward ✓')\n",
    "                break\n",
    "\n",
    "        final_eval_rwd_list = self.evaluateAgent()\n",
    "        mean_eval_rwd = np.mean(final_eval_rwd_list)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        self.env.close()\n",
    "\n",
    "        return result, training_time, final_eval_rwd_list, wallclock_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFRMzaMngpys"
   },
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def trainNetwork(self, experiences, epochs):\n",
    "        # this method trains the value network epoch number of times and is called by the trainAgent function\n",
    "        # it essentially uses the experiences to calculate target, using the targets it calculates the error, which\n",
    "        # is further used for calulating the loss. It then uses the optimizer over the loss\n",
    "        # to update the params of the network by backpropagating through the network\n",
    "        # this function does not return anything\n",
    "        # you can try out other loss functions other than MSE like Huber loss, MAE, etc.\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "        batch_size = len(is_terminals)\n",
    "\n",
    "        max_a_q_sp = self.target_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        target_q_sa = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals))\n",
    "        q_sa = self.online_model(states).gather(1, actions)\n",
    "\n",
    "        td_error = q_sa - target_q_sa\n",
    "        value_loss = td_error.pow(2).mul(0.5).mean()\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7sjcmfvjgpys"
   },
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def updateNetwork(self):\n",
    "        #this function updates the onlineNetwork with the target network\n",
    "        for target, online in zip(self.target_model.parameters(),\n",
    "                                  self.online_model.parameters()):\n",
    "            target.data.copy_(online.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITW1nHtjgpys"
   },
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def evaluateAgent(self):\n",
    "        #this function evaluates the agent using the value network, it evaluates agent for MAX_EVAL_EPISODES\n",
    "        #typcially MAX_EVAL_EPISODES = 1\n",
    "        rwd_list = []\n",
    "        for _ in range(self.MAX_EVAL_EPISODES):\n",
    "            s, _ = self.env.reset(seed=self.seed)\n",
    "            rwd_list.append(0)\n",
    "            for _ in count():\n",
    "                a = self.explorationStrategyEval(self.online_model, s)\n",
    "                s, rwd, done, truncated,_ = self.env.step(a)\n",
    "                rwd_list[-1] += rwd\n",
    "                if done or truncated: break\n",
    "        return rwd_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Jt1GlMfjxtc",
    "outputId": "dd097086-f262-4897-ab9a-c92a0437a154",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_count = 0\n",
    "dqn_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "seed_list = [420, 133, 74, 317, 233]\n",
    "\n",
    "for myseed in seed_list:\n",
    "\n",
    "    val_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n",
    "\n",
    "    # Instantiation of the DQN class\n",
    "    dqn_instance = DQN(\n",
    "        env_id='CartPole-v1',\n",
    "        seed=myseed,\n",
    "        gamma=0.99,\n",
    "        epochs = 20,\n",
    "        bufferSize=50000,\n",
    "        batchSize=5,\n",
    "        optimizerFn=val_optimizer_fn,\n",
    "        optimizerLR=1e-3,\n",
    "        MAX_TRAIN_EPISODES=300,\n",
    "        MAX_EVAL_EPISODES=1,\n",
    "        explorationStrategyTrainFn=selectEpsilonExpGreedyAction,\n",
    "        explorationStrategyEvalFn=selectGreedyAction,\n",
    "        updateFrequency=10\n",
    "    )\n",
    "\n",
    "    # Running the NFQ method and appending results\n",
    "    trainRewardsList, trainTimeList, evalRewardsList, wallClockTimeList, final_eval_score = dqn_instance.runDQN()\n",
    "    dqn_results.append(trainRewardsList)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = dqn_instance\n",
    "\n",
    "# Convert dqn_results to a numpy array for any further processing\n",
    "dqn_results = np.array(dqn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_max, steps_max, eval_score_max, train_time_max, wall_clock_max = np.max(dqn_results, axis=0).T\n",
    "reward_min, steps_min, eval_score_min, train_time_min, wall_clock_min = np.min(dqn_results, axis=0).T\n",
    "reward_avg, steps_avg, eval_score_avg, train_time_avg, wall_clock_avg = np.mean(dqn_results, axis=0).T\n",
    "episode_indices = np.arange(len(reward_avg))\n",
    "\n",
    "plt.style.use('ggplot')  \n",
    "fig, plot_areas = plt.subplots(5, 1, figsize=(12, 25), sharex='col')\n",
    "fig.subplots_adjust(hspace=0.5)  \n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "titles = ['Total Steps','Training Reward', 'Evaluation reward',  'Training Duration', 'Wall-clock Time']\n",
    "y_labels = ['Steps', 'Reward', 'Reward', 'Seconds', 'Seconds']\n",
    "data_max = [reward_max, steps_max, eval_score_max, train_time_max, wall_clock_max]\n",
    "data_min = [reward_min, steps_min, eval_score_min, train_time_min, wall_clock_min]\n",
    "data_avg = [reward_avg, steps_avg, eval_score_avg, train_time_avg, wall_clock_avg]\n",
    "\n",
    "# Generate plots\n",
    "for ax, title, color, max_data, min_data, avg_data, y_label in zip(plot_areas, titles, colors, data_max, data_min, data_avg, y_labels):\n",
    "    ax.plot(max_data, linestyle='--', color=color, alpha=0.75)\n",
    "    ax.plot(min_data, linestyle='--', color=color, alpha=0.75)\n",
    "    ax.plot(avg_data, label='DQN', color=color, linewidth=2)\n",
    "    ax.fill_between(episode_indices, min_data, max_data, color=color, alpha=0.2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.legend()\n",
    "\n",
    "plot_areas[-1].set_xlabel('Episodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvwbnKOXgpys"
   },
   "source": [
    "## Double DQN (DDQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1x8wUNHJgpys"
   },
   "source": [
    "Implement the Double DQN agent. We have studied about Double DQN agent in the Lecture. Use the function definitions (given below).\n",
    "\n",
    "This class implements the Double DQN agent, you are required to implement the various methods of this class\n",
    "as outlined below. Note this class is generic and should work with any permissible Gym environment\n",
    "\n",
    "```\n",
    "class DDQN():\n",
    "    def __init__(env, seed, gamma,\n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn,\n",
    "                 explorationStrategyEvalFn,\n",
    "                 updateFrequency)\n",
    "    def initBookKeeping(self)\n",
    "    def performBookKeeping(self, train = True)\n",
    "    def runDDQN(self)\n",
    "    def trainAgent(self)\n",
    "    def trainNetwork(self, experiences)\n",
    "    def updateNetwork(self, onlineNet, targetNet)\n",
    "    def evaluateAgent(self)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSYlC9bDgpys"
   },
   "outputs": [],
   "source": [
    "class DDQN():\n",
    "    def __init__(self,env_id, seed, gamma,epochs,\n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 max_gradient_norm,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn,\n",
    "                 explorationStrategyEvalFn,\n",
    "                 updateFrequency):\n",
    "        #this DDQN method\n",
    "        # 1. creates and initializes (with seed) the environment, train/eval episodes, gamma, etc.\n",
    "        # 2. creates and intializes all the variables required for book-keeping values via the initBookKeeping method\n",
    "        # 3. creates tareget and online Q-networks using the createValueNetwork above\n",
    "        # 4. creates and initializes (with network params) the optimizer function\n",
    "        # 5. sets the explorationStartegy variables/functions for train and evaluation\n",
    "        # 6. sets the batchSize for the number of experiences\n",
    "        # 7. Creates the replayBuffer\n",
    "\n",
    "        # 1. Environment initialization using env_id\n",
    "\n",
    "        self.env = gym.make(env_id)\n",
    "        self.seed = seed\n",
    "        reset_env(self.env, seed=seed)  # Seed the environment (affects environment operations)\n",
    "\n",
    "        # Note on seeding:\n",
    "        # Seeding affects global state for NumPy and PyTorch, which might impact other parts of an application.\n",
    "        # It's set here for reproducibility within the scope of this NFQ instance's operations.\n",
    "        torch.manual_seed(seed)  # Seed PyTorch (global effect)\n",
    "        np.random.seed(seed)  # Seed NumPy (global effect)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # 2. Book-keeping initialization\n",
    "        #self.initBookKeeping()\n",
    "\n",
    "        # 3. Creating online Q-network\n",
    "        inDim = self.env.observation_space.shape[0]\n",
    "        outDim = self.env.action_space.n\n",
    "        hDim = [512,128]  # Example hidden layer dimensions, adjust as needed\n",
    "        activation = torch.nn.functional.relu  # Specify the activation function\n",
    "        self.target_model = createValueNetwork(inDim, outDim, hDim, activation)\n",
    "        self.online_model = createValueNetwork(inDim, outDim, hDim, activation)\n",
    "\n",
    "        # 4. Optimizer initialization\n",
    "        self.optimizer = optimizerFn(self.online_model, optimizerLR)\n",
    "\n",
    "        # 5. Exploration strategy setup\n",
    "        self.explorationStrategyTrain = explorationStrategyTrainFn\n",
    "        self.explorationStrategyEval = explorationStrategyEvalFn\n",
    "\n",
    "        # 6. Other variables\n",
    "        self.gamma = gamma\n",
    "        self.epochs = epochs\n",
    "        self.batchSize = batchSize\n",
    "        self.MAX_TRAIN_EPISODES = MAX_TRAIN_EPISODES\n",
    "        self.MAX_EVAL_EPISODES = MAX_EVAL_EPISODES\n",
    "        self.updateFrequency = updateFrequency\n",
    "        self.max_gradient_norm = max_gradient_norm\n",
    "\n",
    "        # 7. Replay buffer\n",
    "        self.replay_buffer_fn = lambda: ReplayBuffer(bufferSize=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpafqOTjgpys"
   },
   "outputs": [],
   "source": [
    "class DDQN(DDQN):\n",
    "    def initBookKeeping(self):\n",
    "        #this method creates and intializes all the variables required for book-keeping values and it is called\n",
    "        #init method\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        #\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIdO1-zngpys"
   },
   "outputs": [],
   "source": [
    "class DDQN(DDQN):\n",
    "    def performBookKeeping(self, train = True):\n",
    "        #this method updates relevant variables for the bookKeeping, this can be called\n",
    "        #multiple times during training\n",
    "        #if you want you can print information using this, so it may help to monitor progress and also help to debug\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        #\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Paoc96Wzjxtd"
   },
   "outputs": [],
   "source": [
    "class DDQN(DDQN):\n",
    "    def stepint(self, state, env):\n",
    "\n",
    "        action = self.explorationStrategyTrain(self.online_model, state)\n",
    "        new_state, reward, is_terminal, is_truncated, info = env.step(action)\n",
    "        is_failure = is_terminal and not is_truncated\n",
    "        experience = (state, action, reward, new_state, float(is_failure))\n",
    "\n",
    "        self.replay_buffer.store(experience)\n",
    "        self.episode_reward[-1] += reward\n",
    "        self.episode_timestep[-1] += 1\n",
    "        #self.episode_exploration[-1] += int(self.training_strategy.exploratory_action_taken)\n",
    "        return new_state, is_terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVs53VQKgpyt"
   },
   "outputs": [],
   "source": [
    "class DDQN(DDQN):\n",
    "    def runDDQN(self):\n",
    "        #this is the main method, it trains the agent, performs bookkeeping while training and finally evaluates\n",
    "        #the agent and returns the following quantities:\n",
    "        #1. episode wise mean train rewards\n",
    "        #2. epsiode wise mean eval rewards\n",
    "        #2. episode wise trainTime (in seconds): time elapsed during training since the start of the first episode\n",
    "        #3. episode wise wallClockTime (in seconds): actual time elapsed since the start of training,\n",
    "        #                               note this will include time for BookKeeping and evaluation\n",
    "        # Note both trainTime and wallClockTime get accumulated as episodes proceed.\n",
    "\n",
    "        #Your code goes in here\n",
    "        resultList, trainTimeList, evalRewardsList, wallClockTimeList = self.trainAgent()\n",
    "        resultEval = self.evaluateAgent()\n",
    "        finalEvalReward  = np.mean(resultEval)\n",
    "\n",
    "        return resultList, trainTimeList, evalRewardsList, wallClockTimeList, finalEvalReward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMt5wYSqmMEQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YosG-HA4gpyt"
   },
   "outputs": [],
   "source": [
    "class DDQN(DDQN):\n",
    "    def trainAgent(self):\n",
    "        #this method collects experiences and trains the agent and does BookKeeping while training.\n",
    "        #this calls the trainNetwork() method internally, it also evaluates the agent per episode\n",
    "        #it trains the agent for MAX_TRAIN_EPISODES\n",
    "        #\n",
    "        training_start = time.time()\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.evaluation_scores = []\n",
    "        self.episode_exploration = []\n",
    "        global train_count\n",
    "        train_count +=1\n",
    "        self.updateNetwork()\n",
    "        self.value_optimizer = self.optimizer\n",
    "        self.replay_buffer = self.replay_buffer_fn()\n",
    "        tempstate, _ = self.env.reset(seed=self.seed)\n",
    "        self.explorationStrategyTrain(self.online_model, tempstate, init=True) #IMPORTANT\n",
    "\n",
    "        max_episodes = self.MAX_TRAIN_EPISODES\n",
    "        result = np.empty((2000, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            print(f'episode {episode} and train called {train_count} times ')\n",
    "            state, _ = self.env.reset(seed=self.seed)\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            for step in count():\n",
    "                state, is_terminal = self.stepint(state, self.env)\n",
    "\n",
    "                # Debug: Log Q-values\n",
    "                with torch.no_grad():\n",
    "                  current_Q_values = self.online_model(state).cpu().detach().numpy().squeeze()\n",
    "                  #print(f\"Debug: Episode {episode}, Step {step}, Q-values: {current_Q_values}\")\n",
    "\n",
    "\n",
    "                min_samples = self.replay_buffer.default_batch_size * self.batchSize\n",
    "                if self.replay_buffer.length() > min_samples:\n",
    "                    experiences = self.replay_buffer.sample()\n",
    "                    experiences = self.online_model.load(experiences)\n",
    "                    self.trainNetwork(experiences, self.epochs)\n",
    "\n",
    "                if np.sum(self.episode_timestep) % self.updateFrequency == 0:\n",
    "                    self.updateNetwork()\n",
    "\n",
    "                if is_terminal:\n",
    "                    break\n",
    "\n",
    "\n",
    "            # Debug: Log replay buffer size\n",
    "            #print(f\"Episode: {episode}, Replay Buffer Size: {self.replay_buffer.length()}\")\n",
    "\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            self.episode_seconds.append(episode_elapsed)\n",
    "            training_time += episode_elapsed\n",
    "            #print('first_evaluate_entered')\n",
    "            evaluation_score= np.mean(self.evaluateAgent())\n",
    "            #print('first_evaluate_exit')\n",
    "\n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "\n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            #lst_100_exp_rat = np.array(self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
    "            #mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
    "            #std_100_exp_rat = np.std(lst_100_exp_rat)\n",
    "\n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode-1] = total_step, mean_100_reward, mean_100_eval_score, training_time, wallclock_elapsed\n",
    "            reached_max_minutes = wallclock_elapsed >= 30 * 60\n",
    "            reached_max_episodes = episode >= 10000\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= 475\n",
    "            training_is_over = reached_max_minutes or reached_max_episodes or reached_goal_mean_reward\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "\n",
    "        final_eval_rwd_list = self.evaluateAgent()\n",
    "        mean_eval_rwd = np.mean(final_eval_rwd_list)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        self.env.close()\n",
    "\n",
    "        return result, training_time, final_eval_rwd_list, wallclock_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fT6-kmzHgpyt"
   },
   "outputs": [],
   "source": [
    "class DDQN(DDQN):\n",
    "    def trainNetwork(self, experiences, epochs):\n",
    "        # this method trains the value network epoch number of times and is called by the trainAgent function\n",
    "        # it essentially uses the experiences to calculate target, using the targets it calculates the error, which\n",
    "        # is further used for calulating the loss. It then uses the optimizer over the loss\n",
    "        # to update the params of the network by backpropagating through the network\n",
    "        # this function does not return anything\n",
    "        # you can try out other loss functions other than MSE like Huber loss, MAE, etc.\n",
    "\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "        batch_size = len(is_terminals)\n",
    "\n",
    "        # argmax_a_q_sp = self.target_model(next_states).max(1)[1]\n",
    "        argmax_a_q_sp = self.online_model(next_states).max(1)[1]\n",
    "        q_sp = self.target_model(next_states).detach()\n",
    "        max_a_q_sp = q_sp[\n",
    "            np.arange(batch_size), argmax_a_q_sp].unsqueeze(1)\n",
    "        target_q_sa = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals))\n",
    "        q_sa = self.online_model(states).gather(1, actions)\n",
    "\n",
    "        td_error = q_sa - target_q_sa\n",
    "        value_loss = td_error.pow(2).mul(0.5).mean()\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_model.parameters(),\n",
    "                                       self.max_gradient_norm)\n",
    "        self.value_optimizer.step()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EZWcSERgpyt"
   },
   "outputs": [],
   "source": [
    "class DDQN(DDQN):\n",
    "    def updateNetwork(self):\n",
    "        for target, online in zip(self.target_model.parameters(), self.online_model.parameters()):\n",
    "            target.data.copy_(online.data)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IHpGtqZgpyt"
   },
   "outputs": [],
   "source": [
    "class DDQN(DDQN):\n",
    "    def evaluateAgent(self):\n",
    "        rwd_list = []\n",
    "        for eval_episode in range(self.MAX_EVAL_EPISODES):\n",
    "            s, _ = self.env.reset(seed=self.seed)\n",
    "            total_reward = 0\n",
    "            rwd_list.append(0)\n",
    "            for step in count():\n",
    "                a = self.explorationStrategyEval(self.online_model, s)\n",
    "                s, rwd, done, truncated,_ = self.env.step(a)\n",
    "                total_reward += rwd\n",
    "\n",
    "                # Debug: Log action, reward, and step\n",
    "                #print(f\"Evaluation Episode {eval_episode + 1}, Step {step}: State {s}, Action {a}, Reward {rwd}, Total Reward {total_reward}\")\n",
    "\n",
    "                rwd_list[-1] += rwd\n",
    "                if done or truncated: break\n",
    "        return rwd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OIB8j07mQdR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XxlEmuW0jxte",
    "outputId": "04d8ec46-5bc3-401f-b211-cccf2b613241"
   },
   "outputs": [],
   "source": [
    "train_count=0\n",
    "ddqn_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "seed_list = [420, 133, 74, 317, 233]\n",
    "\n",
    "for myseed in seed_list:\n",
    "\n",
    "    val_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n",
    "\n",
    "    # Instantiation of the DQN class\n",
    "    ddqn_instance = DDQN(\n",
    "        env_id='CartPole-v1',\n",
    "        seed=myseed,\n",
    "        gamma=0.99,\n",
    "        epochs = 20,\n",
    "        bufferSize=50000,\n",
    "        batchSize=5,\n",
    "        optimizerFn=val_optimizer_fn,\n",
    "        optimizerLR=1e-3,\n",
    "        max_gradient_norm = float('inf'),\n",
    "        MAX_TRAIN_EPISODES=500,\n",
    "        MAX_EVAL_EPISODES=1,\n",
    "        explorationStrategyTrainFn=selectEpsilonExpGreedyAction,\n",
    "        explorationStrategyEvalFn=selectGreedyAction,\n",
    "        updateFrequency=10\n",
    "    )\n",
    "\n",
    "    # Running the NFQ method and appending results\n",
    "    trainRewardsList, trainTimeList, evalRewardsList, wallClockTimeList, final_eval_score = ddqn_instance.runDDQN()\n",
    "    ddqn_results.append(trainRewardsList)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = ddqn_instance\n",
    "\n",
    "# Convert dqn_results to a numpy array for any further processing\n",
    "ddqn_results = np.array(ddqn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_max, steps_max, eval_score_max, train_time_max, wall_clock_max = np.max(ddqn_results, axis=0).T\n",
    "reward_min, steps_min, eval_score_min, train_time_min, wall_clock_min = np.min(ddqn_results, axis=0).T\n",
    "reward_avg, steps_avg, eval_score_avg, train_time_avg, wall_clock_avg = np.mean(ddqn_results, axis=0).T\n",
    "episode_indices = np.arange(len(reward_avg))\n",
    "\n",
    "plt.style.use('ggplot')  \n",
    "fig, plot_areas = plt.subplots(5, 1, figsize=(12, 25), sharex='col')\n",
    "fig.subplots_adjust(hspace=0.5)  \n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "titles = ['Total Steps','Training Reward', 'Evaluation reward',  'Training Duration', 'Wall-clock Time']\n",
    "y_labels = ['Steps', 'Reward', 'Reward', 'Seconds', 'Seconds']\n",
    "data_max = [reward_max, steps_max, eval_score_max, train_time_max, wall_clock_max]\n",
    "data_min = [reward_min, steps_min, eval_score_min, train_time_min, wall_clock_min]\n",
    "data_avg = [reward_avg, steps_avg, eval_score_avg, train_time_avg, wall_clock_avg]\n",
    "\n",
    "# Generate plots\n",
    "for ax, title, color, max_data, min_data, avg_data, y_label in zip(plot_areas, titles, colors, data_max, data_min, data_avg, y_labels):\n",
    "    ax.plot(max_data, linestyle='--', color=color, alpha=0.75)\n",
    "    ax.plot(min_data, linestyle='--', color=color, alpha=0.75)\n",
    "    ax.plot(avg_data, label='Average', color=color, linewidth=2)\n",
    "    ax.fill_between(episode_indices, min_data, max_data, color=color, alpha=0.2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.legend()\n",
    "\n",
    "plot_areas[-1].set_xlabel('Episodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vG3PPiKEgpyt"
   },
   "source": [
    "## Dueling DDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGCuTt1Zgpyt"
   },
   "source": [
    "Implement the Dueling Double Deep Q algorithm. We have studied about Dueling Double DQN agent in the Lecture. Use the function definitions (given below).\n",
    "\n",
    "This class implements the Dueling Double DQN agent, you are required to implement the various methods of this class\n",
    "as outlined below. Note this class is generic and should work with any permissible Gym environment\n",
    "\n",
    "```\n",
    "class D3QN():\n",
    "    def __init__(env, seed, gamma, tau,\n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn,\n",
    "                 explorationStrategyEvalFn,\n",
    "                 updateFrequency)\n",
    "    def initBookKeeping(self)\n",
    "    def performBookKeeping(self, train = True)\n",
    "    def runD3QN(self)\n",
    "    def trainAgent(self)\n",
    "    def trainNetwork(self, experiences)\n",
    "    def updateNetwork(self, onlineNet, targetNet)\n",
    "    def evaluateAgent(self)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o74zJTrogpyu"
   },
   "outputs": [],
   "source": [
    "class D3QN():\n",
    "    def __init__(self,env_id, seed, gamma,epochs,\n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 max_gradient_norm,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn,\n",
    "                 explorationStrategyEvalFn,\n",
    "                 updateFrequency, tau):\n",
    "        \n",
    "        #this D3QN method\n",
    "        # 1. creates and initializes (with seed) the environment, train/eval episodes, gamma, etc.\n",
    "        # 2. creates and intializes all the variables required for book-keeping values via the initBookKeeping method\n",
    "        # 3. creates tareget and online Q-networks using the createValueNetwork above\n",
    "        # 4. creates and initializes (with network params) the optimizer function\n",
    "        # 5. sets the explorationStartegy variables/functions for train and evaluation\n",
    "        # 6. sets the batchSize for the number of experiences\n",
    "        # 7. Creates the replayBuffer\n",
    "\n",
    "        #Your code goes in here\n",
    "\n",
    "        # 1. Environment initialization using env_id\n",
    "\n",
    "        self.env = gym.make(env_id)\n",
    "        self.seed = seed\n",
    "        reset_env(self.env, seed=seed)  # Seed the environment (affects environment operations)\n",
    "\n",
    "        # Note on seeding:\n",
    "        # Seeding affects global state for NumPy and PyTorch, which might impact other parts of an application.\n",
    "        # It's set here for reproducibility within the scope of this NFQ instance's operations.\n",
    "        torch.manual_seed(seed)  # Seed PyTorch (global effect)\n",
    "        np.random.seed(seed)  # Seed NumPy (global effect)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # 2. Book-keeping initialization\n",
    "        #self.initBookKeeping()\n",
    "\n",
    "        # 3. Creating online Q-network\n",
    "        inDim = self.env.observation_space.shape[0]\n",
    "        outDim = self.env.action_space.n\n",
    "        hDim = [512,128]  # Example hidden layer dimensions, adjust as needed\n",
    "        activation = torch.nn.functional.relu  # Specify the activation function\n",
    "        self.target_model = createDuelingNetwork(inDim, outDim, hDim, activation)\n",
    "        self.online_model = createDuelingNetwork(inDim, outDim, hDim, activation)\n",
    "\n",
    "        # 4. Optimizer initialization\n",
    "        self.optimizer = optimizerFn(self.online_model, optimizerLR)\n",
    "\n",
    "        # 5. Exploration strategy setup\n",
    "        self.explorationStrategyTrain = explorationStrategyTrainFn\n",
    "        self.explorationStrategyEval = explorationStrategyEvalFn\n",
    "\n",
    "        # 6. Other variables\n",
    "        self.gamma = gamma\n",
    "        self.epochs = epochs\n",
    "        self.batchSize = batchSize\n",
    "        self.MAX_TRAIN_EPISODES = MAX_TRAIN_EPISODES\n",
    "        self.MAX_EVAL_EPISODES = MAX_EVAL_EPISODES\n",
    "        self.updateFrequency = updateFrequency\n",
    "        self.max_gradient_norm = max_gradient_norm\n",
    "        self.tau = tau\n",
    "\n",
    "        # 7. Replay buffer\n",
    "        self.replay_buffer_fn = lambda: ReplayBuffer(bufferSize=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUI3CgvXgpyu"
   },
   "outputs": [],
   "source": [
    "class D3QN(D3QN):\n",
    "    def initBookKeeping(self):\n",
    "        #this method creates and intializes all the variables required for book-keeping values and it is called\n",
    "        #init method\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        #\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRBojBhYgpyu"
   },
   "outputs": [],
   "source": [
    "class D3QN(D3QN):\n",
    "    def performBookKeeping(self, train = True):\n",
    "        #this method updates relevant variables for the bookKeeping, this can be called\n",
    "        #multiple times during training\n",
    "        #if you want you can print information using this, so it may help to monitor progress and also help to debug\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        #\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D3QN(D3QN):\n",
    "    def stepint(self, state, env):\n",
    "\n",
    "        action = self.explorationStrategyTrain(self.online_model, state)\n",
    "        new_state, reward, is_terminal, is_truncated, info = env.step(action)\n",
    "        is_failure = is_terminal and not is_truncated\n",
    "        experience = (state, action, reward, new_state, float(is_failure))\n",
    "        self.replay_buffer.store(experience)\n",
    "        self.episode_reward[-1] += reward\n",
    "        self.episode_timestep[-1] += 1\n",
    "        #self.episode_exploration[-1] += int(self.training_strategy.exploratory_action_taken)\n",
    "        return new_state, is_terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wD9Yy4Dggpyu"
   },
   "outputs": [],
   "source": [
    "class D3QN(D3QN):\n",
    "    def runD3QN(self):\n",
    "        #this is the main method, it trains the agent, performs bookkeeping while training and finally evaluates\n",
    "        #the agent and returns the following quantities:\n",
    "        #1. episode wise mean train rewards\n",
    "        #2. epsiode wise mean eval rewards\n",
    "        #2. episode wise trainTime (in seconds): time elapsed during training since the start of the first episode\n",
    "        #3. episode wise wallClockTime (in seconds): actual time elapsed since the start of training,\n",
    "        #                               note this will include time for BookKeeping and evaluation\n",
    "        # Note both trainTime and wallClockTime get accumulated as episodes proceed.\n",
    "\n",
    "        #Your code goes in here\n",
    "        resultList, trainTimeList, evalRewardsList, wallClockTimeList = self.trainAgent()\n",
    "        resultEval = self.evaluateAgent()\n",
    "        finalEvalReward  = np.mean(resultEval)\n",
    "\n",
    "        return resultList, trainTimeList, evalRewardsList, wallClockTimeList, finalEvalReward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohnMqUBPgpyu"
   },
   "outputs": [],
   "source": [
    "class D3QN(D3QN):\n",
    "    def trainAgent(self):\n",
    "        #this method collects experiences and trains the agent and does BookKeeping while training.\n",
    "        #this calls the trainNetwork() method internally, it also evaluates the agent per episode\n",
    "        #it trains the agent for MAX_TRAIN_EPISODES\n",
    "        #\n",
    "        training_start = time.time()\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.evaluation_scores = []\n",
    "        self.episode_exploration = []\n",
    "        global train_count\n",
    "        train_count +=1\n",
    "        self.updateNetwork(tau=1.0)\n",
    "        self.value_optimizer = self.optimizer\n",
    "        self.replay_buffer = self.replay_buffer_fn()\n",
    "        tempstate, _ = self.env.reset(seed=self.seed)\n",
    "        self.explorationStrategyTrain(self.online_model, tempstate, init=True) #IMPORTANT\n",
    "\n",
    "        max_episodes = self.MAX_TRAIN_EPISODES\n",
    "        result = np.empty((2000, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            print(f'episode {episode} and train called {train_count} times ')\n",
    "            state, _ = self.env.reset(seed=self.seed)\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            for step in count():\n",
    "                state, is_terminal = self.stepint(state, self.env)\n",
    "\n",
    "                # Debug: Log Q-values\n",
    "                with torch.no_grad():\n",
    "                  current_Q_values = self.online_model(state).cpu().detach().numpy().squeeze()\n",
    "                  #print(f\"Debug: Episode {episode}, Step {step}, Q-values: {current_Q_values}\")\n",
    "\n",
    "\n",
    "                min_samples = self.replay_buffer.default_batch_size * self.batchSize\n",
    "                if self.replay_buffer.length() > min_samples:\n",
    "                    experiences = self.replay_buffer.sample()\n",
    "                    experiences = self.online_model.load(experiences)\n",
    "                    self.trainNetwork(experiences, self.epochs)\n",
    "\n",
    "                if np.sum(self.episode_timestep) % self.updateFrequency == 0:\n",
    "                    self.updateNetwork()\n",
    "\n",
    "                if is_terminal:\n",
    "                    break\n",
    "\n",
    "\n",
    "            # Debug: Log replay buffer size\n",
    "            #print(f\"Episode: {episode}, Replay Buffer Size: {self.replay_buffer.length()}\")\n",
    "\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            self.episode_seconds.append(episode_elapsed)\n",
    "            training_time += episode_elapsed\n",
    "            #print('first_evaluate_entered')\n",
    "            evaluation_score= np.mean(self.evaluateAgent())\n",
    "            #print('first_evaluate_exit')\n",
    "\n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "\n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            #lst_100_exp_rat = np.array(self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
    "            #mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
    "            #std_100_exp_rat = np.std(lst_100_exp_rat)\n",
    "\n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode-1] = total_step, mean_100_reward, mean_100_eval_score, training_time, wallclock_elapsed\n",
    "            reached_max_minutes = wallclock_elapsed >= 30 * 60\n",
    "            reached_max_episodes = episode >= 10000\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= 475\n",
    "            training_is_over = reached_max_minutes or reached_max_episodes or reached_goal_mean_reward\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "\n",
    "        final_eval_rwd_list = self.evaluateAgent()\n",
    "        mean_eval_rwd = np.mean(final_eval_rwd_list)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        self.env.close()\n",
    "\n",
    "        return result, training_time, final_eval_rwd_list, wallclock_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RI8bFvngpyu"
   },
   "outputs": [],
   "source": [
    "class D3QN(D3QN):\n",
    "    def trainNetwork(self, experiences, epochs):\n",
    "        # this method trains the value network epoch number of times and is called by the trainAgent function\n",
    "        # it essentially uses the experiences to calculate target, using the targets it calculates the error, which\n",
    "        # is further used for calulating the loss. It then uses the optimizer over the loss\n",
    "        # to update the params of the network by backpropagating through the network\n",
    "        # this function does not return anything\n",
    "        # you can try out other loss functions other than MSE like Huber loss, MAE, etc.\n",
    "\n",
    "        #Your code goes in here\n",
    "        \n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "        batch_size = len(is_terminals)\n",
    "\n",
    "        argmax_a_q_sp = self.online_model(next_states).max(1)[1]\n",
    "        q_sp = self.target_model(next_states).detach()\n",
    "        max_a_q_sp = q_sp[\n",
    "            np.arange(batch_size), argmax_a_q_sp].unsqueeze(1)\n",
    "        target_q_sa = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals))\n",
    "        q_sa = self.online_model(states).gather(1, actions)\n",
    "\n",
    "        td_error = q_sa - target_q_sa\n",
    "        value_loss = td_error.pow(2).mul(0.5).mean()\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()        \n",
    "        torch.nn.utils.clip_grad_norm_(self.online_model.parameters(), \n",
    "                                       self.max_gradient_norm)\n",
    "        self.value_optimizer.step()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UaafuY1mgpyu"
   },
   "outputs": [],
   "source": [
    "class D3QN(D3QN):\n",
    "    def updateNetwork(self, tau=None):\n",
    "        #this function updates the onlineNetwork with the target network using Polyak averaging\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        tau = self.tau if tau is None else tau\n",
    "        for target, online in zip(self.target_model.parameters(), \n",
    "                                  self.online_model.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "            \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNRYslhMgpyu"
   },
   "outputs": [],
   "source": [
    "class D3QN(D3QN):\n",
    "    def evaluateAgent(self):\n",
    "        rwd_list = []\n",
    "        for eval_episode in range(self.MAX_EVAL_EPISODES):\n",
    "            s, _ = self.env.reset(seed=self.seed)\n",
    "            total_reward = 0\n",
    "            rwd_list.append(0)\n",
    "            for step in count():\n",
    "                a = self.explorationStrategyEval(self.online_model, s)\n",
    "                s, rwd, done, truncated,_ = self.env.step(a)\n",
    "                total_reward += rwd\n",
    "\n",
    "                rwd_list[-1] += rwd\n",
    "                if done or truncated: break\n",
    "        return rwd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count=0\n",
    "d3qn_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "seed_list = [420, 133, 74, 317, 233]\n",
    "\n",
    "for myseed in seed_list:\n",
    "\n",
    "    val_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n",
    "\n",
    "    # Instantiation of the D3QN class\n",
    "    d3qn_instance = D3QN(\n",
    "        env_id='CartPole-v1',\n",
    "        seed=myseed,\n",
    "        gamma=0.99,\n",
    "        epochs = 40,\n",
    "        bufferSize=50000,\n",
    "        batchSize=5,\n",
    "        optimizerFn=val_optimizer_fn,\n",
    "        optimizerLR=0.0005,\n",
    "        max_gradient_norm = float('inf'),\n",
    "        MAX_TRAIN_EPISODES=300,\n",
    "        MAX_EVAL_EPISODES=1,\n",
    "        explorationStrategyTrainFn=selectEpsilonExpGreedyAction,\n",
    "        explorationStrategyEvalFn=selectGreedyAction,\n",
    "        updateFrequency=1,\n",
    "        tau = 0.1\n",
    "    )\n",
    "\n",
    "    # Running the NFQ method and appending results\n",
    "    trainRewardsList, trainTimeList, evalRewardsList, wallClockTimeList, final_eval_score = d3qn_instance.runD3QN()\n",
    "    d3qn_results.append(trainRewardsList)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = d3qn_instance\n",
    "\n",
    "# Convert dqn_results to a numpy array for any further processing\n",
    "d3qn_results = np.array(d3qn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_max, steps_max, eval_score_max, train_time_max, wall_clock_max = np.max(d3qn_results, axis=0).T\n",
    "reward_min, steps_min, eval_score_min, train_time_min, wall_clock_min = np.min(d3qn_results, axis=0).T\n",
    "reward_avg, steps_avg, eval_score_avg, train_time_avg, wall_clock_avg = np.mean(d3qn_results, axis=0).T\n",
    "episode_indices = np.arange(len(reward_avg))\n",
    "\n",
    "plt.style.use('ggplot')  \n",
    "fig, plot_areas = plt.subplots(5, 1, figsize=(12, 25), sharex='col')\n",
    "fig.subplots_adjust(hspace=0.5)  \n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "titles = ['Total Steps','Training Reward', 'Evaluation reward',  'Training Duration', 'Wall-clock Time']\n",
    "y_labels = ['Steps', 'Reward', 'Reward', 'Seconds', 'Seconds']\n",
    "data_max = [reward_max, steps_max, eval_score_max, train_time_max, wall_clock_max]\n",
    "data_min = [reward_min, steps_min, eval_score_min, train_time_min, wall_clock_min]\n",
    "data_avg = [reward_avg, steps_avg, eval_score_avg, train_time_avg, wall_clock_avg]\n",
    "\n",
    "# Generate plots\n",
    "for ax, title, color, max_data, min_data, avg_data, y_label in zip(plot_areas, titles, colors, data_max, data_min, data_avg, y_labels):\n",
    "    ax.plot(max_data, linestyle='--', color=color, alpha=0.75)\n",
    "    ax.plot(min_data, linestyle='--', color=color, alpha=0.75)\n",
    "    ax.plot(avg_data, label='D3QN', color=color, linewidth=2)\n",
    "    ax.fill_between(episode_indices, min_data, max_data, color=color, alpha=0.2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.legend()\n",
    "\n",
    "plot_areas[-1].set_xlabel('Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vK7vjFfgpyv"
   },
   "source": [
    "## Dueling Double Deep Q Network with Prioritized Experience Replay (D3QN-PER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUnveQ6zgpyv"
   },
   "source": [
    "Implement the Dueling Double DQN with Prioritized Experience Replay (D3QN-PER) agent. We have studied about D3QN-PER agent in the Lecture. Use the function definitions (given below).\n",
    "\n",
    "This class implements the D3QN-PER agent, you are required to implement the various methods of this class\n",
    "as outlined below. Note this class is generic and should work with any permissible Gym environment\n",
    "\n",
    "```\n",
    "class D3QN_PER():\n",
    "    def __init__(env, seed, gamma, tau, alpha, beta, beta_rate,\n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn,\n",
    "                 explorationStrategyEvalFn,\n",
    "                 updateFrequency)\n",
    "    def initBookKeeping(self)\n",
    "    def performBookKeeping(self, train = True)\n",
    "    def runD3QN_PER(self)\n",
    "    def trainAgent(self)\n",
    "    def trainNetwork(self, experiences)\n",
    "    def updateNetwork(self, onlineNet, targetNet)\n",
    "    def evaluateAgent(self)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acxIlcQ8gpyv"
   },
   "outputs": [],
   "source": [
    "class D3QN_PER():\n",
    "    def __init__(self,env_id, seed, gamma,epochs,\n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 max_gradient_norm,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn,\n",
    "                 explorationStrategyEvalFn,\n",
    "                 updateFrequency, tau):\n",
    "        \n",
    "        #thisD3QN_PER method\n",
    "        # 1. creates and initializes (with seed) the environment, train/eval episodes, gamma, etc.\n",
    "        # 2. creates and intializes all the variables required for book-keeping values via the initBookKeeping method\n",
    "        # 3. creates tareget and online Q-networks using the createValueNetwork above\n",
    "        # 4. creates and initializes (with network params) the optimizer function\n",
    "        # 5. sets the explorationStartegy variables/functions for train and evaluation\n",
    "        # 6. sets the batchSize for the number of experiences\n",
    "        # 7. Creates the replayBuffer\n",
    "\n",
    "        #Your code goes in here\n",
    "\n",
    "        # 1. Environment initialization using env_id\n",
    "\n",
    "        self.env = gym.make(env_id)\n",
    "        self.seed = seed\n",
    "        reset_env(self.env, seed=seed)  # Seed the environment (affects environment operations)\n",
    "\n",
    "        # Note on seeding:\n",
    "        # Seeding affects global state for NumPy and PyTorch, which might impact other parts of an application.\n",
    "        # It's set here for reproducibility within the scope of this NFQ instance's operations.\n",
    "        torch.manual_seed(seed)  # Seed PyTorch (global effect)\n",
    "        np.random.seed(seed)  # Seed NumPy (global effect)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # 2. Book-keeping initialization\n",
    "        #self.initBookKeeping()\n",
    "\n",
    "        # 3. Creating online Q-network\n",
    "        inDim = self.env.observation_space.shape[0]\n",
    "        outDim = self.env.action_space.n\n",
    "        hDim = [512,128]  # Example hidden layer dimensions, adjust as needed\n",
    "        activation = torch.nn.functional.relu  # Specify the activation function\n",
    "        self.target_model = createDuelingNetwork(inDim, outDim, hDim, activation)\n",
    "        self.online_model = createDuelingNetwork(inDim, outDim, hDim, activation)\n",
    "\n",
    "        # 4. Optimizer initialization\n",
    "        self.optimizer = optimizerFn(self.online_model, optimizerLR)\n",
    "\n",
    "        # 5. Exploration strategy setup\n",
    "        self.explorationStrategyTrain = explorationStrategyTrainFn\n",
    "        self.explorationStrategyEval = explorationStrategyEvalFn\n",
    "\n",
    "        # 6. Other variables\n",
    "        self.gamma = gamma\n",
    "        self.epochs = epochs\n",
    "        self.batchSize = batchSize\n",
    "        self.MAX_TRAIN_EPISODES = MAX_TRAIN_EPISODES\n",
    "        self.MAX_EVAL_EPISODES = MAX_EVAL_EPISODES\n",
    "        self.updateFrequency = updateFrequency\n",
    "        self.max_gradient_norm = max_gradient_norm\n",
    "        self.tau = tau\n",
    "\n",
    "        # 7. Replay buffer\n",
    "        self.replay_buffer_fn = lambda: ReplayBuffer(bufferSize=20000, bufferType = \"D3QN-PER\", max_samples=bufferSize, rank_based=True,alpha=0.6, beta=0.1, beta_rate=0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qjTeiwygpyv"
   },
   "outputs": [],
   "source": [
    "class D3QN_PER(D3QN_PER):\n",
    "    def initBookKeeping(self):\n",
    "        #this method creates and intializes all the variables required for book-keeping values and it is called\n",
    "        #init method\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        #\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UT-LZK9vgpyv"
   },
   "outputs": [],
   "source": [
    "class D3QN_PER(D3QN_PER):\n",
    "    def performBookKeeping(self, train = True):\n",
    "        #this method updates relevant variables for the bookKeeping, this can be called\n",
    "        #multiple times during training\n",
    "        #if you want you can print information using this, so it may help to monitor progress and also help to debug\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        #\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D3QN_PER(D3QN_PER):\n",
    "    \n",
    "    def stepint(self, state, env):\n",
    "\n",
    "        action = self.explorationStrategyTrain(self.online_model, state)\n",
    "        new_state, reward, is_terminal, is_truncated, info = env.step(action)\n",
    "        is_failure = is_terminal and not is_truncated\n",
    "        experience = (state, action, reward, new_state, float(is_failure))\n",
    "        self.replay_buffer.store(experience)\n",
    "        self.episode_reward[-1] += reward\n",
    "        self.episode_timestep[-1] += 1\n",
    "        #self.episode_exploration[-1] += int(self.training_strategy.exploratory_action_taken)\n",
    "        return new_state, is_terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYAmSr0qgpyv"
   },
   "outputs": [],
   "source": [
    "class D3QN_PER(D3QN_PER):\n",
    "    def runD3QN_PER(self):\n",
    "        #this is the main method, it trains the agent, performs bookkeeping while training and finally evaluates\n",
    "        #the agent and returns the following quantities:\n",
    "        #1. episode wise mean train rewards\n",
    "        #2. epsiode wise mean eval rewards\n",
    "        #2. episode wise trainTime (in seconds): time elapsed during training since the start of the first episode\n",
    "        #3. episode wise wallClockTime (in seconds): actual time elapsed since the start of training,\n",
    "        #                               note this will include time for BookKeeping and evaluation\n",
    "        # Note both trainTime and wallClockTime get accumulated as episodes proceed.\n",
    "\n",
    "        resultList, trainTimeList, evalRewardsList, wallClockTimeList = self.trainAgent()\n",
    "        resultEval = self.evaluateAgent()\n",
    "        finalEvalReward  = np.mean(resultEval)\n",
    "\n",
    "        return resultList, trainTimeList, evalRewardsList, wallClockTimeList, finalEvalReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_D4MOSn0gpyv"
   },
   "outputs": [],
   "source": [
    "class D3QN_PER(D3QN_PER):\n",
    "\n",
    "    def trainAgent(self):\n",
    "        #this method collects experiences and trains the agent and does BookKeeping while training.\n",
    "        #this calls the trainNetwork() method internally, it also evaluates the agent per episode\n",
    "        #it trains the agent for MAX_TRAIN_EPISODES\n",
    "        #\n",
    "        training_start = time.time()\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.evaluation_scores = []\n",
    "        self.episode_exploration = []\n",
    "        global train_count\n",
    "        train_count +=1\n",
    "        self.updateNetwork(tau=1.0)\n",
    "        self.value_optimizer = self.optimizer\n",
    "        self.replay_buffer = self.replay_buffer_fn()\n",
    "        tempstate, _ = self.env.reset(seed=self.seed)\n",
    "        self.explorationStrategyTrain(self.online_model, tempstate, init=True) #IMPORTANT\n",
    "\n",
    "        max_episodes = self.MAX_TRAIN_EPISODES\n",
    "        result = np.empty((2000, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            print(f'episode {episode} and train called {train_count} times ')\n",
    "            state, _ = self.env.reset(seed=self.seed)\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            for step in count():\n",
    "                state, is_terminal = self.stepint(state, self.env)\n",
    "\n",
    "                # Debug: Log Q-values\n",
    "                with torch.no_grad():\n",
    "                  current_Q_values = self.online_model(state).cpu().detach().numpy().squeeze()\n",
    "                  #print(f\"Debug: Episode {episode}, Step {step}, Q-values: {current_Q_values}\")\n",
    "\n",
    "\n",
    "                min_samples = self.replay_buffer.batch_size * self.batchSize\n",
    "                if self.replay_buffer.length() > min_samples:\n",
    "                    idxs, weights, samples = self.replay_buffer.sample()\n",
    "                    experiences = self.online_model.load(samples)\n",
    "                    experiences = (idxs, weights) + (experiences,)\n",
    "                    self.trainNetwork(experiences)\n",
    "\n",
    "                if np.sum(self.episode_timestep) % self.updateFrequency == 0:\n",
    "                    self.updateNetwork()\n",
    "\n",
    "                if is_terminal:\n",
    "                    break\n",
    "\n",
    "\n",
    "            # Debug: Log replay buffer size\n",
    "            #print(f\"Episode: {episode}, Replay Buffer Size: {self.replay_buffer.length()}\")\n",
    "\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            self.episode_seconds.append(episode_elapsed)\n",
    "            training_time += episode_elapsed\n",
    "            #print('first_evaluate_entered')\n",
    "            evaluation_score= np.mean(self.evaluateAgent())\n",
    "            #print('first_evaluate_exit')\n",
    "\n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "\n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            #lst_100_exp_rat = np.array(self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
    "            #mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
    "            #std_100_exp_rat = np.std(lst_100_exp_rat)\n",
    "\n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode-1] = total_step, mean_100_reward, mean_100_eval_score, training_time, wallclock_elapsed\n",
    "            reached_max_minutes = wallclock_elapsed >= 30 * 60\n",
    "            reached_max_episodes = episode >= 10000\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= 475\n",
    "            training_is_over = reached_max_minutes or reached_max_episodes or reached_goal_mean_reward\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "\n",
    "        final_eval_rwd_list = self.evaluateAgent()\n",
    "        mean_eval_rwd = np.mean(final_eval_rwd_list)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        self.env.close()\n",
    "\n",
    "        return result, training_time, final_eval_rwd_list, wallclock_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cM155lGOgpyw"
   },
   "outputs": [],
   "source": [
    "class D3QN_PER(D3QN_PER):\n",
    "    def trainNetwork(self, experiences):\n",
    "        # this method trains the value network epoch number of times and is called by the trainAgent function\n",
    "        # it essentially uses the experiences to calculate target, using the targets it calculates the error, which\n",
    "        # is further used for calulating the loss. It then uses the optimizer over the loss\n",
    "        # to update the params of the network by backpropagating through the network\n",
    "        # this function does not return anything\n",
    "        # you can try out other loss functions other than MSE like Huber loss, MAE, etc.\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        idxs, weights, (states, actions, rewards, next_states, is_terminals) = experiences\n",
    "        weights = self.online_model.numpy_float_to_device(weights)\n",
    "        batch_size = len(is_terminals)\n",
    "        \n",
    "        argmax_a_q_sp = self.online_model(next_states).max(1)[1]\n",
    "        q_sp = self.target_model(next_states).detach()\n",
    "        max_a_q_sp = q_sp[np.arange(batch_size), argmax_a_q_sp].unsqueeze(1)\n",
    "        target_q_sa = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals))\n",
    "        q_sa = self.online_model(states).gather(1, actions)\n",
    "\n",
    "        td_error = q_sa - target_q_sa\n",
    "        value_loss = (weights * td_error).pow(2).mul(0.5).mean()\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()        \n",
    "        torch.nn.utils.clip_grad_norm_(self.online_model.parameters(), \n",
    "                                       self.max_gradient_norm)\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "        priorities = np.abs(td_error.detach().cpu().numpy())\n",
    "        self.replay_buffer.update(idxs, priorities)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_glmZ3_Jgpyw"
   },
   "outputs": [],
   "source": [
    "class D3QN_PER(D3QN_PER):\n",
    "    def updateNetwork(self, tau=None):\n",
    "        #this function updates the onlineNetwork with the target network using Polyak averaging \n",
    "        tau = self.tau if tau is None else tau\n",
    "        for target, online in zip(self.target_model.parameters(), \n",
    "                                  self.online_model.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgFxuwXMgpyw"
   },
   "outputs": [],
   "source": [
    "class D3QN_PER(D3QN_PER):\n",
    "    def evaluateAgent(self):\n",
    "        #this function evaluates the agent using the value network, it evaluates agent for MAX_EVAL_EPISODES\n",
    "        #typcially MAX_EVAL_EPISODES = 1\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        rwd_list = []\n",
    "        for eval_episode in range(self.MAX_EVAL_EPISODES):\n",
    "            s, _ = self.env.reset(seed=self.seed)\n",
    "            total_reward = 0\n",
    "            rwd_list.append(0)\n",
    "            for step in count():\n",
    "                a = self.explorationStrategyEval(self.online_model, s)\n",
    "                s, rwd, done, truncated,_ = self.env.step(a)\n",
    "                total_reward += rwd\n",
    "\n",
    "                rwd_list[-1] += rwd\n",
    "                if done or truncated: break\n",
    "        return rwd_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count=0\n",
    "d3qnper_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "seed_list = [420, 133, 74, 317, 233]\n",
    "\n",
    "for myseed in seed_list:\n",
    "\n",
    "    val_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n",
    "\n",
    "    # Instantiation of the D3QN_PER class\n",
    "    d3qnper_instance = D3QN_PER(\n",
    "        env_id='CartPole-v1',\n",
    "        seed=myseed,\n",
    "        gamma=0.99,\n",
    "        epochs = 20,\n",
    "        bufferSize=20000,\n",
    "        batchSize=5,\n",
    "        optimizerFn=val_optimizer_fn,\n",
    "        optimizerLR=0.0005,\n",
    "        max_gradient_norm = float('inf'),\n",
    "        MAX_TRAIN_EPISODES=300,\n",
    "        MAX_EVAL_EPISODES=1,\n",
    "        explorationStrategyTrainFn=selectEpsilonExpGreedyAction,\n",
    "        explorationStrategyEvalFn=selectGreedyAction,\n",
    "        updateFrequency=1,\n",
    "        tau = 0.01\n",
    "    )\n",
    "\n",
    "    # Running the NFQ method and appending results\n",
    "    trainRewardsList, trainTimeList, evalRewardsList, wallClockTimeList, final_eval_score = d3qnper_instance.runD3QN_PER()\n",
    "    d3qnper_results.append(trainRewardsList)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = d3qnper_instance\n",
    "\n",
    "# Convert dqn_results to a numpy array for any further processing\n",
    "d3qnper_results = np.array(d3qnper_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_max, steps_max, eval_score_max, train_time_max, wall_clock_max = np.max(d3qnper_results, axis=0).T\n",
    "reward_min, steps_min, eval_score_min, train_time_min, wall_clock_min = np.min(d3qnper_results, axis=0).T\n",
    "reward_avg, steps_avg, eval_score_avg, train_time_avg, wall_clock_avg = np.mean(d3qnper_results, axis=0).T\n",
    "episode_indices = np.arange(len(reward_avg))\n",
    "\n",
    "plt.style.use('ggplot')  \n",
    "fig, plot_areas = plt.subplots(5, 1, figsize=(12, 25), sharex='col')\n",
    "fig.subplots_adjust(hspace=0.5)  \n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "titles = ['Total Steps','Training Reward', 'Evaluation reward',  'Training Duration', 'Wall-clock Time']\n",
    "y_labels = ['Steps', 'Reward', 'Reward', 'Seconds', 'Seconds']\n",
    "data_max = [reward_max, steps_max, eval_score_max, train_time_max, wall_clock_max]\n",
    "data_min = [reward_min, steps_min, eval_score_min, train_time_min, wall_clock_min]\n",
    "data_avg = [reward_avg, steps_avg, eval_score_avg, train_time_avg, wall_clock_avg]\n",
    "\n",
    "# Generate plots\n",
    "for ax, title, color, max_data, min_data, avg_data, y_label in zip(plot_areas, titles, colors, data_max, data_min, data_avg, y_labels):\n",
    "    ax.plot(max_data, linestyle='--', color=color, alpha=0.75)\n",
    "    ax.plot(min_data, linestyle='--', color=color, alpha=0.75)\n",
    "    ax.plot(avg_data, label='D3QN-PER', color=color, linewidth=2)\n",
    "    ax.fill_between(episode_indices, min_data, max_data, color=color, alpha=0.2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.legend()\n",
    "\n",
    "plot_areas[-1].set_xlabel('Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "em5bUsbGgpyw"
   },
   "source": [
    "# Deep Policy Based RL agents.\n",
    "<a id=\"deep-policy-based\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4HI3W2Qgpyw"
   },
   "source": [
    "### The purpose of this part is to learn about different Deep Policy Based RL agents.\n",
    "\n",
    "In this part of the assignment you will be implementing Deep Policy based RL algorithms we learnt in Lectures. Namely, we will be implementing REINFORCE and VPG.\n",
    "\n",
    "For all the algorithms below, this time we will not be specifying the hyper-parameters, please play with the hyper-params to come up with the best values. This way you will learn to tune the model. Some of the values were specified in the lecture, that would be a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBU5xpicgpyw"
   },
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejJPN2fqgpyw"
   },
   "source": [
    "Implement the REINFORCE algorithm. We have studied about REINFORCE algorithm in the Lecture. Use the function definitions (given below).\n",
    "\n",
    "This class implements the REINFORCE Agent, you are required to implement the various methods of this class\n",
    "as outlined below. Note this class is generic and should work with any permissible Gym environment\n",
    "\n",
    "```\n",
    "class REINFORCE():\n",
    "    def __init__(env, seed, gamma,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn,\n",
    "                 explorationStrategyEvalFn)\n",
    "    def initBookKeeping(self)\n",
    "    def performBookKeeping(self, train = True)\n",
    "    def runREINFORCE(self)\n",
    "    def trainAgent(self)\n",
    "    def trainPolicyNetwork(self, experiences)\n",
    "    def evaluateAgent(self)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naFDR3ZWgpyw"
   },
   "source": [
    "### Implement the methods for the REINFORCE class below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUsNtqEfgpyw"
   },
   "outputs": [],
   "source": [
    "class REINFORCE():\n",
    "\n",
    "    def __init__(self, env_id, seed, gamma, optimizerFn, optimizerLR, MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES):\n",
    "\n",
    "        # 1. Environment initialization using env_id\n",
    "        self.env = gym.make(env_id)\n",
    "        self.seed = seed\n",
    "        reset_env(self.env, seed=seed)  # Seed the environment\n",
    "        torch.manual_seed(seed)  # Seed PyTorch\n",
    "        np.random.seed(seed)  # Seed NumPy\n",
    "        random.seed(seed)\n",
    "\n",
    "        # 3. Creating P-network\n",
    "        inDim = self.env.observation_space.shape[0]\n",
    "        outDim = self.env.action_space.n\n",
    "        hDim = [128, 64]  # Hidden layer dimensions\n",
    "        activation = torch.nn.functional.relu\n",
    "        self.p_network = createPolicyNetworkREINFORCE(inDim, outDim, hDim, activation)\n",
    "\n",
    "        # 4. Optimizer initialization\n",
    "        self.optimizer = optimizerFn(self.p_network.parameters(), optimizerLR)\n",
    "\n",
    "        # 6. Other variables\n",
    "        self.gamma = gamma\n",
    "        self.MAX_TRAIN_EPISODES = MAX_TRAIN_EPISODES\n",
    "        self.MAX_EVAL_EPISODES = MAX_EVAL_EPISODES\n",
    "\n",
    "    def runREINFORCE(self):\n",
    "        resultList, trainTimeList, evalRewardsList, wallClockTimeList = self.trainAgent()\n",
    "        resultEval = self.evaluateAgent()\n",
    "        finalEvalReward = np.mean(resultEval)\n",
    "\n",
    "        return resultList, trainTimeList, evalRewardsList, wallClockTimeList, finalEvalReward\n",
    "\n",
    "    def stepint(self, state, env):\n",
    "        action, is_exploratory, logpa, _ = self.policy_model.full_pass(state)\n",
    "        new_state, reward, is_terminal, _, _ = env.step(action)\n",
    "\n",
    "        self.logpas.append(logpa)\n",
    "        self.rewards.append(reward)\n",
    "        self.episode_reward[-1] += reward\n",
    "        self.episode_timestep[-1] += 1\n",
    "        self.episode_exploration[-1] += int(is_exploratory)\n",
    "\n",
    "        return new_state, is_terminal\n",
    "\n",
    "    def trainAgent(self):\n",
    "        training_start = time.time()\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
    "\n",
    "\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.episode_exploration = []\n",
    "        self.evaluation_scores = []\n",
    "        global train_count\n",
    "        global trunc\n",
    "        trunc  = 0\n",
    "        train_count +=1\n",
    "\n",
    "        self.policy_model = self.p_network\n",
    "        max_episodes = self.MAX_TRAIN_EPISODES\n",
    "        self.policy_optimizer = self.optimizer\n",
    "\n",
    "        result = np.empty((max_episodes, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            print(f'episode {episode} and train count is {train_count}')\n",
    "            state, _ = self.env.reset(seed=self.seed)\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            # collect rollout\n",
    "            self.logpas, self.rewards = [], []\n",
    "            for step in count():\n",
    "                state, is_terminal = self.stepint(state,self.env)\n",
    "                if is_terminal:\n",
    "                    break\n",
    "\n",
    "            self.trainNetwork()\n",
    "\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            self.episode_seconds.append(episode_elapsed)\n",
    "            training_time += episode_elapsed\n",
    "            evaluation_score= np.mean(self.evaluateAgent())\n",
    "\n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "\n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            #lst_100_exp_rat = np.array(self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
    "            #mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
    "            #std_100_exp_rat = np.std(lst_100_exp_rat)\n",
    "\n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode-1] = total_step, mean_100_reward, mean_100_eval_score, training_time, wallclock_elapsed\n",
    "            reached_max_minutes = wallclock_elapsed >= 10 * 60\n",
    "            reached_max_episodes = episode >= 10000\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= 475\n",
    "            #reached_goal_mean_reward=False\n",
    "            training_is_over = reached_max_minutes or reached_max_episodes or reached_goal_mean_reward\n",
    "\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "\n",
    "        final_eval_rwd_list = self.evaluateAgent()\n",
    "        mean_eval_rwd = np.mean(final_eval_rwd_list)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        print(f'truncation happens {trunc} times for train {train_count}')\n",
    "        self.env.close()\n",
    "\n",
    "        return result, training_time, final_eval_rwd_list, wallclock_time\n",
    "\n",
    "\n",
    "    def trainNetwork(self):\n",
    "        T = len(self.rewards)\n",
    "        discounts = np.logspace(0, T, num=T, base=self.gamma, endpoint=False)\n",
    "        returns = np.array([np.sum(discounts[:T-t] * self.rewards[t:]) for t in range(T)])\n",
    "\n",
    "        discounts = torch.FloatTensor(discounts).unsqueeze(1)\n",
    "        returns = torch.FloatTensor(returns).unsqueeze(1)\n",
    "        self.logpas = torch.cat(self.logpas)\n",
    "        policy_loss = -(discounts * returns * self.logpas).mean()\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "    def evaluateAgent(self, greedy=True):\n",
    "        rwd_list = []\n",
    "        global trunc\n",
    "        for _ in range(self.MAX_EVAL_EPISODES):\n",
    "            s, _ = self.env.reset(seed=self.seed)\n",
    "            rwd_list.append(0)\n",
    "            begin = 0\n",
    "            for _ in count():\n",
    "                if greedy:\n",
    "                    a = self.policy_model.select_greedy_action(s)\n",
    "                else:\n",
    "                    a = self.policy_model.select_action(s)\n",
    "                s, r, done,truncate, _ = self.env.step(a)\n",
    "                rwd_list[-1] += r\n",
    "                if truncate:\n",
    "                    begin +=1\n",
    "                    print(f'truncation happening {trunc} times')\n",
    "                if done or truncate: break    \n",
    "            \n",
    "            if (begin):\n",
    "                print(f'truncation happening {begin} times')\n",
    "                trunc +=1\n",
    "                \n",
    "                \n",
    "\n",
    "        return rwd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETs2GiAwgpyx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfodZFXg-Lpi",
    "outputId": "956befd3-f41c-445e-ce10-4cfaead84c22"
   },
   "outputs": [],
   "source": [
    "train_count = 0\n",
    "trunc = 0\n",
    "reinforce_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "seed_list = [420,133,74,317,233]\n",
    "for seed in seed_list:\n",
    "\n",
    "    policy_optimizer_fn = lambda net, lr: optim.Adam(net, lr=lr)\n",
    "\n",
    "    reinforce_instance = REINFORCE(env_id='CartPole-v1', seed=seed, gamma=1.0, optimizerFn = policy_optimizer_fn, optimizerLR = 0.0005,MAX_TRAIN_EPISODES = 500,MAX_EVAL_EPISODES = 1)\n",
    "\n",
    "    trainRewardsList, trainTimeList, evalRewardsList, wallClockTimeList, final_eval_score = reinforce_instance.runREINFORCE()\n",
    "    reinforce_results.append(trainRewardsList)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = reinforce_instance\n",
    "\n",
    "reinforce_results = np.array(reinforce_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_max, steps_max, eval_score_max, train_time_max, wall_clock_max = np.max(reinforce_results, axis=0).T\n",
    "reward_min, steps_min, eval_score_min, train_time_min, wall_clock_min = np.min(reinforce_results, axis=0).T\n",
    "reward_avg, steps_avg, eval_score_avg, train_time_avg, wall_clock_avg = np.mean(reinforce_results, axis=0).T\n",
    "episode_indices = np.arange(len(reward_avg))\n",
    "\n",
    "plt.style.use('ggplot')  \n",
    "fig, plot_areas = plt.subplots(5, 1, figsize=(12, 25), sharex='col')\n",
    "fig.subplots_adjust(hspace=0.5)  \n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "titles = ['Total Steps','Training Reward', 'Evaluation reward',  'Training Duration', 'Wall-clock Time']\n",
    "y_labels = ['Reward', 'Score', 'Steps', 'Seconds', 'Seconds']\n",
    "data_max = [reward_max, steps_max, eval_score_max, train_time_max, wall_clock_max]\n",
    "data_min = [reward_min, steps_min, eval_score_min, train_time_min, wall_clock_min]\n",
    "data_avg = [reward_avg, steps_avg, eval_score_avg, train_time_avg, wall_clock_avg]\n",
    "\n",
    "# Generate plots\n",
    "for ax, title, color, max_data, min_data, avg_data, y_label in zip(plot_areas, titles, colors, data_max, data_min, data_avg, y_labels):\n",
    "    ax.plot(max_data, linestyle='--', color=color, alpha=0.75)\n",
    "    ax.plot(min_data, linestyle='--', color=color, alpha=0.75)\n",
    "    ax.plot(avg_data, label='Average', color=color, linewidth=2)\n",
    "    ax.fill_between(episode_indices, min_data, max_data, color=color, alpha=0.2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.legend()\n",
    "\n",
    "plot_areas[-1].set_xlabel('Episodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUQ6BDYYgpyx"
   },
   "source": [
    "## Vanilla Policy Gradient (VPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swA6f5hAgpyx"
   },
   "source": [
    "Implement the VPG algorithm. We have studied about VPG algorithm in the Lecture. Use the function definitions (given below).\n",
    "\n",
    "This class implements the VPG Agent, you are required to implement the various methods of this class\n",
    "as outlined below. Note this class is generic and should work with any permissible Gym environment\n",
    "\n",
    "```\n",
    "class VPG():\n",
    "    def __init__(env, seed, gamma, beta,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn,\n",
    "                 explorationStrategyEvalFn)\n",
    "    def initBookKeeping(self)\n",
    "    def performBookKeeping(self, train = True)\n",
    "    def runVPG(self)\n",
    "    def trainAgent(self)\n",
    "    def trainPolicyNetwork(self, experiences)\n",
    "    def evaluateAgent(self)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLJ6SAYg6G68"
   },
   "outputs": [],
   "source": [
    "class VPG():\n",
    "    def __init__(self, env_id, seed, gamma,\n",
    "                 policy_model_max_grad_norm,\n",
    "                 policy_optimizer_fn,\n",
    "                 policy_optimizer_lr,\n",
    "                 value_model_max_grad_norm,\n",
    "                 value_optimizer_fn,\n",
    "                 value_optimizer_lr,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 entropy_loss_weight):\n",
    "        pass\n",
    "\n",
    "    def initBookKeeping(self):\n",
    "        pass\n",
    "\n",
    "    def performBookKeeping(self, train=True):\n",
    "        pass\n",
    "\n",
    "    def runVPG(self):\n",
    "        pass\n",
    "\n",
    "    def trainAgent(self):\n",
    "        pass\n",
    "\n",
    "    def trainPolicyNetwork(self):\n",
    "        pass\n",
    "\n",
    "    def evaluateAgent(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvHLd5g_gpyx"
   },
   "outputs": [],
   "source": [
    "class VPG():\n",
    "    def __init__(self, env_id, seed, gamma,\n",
    "                 policy_model_max_grad_norm,\n",
    "                 policy_optimizer_fn,\n",
    "                 policy_optimizer_lr,\n",
    "                 value_model_max_grad_norm,\n",
    "                 value_optimizer_fn,\n",
    "                 value_optimizer_lr,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 entropy_loss_weight):\n",
    "\n",
    "        # 1. Environment initialization using env_id\n",
    "        self.env = gym.make(env_id)\n",
    "        self.seed = seed\n",
    "        reset_env(self.env, seed=seed)  # Seed the environment\n",
    "        torch.manual_seed(seed)  # Seed PyTorch\n",
    "        np.random.seed(seed)  # Seed NumPy\n",
    "        random.seed(seed)\n",
    "\n",
    "        # 2. Creating P-network\n",
    "        inDim = self.env.observation_space.shape[0]\n",
    "        outDim = self.env.action_space.n\n",
    "        hDim = [128, 64]  # Hidden layer dimensions\n",
    "        activation = torch.nn.functional.relu\n",
    "        self.p_network = createPolicyNetworkREINFORCE(inDim, outDim, hDim, activation)\n",
    "\n",
    "        # 3. Creating V-network\n",
    "        hDim = [256, 128]\n",
    "        self.v_network = createPolicyNetworkVPG(inDim, outDim, hDim, activation)\n",
    "\n",
    "        # 4. Optimizers initialization\n",
    "        self.p_optimizer = policy_optimizer_fn(self.p_network.parameters(), policy_optimizer_lr)\n",
    "        self.v_optimizer = value_optimizer_fn(self.v_network.parameters(), value_optimizer_lr)\n",
    "\n",
    "        # 5. Other variables\n",
    "        self.gamma = gamma\n",
    "        self.policy_model_max_grad_norm = policy_model_max_grad_norm\n",
    "        self.value_model_max_grad_norm = value_model_max_grad_norm\n",
    "        self.entropy_loss_weight = entropy_loss_weight\n",
    "        self.MAX_TRAIN_EPISODES = MAX_TRAIN_EPISODES\n",
    "        self.MAX_EVAL_EPISODES = MAX_EVAL_EPISODES\n",
    "\n",
    "    def stepint(self, state, env):\n",
    "        action, is_exploratory, logpa, entropy = self.policy_model.full_pass(state)\n",
    "        new_state, reward, is_terminal, is_truncated, info = self.env.step(action)\n",
    "\n",
    "        self.logpas.append(logpa)\n",
    "        self.entropies.append(entropy)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(self.value_model(state))\n",
    "\n",
    "        self.episode_reward[-1] += reward\n",
    "        self.episode_timestep[-1] += 1\n",
    "        self.episode_exploration[-1] += int(is_exploratory)\n",
    "        return new_state, is_terminal, is_truncated\n",
    "\n",
    "    def trainAgent(self):\n",
    "\n",
    "        training_start= time.time()\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.episode_exploration = []\n",
    "        self.evaluation_scores = []\n",
    "        global train_count\n",
    "        global trunc\n",
    "        trunc = 0\n",
    "        train_count +=1\n",
    "\n",
    "        self.policy_model = self.p_network\n",
    "        self.policy_optimizer = self.p_optimizer\n",
    "        self.value_model = self.v_network\n",
    "        self.value_optimizer = self.v_optimizer\n",
    "        max_episodes = self.MAX_TRAIN_EPISODES\n",
    "\n",
    "        result = np.empty((max_episodes, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            print(f'episode {episode} and train is called {train_count} times')\n",
    "            state, _  = self.env.reset(seed=self.seed)\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            # collect rollout\n",
    "            self.logpas, self.entropies, self.rewards, self.values = [], [], [], []\n",
    "            for step in count():\n",
    "                state, is_terminal, is_truncated = self.stepint(state, self.env)\n",
    "                if is_terminal:\n",
    "                    break\n",
    "\n",
    "            is_failure = is_terminal and not is_truncated\n",
    "            next_value = 0 if is_failure else self.value_model(state).detach().item()\n",
    "            self.rewards.append(next_value)\n",
    "            self.trainPolicyNetwork()\n",
    "\n",
    "            # stats\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            self.episode_seconds.append(episode_elapsed)\n",
    "            training_time += episode_elapsed\n",
    "            evaluation_score= np.mean(self.evaluateAgent())\n",
    "\n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "\n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            #lst_100_exp_rat = np.array(self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
    "            #mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
    "            #std_100_exp_rat = np.std(lst_100_exp_rat)\n",
    "\n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode-1] = total_step, mean_100_reward, mean_100_eval_score, training_time, wallclock_elapsed\n",
    "            reached_max_minutes = wallclock_elapsed >= 10 * 60\n",
    "            reached_max_episodes = episode >= 10000\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= 475\n",
    "            #reached_goal_mean_reward=False\n",
    "            training_is_over = reached_max_minutes or reached_max_episodes or reached_goal_mean_reward\n",
    "\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "\n",
    "        final_eval_rwd_list = self.evaluateAgent()\n",
    "        mean_eval_rwd = np.mean(final_eval_rwd_list)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        self.env.close()\n",
    "        print(f'truncation happens {trunc} times for train {train_count}')\n",
    "\n",
    "        return result, training_time, final_eval_rwd_list, wallclock_time\n",
    "\n",
    "    def trainPolicyNetwork(self):\n",
    "        T = len(self.rewards)\n",
    "        discounts = np.logspace(0, T, num=T, base=self.gamma, endpoint=False)\n",
    "        returns = np.array([np.sum(discounts[:T-t] * self.rewards[t:]) for t in range(T)])\n",
    "        discounts = torch.FloatTensor(discounts[:-1]).unsqueeze(1)\n",
    "        returns = torch.FloatTensor(returns[:-1]).unsqueeze(1)\n",
    "\n",
    "        self.logpas = torch.cat(self.logpas)\n",
    "        self.entropies = torch.cat(self.entropies)\n",
    "        self.values = torch.cat(self.values)\n",
    "\n",
    "        value_error = returns - self.values\n",
    "        policy_loss = -(discounts * value_error.detach() * self.logpas).mean()\n",
    "        entropy_loss = -self.entropies.mean()\n",
    "        loss = policy_loss + self.entropy_loss_weight * entropy_loss\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(),\n",
    "                                       self.policy_model_max_grad_norm)\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        value_loss = value_error.pow(2).mul(0.5).mean()\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.value_model.parameters(),\n",
    "                                       self.value_model_max_grad_norm)\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "    def evaluateAgent(self, greedy=True):\n",
    "        rwd_list = []\n",
    "        global trunc\n",
    "        for _ in range(self.MAX_EVAL_EPISODES):\n",
    "            s, _ = self.env.reset(seed=self.seed)\n",
    "            rwd_list.append(0)\n",
    "            begin = 0\n",
    "            for _ in count():\n",
    "                if greedy:\n",
    "                    a = self.policy_model.select_greedy_action(s)\n",
    "                else:\n",
    "                    a = self.policy_model.select_action(s)\n",
    "                s, r, done,truncate, _ = self.env.step(a)\n",
    "                rwd_list[-1] += r\n",
    "                if truncate:\n",
    "                  begin +=1\n",
    "                if done or truncate: break\n",
    "\n",
    "            if (begin):\n",
    "                print(f'truncation happening {begin} times')\n",
    "                trunc +=1\n",
    "        return rwd_list\n",
    "\n",
    "    def runVPG(self):\n",
    "        resultList, trainTimeList, evalRewardsList, wallClockTimeList = self.trainAgent()\n",
    "        resultEval = self.evaluateAgent()\n",
    "        finalEvalReward = np.mean(resultEval)\n",
    "\n",
    "        return resultList, trainTimeList, evalRewardsList, wallClockTimeList, finalEvalReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeHAvSrQgpyy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7TKWeB-mgpyy",
    "outputId": "836fef96-9aa4-4025-9c4a-91e6010ff5e3"
   },
   "outputs": [],
   "source": [
    "train_count = 0\n",
    "trunc = 0\n",
    "vpg_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "seed_list = [420,133,74,317,233]\n",
    "for seed in seed_list:\n",
    "\n",
    "    policy_model_max_grad_norm = 1\n",
    "    policy_optimizer_exp = lambda net, lr: optim.Adam(net, lr=lr)\n",
    "    policy_optimizer_lr = 0.0005\n",
    "\n",
    "    value_model_max_grad_norm = float('inf')\n",
    "    value_optimizer_exp = lambda net, lr: optim.RMSprop(net, lr=lr)\n",
    "    value_optimizer_lr = 0.0007\n",
    "\n",
    "    entropy_loss_weight = 0.001\n",
    "\n",
    "    vpg_instance = VPG(env_id = 'CartPole-v1',\n",
    "                seed = seed,\n",
    "                gamma = 1.0,\n",
    "                policy_model_max_grad_norm = 1,\n",
    "                policy_optimizer_fn = policy_optimizer_exp,\n",
    "                policy_optimizer_lr = 0.000485,\n",
    "                value_model_max_grad_norm = float('inf'),\n",
    "                value_optimizer_fn= value_optimizer_exp,\n",
    "                value_optimizer_lr = 0.00068,\n",
    "                MAX_TRAIN_EPISODES = 500,\n",
    "                MAX_EVAL_EPISODES = 3,\n",
    "                entropy_loss_weight = 0.0011)\n",
    "\n",
    "    trainRewardsList, trainTimeList, evalRewardsList, wallClockTimeList, final_eval_score = vpg_instance.runVPG()\n",
    "    vpg_results.append(trainRewardsList)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = vpg_instance\n",
    "vpg_results = np.array(vpg_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_max, steps_max, eval_score_max, train_time_max, wall_clock_max = np.max(vpg_results, axis=0).T\n",
    "reward_min, steps_min, eval_score_min, train_time_min, wall_clock_min = np.min(vpg_results, axis=0).T\n",
    "reward_avg, steps_avg, eval_score_avg, train_time_avg, wall_clock_avg = np.mean(vpg_results, axis=0).T\n",
    "episode_indices = np.arange(len(reward_avg))\n",
    "\n",
    "plt.style.use('ggplot')  \n",
    "fig, plot_areas = plt.subplots(5, 1, figsize=(12, 25), sharex='col')\n",
    "fig.subplots_adjust(hspace=0.5)  \n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "titles = ['Total Steps','Training Reward', 'Evaluation reward',  'Training Duration', 'Wall-clock Time']\n",
    "y_labels = ['Reward', 'Score', 'Steps', 'Seconds', 'Seconds']\n",
    "data_max = [reward_max, steps_max, eval_score_max, train_time_max, wall_clock_max]\n",
    "data_min = [reward_min, steps_min, eval_score_min, train_time_min, wall_clock_min]\n",
    "data_avg = [reward_avg, steps_avg, eval_score_avg, train_time_avg, wall_clock_avg]\n",
    "\n",
    "# Generate plots\n",
    "for ax, title, color, max_data, min_data, avg_data, y_label in zip(plot_areas, titles, colors, data_max, data_min, data_avg, y_labels):\n",
    "    ax.plot(max_data, linestyle='--', color=color, alpha=0.75)\n",
    "    ax.plot(min_data, linestyle='--', color=color, alpha=0.75)\n",
    "    ax.plot(avg_data, label='VPG', color=color, linewidth=2)\n",
    "    ax.fill_between(episode_indices, min_data, max_data, color=color, alpha=0.2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.legend()\n",
    "\n",
    "plot_areas[-1].set_xlabel('Episodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voD6wBRSgpyy"
   },
   "source": [
    "# Experiments and Plots\n",
    "<a id=\"experiments\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbZGyq1Ngpyy"
   },
   "source": [
    "Run the NFQ, DQN, Double DQN, Dueling Double DQN, Dueling Double Deep Q Network with Prioritized Experience Replay, REINFORCE and VPG agent on CartPole environment and MountainCar enviroment.\n",
    "\n",
    "Plot the following for each of the environment separately. Note based on different hyper-parameters and stratgies you use, can you have multiple plots for each of the below.\n",
    "\n",
    "As you are aware from your past experience, single run of the agent over the environment results in plots that have lot of variance and look very noisy. One way to overcome this is to create several different instances of the environment using different seeds and then average out the results across these and plot these. For all the plots below, you this strategy. You need to run 5 different instances of the environment for each agent. As you have seen in the lecture slides, we plot the maximum and minimum values around the mean in the plots, so this gives us the shaded plot with the mean curve in the between. In this assignment, you are required to do the same. Generate plots with envelop between maximum and minimum value (check the plotQuantity() function in the helper functions).\n",
    "\n",
    "For each of the quantity of interest, plot each of the agent within the same plot using different colors for the envelop. Choose colors such that that there is clear contrast between the plots corresponding to different agents.\n",
    "\n",
    "1. Plot mean train rewards vs episodes for Cartpole environment.\n",
    "2. Plot mean train rewards vs episodes for MountatinCar environment.\n",
    "3. Plot mean evaluation rewards vs episodes\n",
    "4. Plot mean evaluation rewards vs episodes\n",
    "5. Plot total steps vs episode for Cartpole environment.\n",
    "6. Plot total steps vs episode for MountatinCar environment.\n",
    "7. Plot train time vs episode for Cartpole environment.\n",
    "8. Plot train time vs episode for MountatinCar environment.\n",
    "9. Plot wall clock time vs episode for Cartpole environment.\n",
    "10. Plot wall clock time vs episode for MountatinCar environment.\n",
    "11. Based on plots for CartPole environment, what are your observations about different agents. Compare different agents.  \n",
    "12. Based on plots for MountainCar environment, what are your observations about different agents. Compare different agents. Do these observations concur with the ones for CartPole environment?\n",
    "13. Based on both the environments, can you generalize some of the findings for the value-based agents? If yes what are those findings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFeapnp5gpyy"
   },
   "outputs": [],
   "source": [
    "def runDeepValueBasedAgents():\n",
    "    # this function will initialize 5 different instances of the env (using different seeds), run all the agents\n",
    "    # over these different instances. Collects results and generate the plots state above.\n",
    "    # generate your plots in the cells below\n",
    "    # write the answers to part 11, 12 and 13 in the cells below the plot-cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ma0O9PfFgpyy"
   },
   "outputs": [],
   "source": [
    "def runDeepPolicyBasedAgents():\n",
    "    # this function will initialize 5 different instances of the env (using different seeds), run all the agents\n",
    "    # over these different instances. Collects results and generate the plots state above.\n",
    "    # generate your plots in the cells below\n",
    "    # write the answers to part 11, 12 and 13 in the cells below the plot-cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDb-MCaygpyz"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUMFcG2Ngpyz"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
