{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 3\n",
    "# Submission Deadline: 20/03/2024 at 10 AM\n",
    "# Submission Link: https://forms.gle/b8s6xYHUYqTJtSNeA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Provide Information](#Provide-Information)\n",
    "2. [Instructions](#Instructions)\n",
    "3. [Environment](#Environment)\n",
    "4. [Hyperparameters](#Hyperparameters)\n",
    "5. [Helper Functions](#helper)\n",
    "6. [Deep Value Based RL Agents](#deep-value-based)\n",
    "7. [Deep Policy Based RL Agents](#deep-policy-based)\n",
    "8. [Experiments to Run](#experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provide Information\n",
    "<a id=\"Provide-Information\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: **Kislay Aditya Oj**\n",
    "\n",
    "Roll No.: **210524**\n",
    "\n",
    "IITK EMail: **kislay21@iitk.ac.in**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "<a id=\"Instructions\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read all the instructions below carefully before you start working on the assignment.**\n",
    "- The purpose of this course is that you learn RL and the best way to do that is by implementation and experimentation.\n",
    "- The assignment requires your to implement some algorithms and you are required report your findings after experimenting with those algorithms.\n",
    "- **You are required to submit ZIP file containing a Jupyter notebook (.ipynb), and an image folder. The notebook would include the code, graphs/plots of the experiments you run and your findings/observations. Image folder is the folder having plots, images, etc.**\n",
    "- In case you use any maths in your explanations, render it using latex in the Jupyter notebook.\n",
    "- You are expected to implement algorithms on your own and not copy it from other sources/class mates. Of course, you can refer to lecture slides.\n",
    "- If you use any reference or material (including code), please cite the source, else it will be considered plagiarism. But referring to other sources that directly solve the problems given in the assignment is not allowed. There is a limit to which you can refer to outside material.\n",
    "- This is an individual assignment.\n",
    "- In case your solution is found to have an overlap with solution by someone else (including external sources), all the parties involved will get zero in this and all future assignments plus further more penalties in the overall grade. We will check not just for lexical but also semantic overlap. Same applies for the code as well. Even an iota of cheating would NOT be tolerated. If you cheat one line or cheat one page the penalty would be same.\n",
    "- Be a smart agent, think long term, if you cheat we will discover it somehow, the price you would be paying is not worth it.\n",
    "- In case you are struggling with the assignment, seek help from TAs. Cheating is not an option! I respect honesty and would be lenient if you are not able to solve some questions due to difficulty in understanding. Remember we are there to help you out, seek help if something is difficult to understand.\n",
    "- The deadline for the submission is given above. Submit at least 30 minutes before the deadline, lot can happen at the last moment, your internet can fail, there can be a power failure, you can be abducted by aliens, etc.\n",
    "- You have to submit your assignment via the Google Form (link above)\n",
    "- The form would close after the deadline and we will not accept any solution. No reason what-so-ever would be accepted for not being able to submit before the deadline.\n",
    "- Since the assignment involves experimentation, reporting your results and observations, there is a lot of scope for creativity and innovation and presenting new perspectives. Such efforts would be highly appreciated and accordingly well rewarded. Be an exploratory agent!\n",
    "- Your code should be very well documented, there are marks for that.\n",
    "- In your plots, have a clear legend and clear lines, etc. Of course you would generating the plots in your code but you must also put these plots in your notebook. Generate high resolution pdf/svg version of the plots so that it doesn't pixilate on zooming.\n",
    "- For all experiments, report about the seed used in the code documentation, write about the seed used.\n",
    "- In your notebook write about all things that are not obvious from the code e.g., if you have made any assumptions, references/sources, running time, etc.\n",
    "-  **DO NOT Forget to write name, roll no and email details above**\n",
    "- **In addition to checking your code, we will be conducting one-on-one viva for the evaluation. So please make sure that you do not cheat!**\n",
    "- **Use of LLMs based tools or AI-based code tools is strictly prohibited! Use of ChatGPT, VS Code, Gemini, CO-Pilot, etc. is not allowed. NOTE VS code is also not allowed. Even in Colab disable the AI assistant. If you use it, we will know it very easily. Use of any of the tools would be counted as cheating and would be given a ZERO, with no questions asked.**\n",
    "- For each of the sub-part in the question create a new cell below the question and put your answer in there. This includes the plots as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# OpenAI Gym Environments\n",
    "<a id=\"Environment\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports go in here\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time \n",
    "import copy \n",
    "import random\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this assignment we will be exploring Deep RL algorithms and for this we will be using environmentd provided by OpenAI Gym. In particualr we will be exploring \"CartPole-v0\" and \"MountainCar-v0\" environments (https://gymnasium.farama.org/environments/classic_control/ ). The code to instantiate the environments are given in the cells below. Run these cells and play with the environments to learn more details about the environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T17:28:12.371968Z",
     "start_time": "2021-11-05T17:27:46.187228Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create CartPole environment\n",
    "#https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "#env.seed(34)\n",
    "s = env.reset()\n",
    "print(\"Observation Space = \")\n",
    "print(env.observation_space)\n",
    "print(\"Action Space = \")\n",
    "print(env.action_space)\n",
    "done = False\n",
    "for episode in range(20):\n",
    "    print(\"In episode {}\".format(episode))\n",
    "    for i in range(100):\n",
    "        env.render()\n",
    "        print(s)\n",
    "        a = env.action_space.sample()\n",
    "        s, r, terminated, truncated , info = env.step(a)\n",
    "        if terminated or truncated:\n",
    "            print(\"Finished after {} timestep\".format(i+1))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T07:58:34.740393Z",
     "start_time": "2021-11-03T07:58:31.341154Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create MountainCar environment: \n",
    "# https://gymnasium.farama.org/environments/classic_control/mountain_car/\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "#env.seed(45)\n",
    "s = env.reset()\n",
    "print(\"Observation Space = \")\n",
    "print(env.observation_space)\n",
    "print(\"Action Space = \")\n",
    "print(env.action_space)\n",
    "done = False\n",
    "for episode in range(20):\n",
    "    print(\"In episode {}\".format(episode))\n",
    "    for i in range(100):\n",
    "        env.render()\n",
    "        print(s)\n",
    "        a = env.action_space.sample()\n",
    "        s, r, terminated, truncated , info = env.step(a)\n",
    "        if terminated or truncated :\n",
    "            print(\"Finished after {} timestep\".format(i+1))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Hyperparameters\n",
    "<a id=\"Hyperparameters\"></a>\n",
    "\n",
    "All your hyperparameters should be stated here. We will change their value here and your code should work  accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mention the values of all the hyperparameters (you can add more hyper-paramters as well) to be used in \n",
    "# the entire notebook,put the values that gave the best performance and were finally used for the agent\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon = 0.5 #epsilon greedy strategy\n",
    "temp = 0.4#softmax strategy \n",
    "delta = 1#huber loss\n",
    "tau = 0.1#D3QN\n",
    "alpha = 0.1#D3QN-PER\n",
    "beta = 0.9#D3QN-PER\n",
    "beta_rate = 0.99992#D3QN-PER\n",
    "MAX_TRAIN_EPISODES = 1000\n",
    "MAX_EVAL_EPISODES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T09:29:28.557269Z",
     "start_time": "2021-10-27T09:29:28.543908Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Helper Functions\n",
    "<a id=\"helper\"></a>\n",
    "\n",
    "Write all the helper functions that will be used for value-based and policy based algorithms below. In case you want to add more helper functions, please feel free to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def selectGreedyAction(net, state):\n",
    "    #this function gets q-values via the network and selects greedy action from q-values and returns it\n",
    "    #Your code goes in here\n",
    "    \n",
    "    q_values = net(state)\n",
    "    greedyaction = torch.argmax(q_values, dim=1)[0]\n",
    "\n",
    "    return greedyaction.detach().item()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def selectEpsilonGreedyAction(net,  state, epsilon = 0.9 ):\n",
    "    #this function gets q-values via the network and selects an action from q-values using epsilon greedy strategy\n",
    "    #and returns it\n",
    "    #note this function can be used for decaying epsilon greedy strategy, \n",
    "    #you would need to create a wrapper function that will handle decaying epsilon\n",
    "    #you can create this wrapper in this helper function section\n",
    "    #for the agents you would be implementing it would be nice to play with decaying parameter to get optimal \n",
    "    #results\n",
    "    \n",
    "    #Your code goes in here\n",
    "    random_value = random.random()\n",
    "    action = 0\n",
    "    if random_value <= epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else :\n",
    "        action = selectGreedyAction(net, state)\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def selectSoftMaxAction(net, state, temp):\n",
    "    #this function gets q-values via the network and selects an action from q-values using softmax strategy\n",
    "    #and returns it\n",
    "    #note this function can be used for decaying temperature softmax strategy, \n",
    "    #you would need to create a wrapper function that will handle decaying temperature\n",
    "    #you can create this wrapper in this helper function section\n",
    "    #for the agents you would be implementing it would be nice to play with decaying parameter to get optimal \n",
    "    #results\n",
    "    \n",
    "    #Your code goes in here\n",
    "    q_values = net(state)\n",
    "    \n",
    "    softmax_probs = F.softmax(q_values / temp, dim=1)\n",
    "    \n",
    "    soft_action = torch.multinomial(softmax_probs, 1).item()\n",
    "    \n",
    "    return softAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, inDim, outDim, hDim=[32, 32], activation=nn.ReLU()):\n",
    "        super(Network, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(inDim, hDim[0]))\n",
    "        layers.append(activation)\n",
    "        \n",
    "        for i in range(len(hDim) - 1):\n",
    "            layers.append(nn.Linear(hDim[i], hDim[i + 1]))\n",
    "            layers.append(activation)\n",
    "            \n",
    "        layers.append(nn.Linear(hDim[-1], outDim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelNetwork(nn.Module):\n",
    "    def __init__(self, inDim, outDim, hDim=[32, 32], activation=nn.ReLU()):\n",
    "        super(DuelNetwork, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(inDim, hDim[0])\n",
    "        self.act = activation\n",
    "        self.fc_value = nn.Linear(hDim[0], hDim[1])\n",
    "        self.fc_adv = nn.Linear(hDim[0], hDim[1])\n",
    "\n",
    "        self.value = nn.Linear(hDim[1], 1)\n",
    "        self.adv = nn.Linear(hDim[1], outDim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        y = self.act(self.fc1(state))\n",
    "        value = self.act(self.fc_value(y))\n",
    "        adv = self.act(self.fc_adv(y))\n",
    "\n",
    "        value = self.value(value)\n",
    "        adv = self.adv(adv)\n",
    "\n",
    "        advAverage = torch.mean(adv, dim=1, keepdim=True)\n",
    "        Q = value + adv - advAverage\n",
    "\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Value Network\n",
    "def createValueNetwork(inDim, outDim, hDim = [32,32], activation = nn.ReLU()):\n",
    "    #this creates a Feed Forward Neural Network class and instantiates it and returns the class\n",
    "    #the class should be derived from torch nn.Module and it should have init and forward method at the very least\n",
    "    #the forward function should return q-value for each possible action\n",
    "    \n",
    "    #Your code goes in here\n",
    "    return Network(inDim, outDim, hDim, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Dueling Network\n",
    "def createDuelingNetwork(inDim, outDim, hDim = [32,32], activation = nn.ReLU()):\n",
    "    #this creates a Feed Forward Neural Network class and instantiates it and returns the class\n",
    "    #the class should be derived from torch nn.Module and it should have init and forward method at the very least\n",
    "    #the forward function should return q-value which is derived \n",
    "    #internally from action-advantage function and v-function, \n",
    "    #Note we center the advantage values, basically we subtract the mean from each state-action value\n",
    "    \n",
    "    #Your code goes in here\n",
    "    \n",
    "    return DuelNetwork(inDim, outDim, hDim, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Policy Network\n",
    "def createPolicyNetwork(inDim, outDim, hDim = [32,32], activation = nn.ReLU()):\n",
    "    #this creates a Feed Forward Neural Network class and instantiates it and returns the class\n",
    "    #the class should be derived from torch nn.Module and it should have init and forward method at the very least\n",
    "    #the forward function should return action logit vector \n",
    "    \n",
    "    #Your code goes in here\n",
    "    return Network(inDim, outDim, hDim, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plotQuantity(quantityListDict, totalEpisodeCount, descriptionList,window_size=10):\n",
    "    # This function takes in the quantityListDict and plots quantity vs episodes.\n",
    "    # quantityListListDict = {envInstanceCount: quantityList}\n",
    "    # quantityList is list of the quantity per episode,\n",
    "    # for example, it could be mean reward per episode, train time per episode, etc.\n",
    "    #\n",
    "    # NOTE: len(quantityList) == totalEpisodeCount\n",
    "    #\n",
    "    # Since we run multiple instances of the environment, there will be variance across environments\n",
    "    # so in the plot, you will plot per episode maximum, minimum, and average value across all env instances\n",
    "    # Basically, you need to envelop (e.g., via color) the quantity between max and min with mean value in between\n",
    "    #\n",
    "    # use the descriptionList parameter to put legends, title, etc.\n",
    "    # For each of the plot, create the legend on the left/right side so that it doesn't overlay on the\n",
    "    # plot lines/envelop.\n",
    "    #\n",
    "    # this is a generic function and can be used to plot any of the quantity of interest\n",
    "    # In particular we will be using this function to plot:\n",
    "    #        mean train rewards vs episodes\n",
    "    #        mean evaluation rewards vs episodes\n",
    "    #        total steps vs episode\n",
    "    #        train time vs episode\n",
    "    #        wall clock time vs episode\n",
    "    #\n",
    "    # this function doesn't return anything\n",
    "    \n",
    "    \n",
    "    mean_values = np.zeros(totalEpisodeCount)\n",
    "    max_values = np.full(totalEpisodeCount, -float('inf'))\n",
    "    min_values = np.full(totalEpisodeCount, float('inf'))\n",
    "    count = 0\n",
    "    \n",
    "    \n",
    "    for quantityList in quantityListDict.values():\n",
    "        quantityList = np.array(quantityList, dtype=float)\n",
    "        \n",
    "        if len(quantityList) > totalEpisodeCount:\n",
    "            quantityList = quantityList[:totalEpisodeCount]\n",
    "            \n",
    "        quantityList = np.pad(quantityList, (0, totalEpisodeCount - len(quantityList)), mode='constant', constant_values=np.nan)\n",
    "        \n",
    "        \n",
    "        mean_values += quantityList\n",
    "        max_values = np.maximum(max_values, quantityList)\n",
    "        min_values = np.minimum(min_values, quantityList)\n",
    "        \n",
    "        count += 1\n",
    "    \n",
    "    episode_numbers = np.arange(1, totalEpisodeCount + 1)\n",
    "    mean_values = mean_values / count\n",
    "    \n",
    "    # Applying simple moving average \n",
    "    kernel = np.ones(window_size) / window_size\n",
    "    mean_values_smooth = np.convolve(mean_values, kernel, mode='valid')\n",
    "    \n",
    "    # Plot \n",
    "    plt.fill_between(episode_numbers[window_size-1:], min_values[window_size-1:], max_values[window_size-1:], alpha=0.3)\n",
    "    plt.plot(episode_numbers[window_size-1:], mean_values_smooth, label=descriptionList[2])\n",
    "    \n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel(descriptionList[0])\n",
    "    plt.title(descriptionList[1])\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def huberLoss(error, delta):\n",
    "    #this function calculates the huber loss for the error using the delta parameter\n",
    "    \n",
    "    #Your code goes in here\n",
    "    abs_error = torch.abs(error)\n",
    "    quadratic = torch.minimum(abs_error, delta)\n",
    "    linear = abs_error - quadratic\n",
    "    hLoss = 0.5 * quadratic**2 + delta * linear\n",
    "    return hLoss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#in case you want to create any other helper function, the code goes in here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#in case you want to create any other helper function, the code goes in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Deep Value Based RL agents.\n",
    "<a id=\"deep-value-based\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### The purpose of this part is to learn about different Deep Value Based RL agents.\n",
    "\n",
    "In this part of the assignment you will be implementing Deep RL algorithms we learnt in Lectures. Namely, we will be implementing NFQ, DQN, Double DQN (DDQN), Duelling Double DQN (D3QN), and Duelling Double DQN with Prioritized Experience Replay (D3QN-PER). For all the algorithms below, this time we will not be specifying the hyper-parameters, please play with the hyper-params to come up with the best values. This way you will learn to tune the model. Some of the values were specified in the lecture, that would be a good starting point. Your aim is to develop the best NFQ/DQN/DDQN/D3QN/D3QN-PER agent for each of the setting.  \n",
    "\n",
    "For those of you who follow TEDEd, here is an interesting video by TED on DQN and Atari Games: https://www.youtube.com/watch?v=PP8Zc778B8s \n",
    "\n",
    "Also since these environments are available in Gymanasium, there are public leaderboards (https://github.com/openai/gym/wiki/Leaderboard) for each of these environments. Compare where does your agent stand on these leaderboard for each of these environments, try to tune your agents so that it is on the top of the leaderboard. In fact, if your agent performs well on these environments, you can alse make your entry on the leaderboard.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'> Do not change any Class/Methods definition. We have split the class methods across cells for code readibility purposes. This requires to inherit the same class, please do not change it. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ReplayBuffer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In next few cells, you will implement replaybuffer class. \n",
    "\n",
    "This class creates a buffer for storing and retrieving experiences. This is a generic class and can be used\n",
    "for different agents like NFQ, DQN, DDQN, PER_DDQN, etc. \n",
    "Following are the methods for this class which are implemented in subsequent cells\n",
    "\n",
    "```\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, bufferSize, **kwargs)\n",
    "    def store(self, experience)\n",
    "    def update(self, indices, priorities) \n",
    "    def collectExperiences(env, state, explorationStrategy, net = None)\n",
    "    def sample(self, batchSize, **kwargs)\n",
    "    def splitExperiences(self, experiences)\n",
    "    def length(self)\n",
    "```   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, bufferSize, bufferType = 'DQN', **kwargs):\n",
    "        # this function creates the relevant data-structures, and intializes all relevant variables\n",
    "        # it can take variable number of parameters like alpha, beta, beta_rate (required for PER)\n",
    "        # here the bufferType variable can be used to maintain one class for all types of agents\n",
    "        # using the bufferType parameter in the methods below, you can implement all possible functionalities \n",
    "        # that could be used for different types of agents\n",
    "        \n",
    "        # permissible values for bufferType = NFQ, DQN, DDQN, D3QN and PER-D3QN\n",
    "        \n",
    "        #Your code goes in here\n",
    "        self.bufferSize = bufferSize\n",
    "        self.bufferType = bufferType\n",
    "        self.buffer     = deque(maxlen = bufferSize)\n",
    "        \n",
    "        self.is_priority = False\n",
    "        if bufferType == 'PER-D3QN':\n",
    "            self.is_priority = True\n",
    "            self.priority_buffer = deque(maxlen = bufferSize)\n",
    "            self.priority_alpha = kwargs['priority_alpha']\n",
    "            self.priority_beta  = kwargs['priority_beta']\n",
    "            self.priority_beta_rate = kwargs['priority_beta_rate']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "    def store(self, experience):\n",
    "        #stores the experiences, based on parameters in init it can assign priorities, etc.  \n",
    "        #\n",
    "        #this function does not return anything\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        \n",
    "        # for normal cases \n",
    "        \n",
    "        #print(\"I am here .......... in self store fun\")\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "        # for PER-D3QN\n",
    "        if self.is_priority:\n",
    "            if self.priority_buffer:\n",
    "                max_priority = max(self.priority_buffer)\n",
    "            else:\n",
    "                max_priority = 1.0  # Set max_priority to a default value if priority_buffer is empty\n",
    "            self.priority_buffer.append(max_priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "    def update(self, indices, priorities):\n",
    "        # This is mainly used for PER-DDQN\n",
    "        # Otherwise just have a pass in this method\n",
    "        #\n",
    "        # This function does not return anything\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        if self.is_priority:\n",
    "            priority_epsilon = 0.001\n",
    "            mean_priority = torch.mean(torch.abs(priorities)).item()\n",
    "            for index in indices:\n",
    "                self.priority_buffer[index] = mean_priority + priority_epsilon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "    def collectExperiences(self,env, state, explorationStrategy, countExperiences, net = None):\n",
    "        #this method allows the agent to interact with the environment starting from a state and it collects\n",
    "        #experiences during the interaction, it uses network to get the value function and uses exploration \n",
    "        #strategy to select action. It collects countExperiences and in case the environment terminates  \n",
    "        #before that it returns the function calling this method needs to handle early termination accordingly.\n",
    "        #\n",
    "        #this function does not return anything\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        \n",
    "        #print(\"I am here .......... in collect exp fun\")\n",
    "        \n",
    "        experience_buffer = []\n",
    "        reward_count = 0\n",
    "        steps = 0\n",
    "        flag = False \n",
    "        state , info = env.reset()\n",
    "        \n",
    "        while not flag :\n",
    "            \n",
    "           \n",
    "            action = explorationStrategy(net, torch.tensor([state], dtype=torch.float32))\n",
    "            new_state , reward , terminated , truncated , info = env.step(action)\n",
    "            \n",
    "            if countExperiences == -1 or steps <= countExperiences :\n",
    "                experience_buffer.append([state , action , reward , terminated , truncated , new_state])\n",
    "                \n",
    "            reward_count += reward \n",
    "            \n",
    "            if terminated or truncated :\n",
    "                state , info = env.reset()\n",
    "                break\n",
    "            \n",
    "            state = new_state\n",
    "            \n",
    "            steps+=1\n",
    "            \n",
    "            if countExperiences != -1 and len(experience_buffer) >= countExperiences:\n",
    "                state , info = env.reset()\n",
    "                flag = True\n",
    "            \n",
    "        self.episode_reward = reward_count \n",
    "        self.episode_total_steps = steps \n",
    "        \n",
    "        # for PER-D3QN \n",
    "        if self.is_priority == True :\n",
    "            # multiply beta by beta_annealing_rate typically 0.99992\n",
    "            self.priority_beta =  self.priority_beta * self.priority_beta_rate\n",
    "            \n",
    "        if countExperiences != -1 and len(experience_buffer) < countExperiences : \n",
    "            return\n",
    "        for exp in experience_buffer :\n",
    "            self.store(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "    def sample(self, batchSize, **kwargs):\n",
    "        # this method returns batchSize number of experiences\n",
    "        # based on extra arguments, it could do sampling or it could return the latest batchSize experiences or\n",
    "        # via some other strategy\n",
    "        #\n",
    "        # in the case of Prioritized Experience Replay (PER) the sampling needs to take into account the priorities\n",
    "        #\n",
    "        # this function returns experiences samples\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        \n",
    "        \n",
    "        # types of sampling -> latest , random and prioritized \n",
    "        \n",
    "        if self.is_priority == True :\n",
    "            \n",
    "            # prioritized sampling\n",
    "            \n",
    "            temp = (np.array(self.priority_buffer) ** self.priority_alpha)\n",
    "            probability = temp / np.sum(temp)\n",
    "            probability_index = random.choices(range(len(self.buffer)), k=batchSize, weights=probability)\n",
    "        \n",
    "            exp_List = [self.buffer[i] for i in probability_index]\n",
    "\n",
    "            weights = (len(self.buffer)*probability[probability_index])\n",
    "            weights = weights ** (-1*self.priority_beta)\n",
    "            weights = weights/np.max(weights)\n",
    "\n",
    "            exp_List = [*zip(exp_List, weights , probability_index)]\n",
    "            return exp_List\n",
    "        \n",
    "        else :\n",
    "            \n",
    "            # latest sampling\n",
    "            \n",
    "            if 'sampling_type' in kwargs and kwargs['sampling_type']=='latest':\n",
    "                exp_List = self.buffer[-batchSize:]\n",
    "                return exp_List\n",
    "            \n",
    "            # random sampling ( by default )\n",
    "            \n",
    "            else :\n",
    "                exp_List = random.sample(self.buffer, batchSize )\n",
    "                return exp_List\n",
    "        \n",
    "        \n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "    def splitExperiences(self, experiences):\n",
    "        #it takes in experiences and gives the following:\n",
    "        #states, actions, rewards, nextStates, dones\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        \n",
    "        if self.is_priority == True :\n",
    "            experiences , weights , index = [*zip(*experiences)]\n",
    "        \n",
    "        \n",
    "        curr_states  = np.asarray([e[0] for e in experiences])\n",
    "        actions      = np.asarray([e[1] for e in experiences])\n",
    "        rewards      = np.asarray([e[2] for e in experiences])\n",
    "        terminated_s = np.asarray([e[3] for e in experiences])\n",
    "        truncated_s  = np.asarray([e[4] for e in experiences])\n",
    "        new_states   = np.asarray([e[5] for e in experiences])\n",
    "        \n",
    "        curr_states_t   = torch.as_tensor(curr_states, dtype=torch.float32)\n",
    "        actions_t       = torch.as_tensor(actions,dtype=torch.int64).unsqueeze(-1)\n",
    "        rewards_t       = torch.as_tensor(rewards,dtype=torch.float32).unsqueeze(-1)\n",
    "        terminated_s_t  = torch.as_tensor(terminated_s,dtype=torch.float32).unsqueeze(-1)\n",
    "        truncated_s_t   = torch.as_tensor(truncated_s,dtype=torch.float32).unsqueeze(-1)\n",
    "        new_states_t    = torch.as_tensor(new_states,dtype=torch.float32)\n",
    "        \n",
    "        if self.is_priority==True:\n",
    "            weights = torch.tensor([weights])\n",
    "            return curr_states_t, actions_t, rewards_t, new_states_t, terminated_s_t, truncated_s_t,weights, index\n",
    "\n",
    "        \n",
    "        return curr_states_t, actions_t, rewards_t, new_states_t, terminated_s_t, truncated_s_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "    def length(self):\n",
    "        #tells the number of experiences stored in the internal buffer\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Neural Fitted Q (NFQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Implement the Neural Fitted Q algorithm. We have studied about NFQ algorithm in the Lecture. Use the function definitions (given below).\n",
    "\n",
    "This class implements the NFQ Agent, you are required to implement the various methods of this class\n",
    "as outlined below. Note this class is generic and should work with any permissible Gym environment. \n",
    "Also please feel free to play with different exploration strategies with decaying paramters (epsilon/temperature)\n",
    "\n",
    "```\n",
    "class NFQ():\n",
    "    def __init__(env, seed, gamma, epochs,\n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn, \n",
    "                 explorationStrategyEvalFn)\n",
    "    def initBookKeeping(self)\n",
    "    def performBookKeeping(self, train = True)\n",
    "    def runNFQ(self)\n",
    "    def trainAgent(self)\n",
    "    def trainNetwork(self, experiences, epochs)\n",
    "    def evaluateAgent(self)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NFQ():\n",
    "    def __init__(self, env, seed, gamma,  \n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn, \n",
    "                 explorationStrategyEvalFn):\n",
    "        #this NFQ method \n",
    "        # 1. creates and initializes (with seed) the environment, train/eval episodes, gamma, etc. \n",
    "        # 2. creates and intializes all the variables required for bookkeeping values via the initBookKeepingmethod\n",
    "        # 3. creates Q-network using the createValueNetwork above\n",
    "        # 4. creates and initializes (with network params) the optimizer function\n",
    "        # 5. sets the explorationStartegy variables/functions for train and evaluation\n",
    "        # 6. sets the batchSize for the number of experiences \n",
    "        # 7. Creates the replayBuffer\n",
    "    \n",
    "        #Your code goes in here\n",
    "        # 1 , 6\n",
    "        self.gamma = gamma \n",
    "        self.buffer_size = bufferSize\n",
    "        self.batch_size = batchSize \n",
    "        self.max_train_ep = MAX_TRAIN_EPISODES\n",
    "        self.max_test_ep = MAX_EVAL_EPISODES\n",
    "        \n",
    "        \n",
    "        # 2\n",
    "        self.train_reward = 0\n",
    "        self.test_reward = 0\n",
    "        self.total_steps = 0 \n",
    "        self.cpu_time = 0\n",
    "        self.wall_time = 0\n",
    "        self.episode = 0\n",
    "        self.initBookKeeping()\n",
    "        \n",
    "        \n",
    "        # 5\n",
    "        self.exploration_train = explorationStrategyTrainFn\n",
    "        self.exploration_eval  = explorationStrategyEvalFn\n",
    "        \n",
    "        self.state = env.reset(seed = seed)\n",
    "        self.seed = seed \n",
    "        self.env = env \n",
    "        \n",
    "        # 3\n",
    "        inDim = int(np.prod(env.observation_space.shape))\n",
    "        outDim = env.action_space.n\n",
    "        self.policy_network = createPolicyNetwork(inDim, outDim, hDim = [512,128], activation = nn.ReLU())\n",
    "        \n",
    "        # 4\n",
    "        self.optimizer = optimizerFn(self.policy_network.parameters(), lr=optimizerLR)\n",
    "        \n",
    "        # 7\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size, bufferType = 'NFQ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NFQ(NFQ):\n",
    "    def initBookKeeping(self):\n",
    "        #this method creates and intializes all the variables required for book-keeping values and it is called\n",
    "        #init method\n",
    "\n",
    "        #Your code goes in here\n",
    "        self.train_reward_array = []\n",
    "        self.test_reward_array = []\n",
    "        self.total_steps_array = []\n",
    "        self.total_cpu_array = []\n",
    "        self.total_wall_array = []\n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NFQ(NFQ):\n",
    "    def performBookKeeping(self, train = True):\n",
    "        #this method updates relevant variables for the bookKeeping, this can be called \n",
    "        #multiple times during training\n",
    "        #if you want you can print information using this, so it may help to monitor progress and also help to debug\n",
    "        \n",
    "    #Your code goes in here\n",
    "        if train == False :\n",
    "            self.test_reward_array.append(self.test_reward)\n",
    "        else :\n",
    "            self.train_reward_array.append(self.train_reward)\n",
    "            self.total_steps_array.append(self.total_steps)\n",
    "            self.total_cpu_array.append(self.cpu_time)\n",
    "            self.total_wall_array.append(self.wall_time)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NFQ(NFQ):\n",
    "    def runNFQ(self):\n",
    "        #this is the main method, it trains the agent, performs bookkeeping while training and finally evaluates\n",
    "        #the agent and returns the following quantities:\n",
    "        #1. episode wise mean train rewards\n",
    "        #2. epsiode wise mean eval rewards \n",
    "        #2. episode wise trainTime (in seconds): time elapsed during training since the start of the first episode \n",
    "        #3. episode wise wallClockTime (in seconds): actual time elapsed since the start of training, \n",
    "        #                               note this will include time for BookKeeping and evaluation \n",
    "        # Note both trainTime and wallClockTime get accumulated as episodes proceed. \n",
    "        \n",
    "        \n",
    "        #Your code goes in here\n",
    "        \n",
    "        \n",
    "        self.trainAgent()\n",
    "        \n",
    "        return self.train_reward_array, self.total_cpu_array, self.total_steps_array, self.total_wall_array, self.test_reward_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NFQ(NFQ):\n",
    "    def trainAgent(self):\n",
    "        #this method collects experiences and trains the NFQ agent and does BookKeeping while training. \n",
    "        #this calls the trainNetwork() method internally, it also evaluates the agent per episode\n",
    "        #it trains the agent for MAX_TRAIN_EPISODES\n",
    "        \n",
    "        #Your code goes in here\n",
    "        \n",
    "        cpu_start = time.time()\n",
    "        wall_start = time.perf_counter()\n",
    "        \n",
    "        for episode in range(self.max_train_ep):\n",
    "            \n",
    "            state , info = self.env.reset(seed = self.seed)\n",
    "            \n",
    "            self.replay_buffer.collectExperiences(self.env, state, self.exploration_train, countExperiences = -1, net = self.policy_network)\n",
    "            \n",
    "            if self.replay_buffer.length() < self.batch_size :\n",
    "                continue \n",
    "                \n",
    "            experience_buffer = self.replay_buffer.sample(self.batch_size)\n",
    "            self.trainNetwork(experience_buffer , 5 )\n",
    "            \n",
    "            self.train_reward = self.replay_buffer.episode_reward \n",
    "            self.total_steps += self.replay_buffer.episode_total_steps\n",
    "            self.cpu_time = time.time()-cpu_start\n",
    "            self.wall_time = time.perf_counter() - wall_start\n",
    "            \n",
    "            self.episode = episode\n",
    "            \n",
    "            self.performBookKeeping(train=True)\n",
    "            self.evaluateAgent()\n",
    "            self.performBookKeeping(train=False)\n",
    "            \n",
    "        \n",
    "        \n",
    "        return self.train_reward_array, self.total_cpu_array, self.total_steps_array, self.total_wall_array\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NFQ(NFQ):\n",
    "    def trainNetwork(self, experiences, epochs):\n",
    "        # this method trains the value network epoch number of times and is called by the trainAgent function\n",
    "        # it essentially uses the experiences to calculate target, using the targets it calculates the error, which\n",
    "        # is further used for calulating the loss. It then uses the optimizer over the loss \n",
    "        # to update the params of the network by backpropagating through the network\n",
    "        # this function does not return anything\n",
    "        # you can try out other loss functions other than MSE like Huber loss, MAE, etc. \n",
    "        \n",
    "        #Your code goes in here\n",
    "        curr_states, actions, rewards, new_states, terminated_s, truncated_s = self.replay_buffer.splitExperiences(experiences)\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            target_q_values      = self.policy_network(new_states).detach()\n",
    "            max_target_q_values  = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "            \n",
    "            q_values  = self.policy_network(curr_states)\n",
    "            temp = torch.logical_or(terminated_s,truncated_s)\n",
    "            targets = rewards + gamma * (torch.logical_not(temp)) * max_target_q_values\n",
    "            action_q_values = torch.gather(input=q_values , dim=1 , index=actions)\n",
    "            \n",
    "            error = F.smooth_l1_loss(action_q_values , targets)  # Huber loss\n",
    "            \n",
    "            # Gradient Descent \n",
    "            self.optimizer.zero_grad()\n",
    "            error.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NFQ(NFQ):\n",
    "    def evaluateAgent(self):\n",
    "        #this function evaluates the agent using the value network, it evaluates agent for MAX_EVAL_EPISODES\n",
    "        #typcially MAX_EVAL_EPISODES = 1\n",
    "        \n",
    "        #Your code goes in here\n",
    "        reward_arr = []\n",
    "        for e in range(self.max_test_ep):\n",
    "            rs = 0\n",
    "            \n",
    "            st, info = self.env.reset()\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            \n",
    "            while not (terminated or truncated):\n",
    "                act = self.exploration_eval(self.policy_network, torch.tensor([st], dtype=torch.float32))\n",
    "                st, rew, terminated, truncated, info = self.env.step(act)\n",
    "                rs += rew\n",
    "                \n",
    "            reward_arr.append(rs)\n",
    "            \n",
    "        self.test_reward = sum(reward_arr) / len(reward_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Deep Q-Network (DQN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Implement the Deep Q algorithm. We have studied about DQN algorithm in the Lecture. Use the function definitions (given below).\n",
    "\n",
    "This class implements the DQN Agent, you are required to implement the various methods of this class\n",
    "as outlined below. Note this class is generic and should work with any permissible Gym environment\n",
    "\n",
    "```\n",
    "class DQN():\n",
    "    def __init__(env, seed, gamma, \n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn, \n",
    "                 explorationStrategyEvalFn,\n",
    "                 updateFrequency)\n",
    "    def initBookKeeping(self)\n",
    "    def performBookKeeping(self, train = True)\n",
    "    def runDQN(self)\n",
    "    def trainAgent(self)\n",
    "    def trainNetwork(self, experiences)\n",
    "    def updateNetwork(self, onlineNet, targetNet)\n",
    "    def evaluateAgent(self)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T10:33:03.909875Z",
     "start_time": "2021-11-05T10:33:03.806829Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self , env, seed, gamma, \n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn, \n",
    "                 explorationStrategyEvalFn,\n",
    "                 updateFrequency):\n",
    "        #this DQN method \n",
    "        # 1. creates and initializes (with seed) the environment, train/eval episodes, gamma, etc. \n",
    "        # 2. creates and intializes all the variables required for book-keeping values via theinitBookKeepingmethod\n",
    "        # 3. creates traget and online Q-networks using the createValueNetwork above\n",
    "        # 4. creates and initializes (with network params) the optimizer function\n",
    "        # 5. sets the explorationStartegy variables/functions for train and evaluation\n",
    "        # 6. sets the batchSize for the number of experiences \n",
    "        # 7. Creates the replayBuffer\n",
    "        \n",
    "        #Your code goes in here\n",
    "        \n",
    "        # 1 , 6\n",
    "        self.gamma = gamma \n",
    "        self.buffer_size = bufferSize\n",
    "        self.batch_size = batchSize \n",
    "        self.max_train_ep = MAX_TRAIN_EPISODES\n",
    "        self.max_test_ep = MAX_EVAL_EPISODES\n",
    "        self.update_frequency = updateFrequency\n",
    "        \n",
    "        # 2\n",
    "        self.train_reward = 0\n",
    "        self.test_reward = 0\n",
    "        self.total_steps = 0 \n",
    "        self.cpu_time = 0\n",
    "        self.wall_time = 0\n",
    "        self.episode = 0\n",
    "        self.initBookKeeping()\n",
    "        \n",
    "        \n",
    "        # 5\n",
    "        self.exploration_train = explorationStrategyTrainFn\n",
    "        self.exploration_eval  = explorationStrategyEvalFn\n",
    "        \n",
    "        self.state = env.reset(seed = seed)\n",
    "        self.seed = seed \n",
    "        self.env = env \n",
    "        \n",
    "        # 3\n",
    "        inDim = int(np.prod(env.observation_space.shape))\n",
    "        outDim = env.action_space.n\n",
    "        self.policy_network = createPolicyNetwork(inDim, outDim, hDim = [512,128], activation = nn.ReLU())\n",
    "        self.target_network = copy.deepcopy(self.policy_network)\n",
    "        \n",
    "        # 4\n",
    "        self.optimizer = optimizerFn(self.policy_network.parameters(), lr=optimizerLR)\n",
    "        \n",
    "        # 7\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size, bufferType = 'DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def initBookKeeping(self):\n",
    "        #this method creates and intializes all the variables required for book-keeping values and it is called\n",
    "        #init method\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        \n",
    "        self.train_reward_array = []\n",
    "        self.test_reward_array = []\n",
    "        self.total_steps_array = []\n",
    "        self.total_cpu_array = []\n",
    "        self.total_wall_array = []\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def performBookKeeping(self, train = True):\n",
    "        #this method updates relevant variables for the bookKeeping, this can be called \n",
    "        #multiple times during training\n",
    "        #if you want you can print information using this, so it may help to monitor progress and also help to debug\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        \n",
    "        if train == False :\n",
    "            self.test_reward_array.append(self.test_reward)\n",
    "        else :\n",
    "            self.train_reward_array.append(self.train_reward)\n",
    "            self.total_steps_array.append(self.total_steps)\n",
    "            self.total_cpu_array.append(self.cpu_time)\n",
    "            self.total_wall_array.append(self.wall_time)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def runDQN(self):\n",
    "        #this is the main method, it trains the agent, performs bookkeeping while training and finally evaluates\n",
    "        #the agent and returns the following quantities:\n",
    "        #1. episode wise mean train rewards\n",
    "        #2. epsiode wise mean eval rewards \n",
    "        #2. episode wise trainTime (in seconds): time elapsed during training since the start of the first episode \n",
    "        #3. episode wise wallClockTime (in seconds): actual time elapsed since the start of training, \n",
    "        #                               note this will include time for BookKeeping and evaluation \n",
    "        # Note both trainTime and wallClockTime get accumulated as episodes proceed. \n",
    "        #\n",
    "        #Your code goes in here\n",
    "        \n",
    "        self.trainAgent()\n",
    "        \n",
    "        return self.train_reward_array, self.total_cpu_array, self.total_steps_array, self.total_wall_array, self.test_reward_array\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def trainAgent(self):\n",
    "        #this method collects experiences and trains the agent and does BookKeeping while training. \n",
    "        #this calls the trainNetwork() method internally, it also evaluates the agent per episode\n",
    "        #it trains the agent for MAX_TRAIN_EPISODES\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        \n",
    "        \n",
    "        #print(\"I am here .......... in train agent fun\")\n",
    "        self.updateNetwork(self.policy_network, self.target_network)\n",
    "        cpu_start = time.time()\n",
    "        wall_start = time.perf_counter()\n",
    "        \n",
    "        for episode in range(self.max_train_ep):\n",
    "            \n",
    "            state , info = self.env.reset(seed = self.seed)\n",
    "            \n",
    "            self.replay_buffer.collectExperiences(self.env, state, self.exploration_train, countExperiences = -1, net = self.policy_network)\n",
    "            \n",
    "            if self.replay_buffer.length() < self.batch_size :\n",
    "                continue \n",
    "                \n",
    "            experience_buffer = self.replay_buffer.sample(self.batch_size)\n",
    "            self.trainNetwork(experience_buffer , 5 )\n",
    "            \n",
    "            self.train_reward = self.replay_buffer.episode_reward \n",
    "            self.total_steps += self.replay_buffer.episode_total_steps\n",
    "            self.cpu_time = time.time()-cpu_start\n",
    "            self.wall_time = time.perf_counter() - wall_start\n",
    "            \n",
    "            self.episode = episode\n",
    "            \n",
    "            self.performBookKeeping(train=True)\n",
    "            self.evaluateAgent()\n",
    "            self.performBookKeeping(train=False)\n",
    "            \n",
    "            if episode % self.update_frequency == 0 :\n",
    "                self.updateNetwork(self.policy_network, self.target_network)\n",
    "        \n",
    "        \n",
    "        return self.train_reward_array, self.total_cpu_array, self.total_steps_array, self.total_wall_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def trainNetwork(self, experiences, epochs):\n",
    "        # this method trains the value network epoch number of times and is called by the trainAgent function\n",
    "        # it essentially uses the experiences to calculate target, using the targets it calculates the error, which\n",
    "        # is further used for calulating the loss. It then uses the optimizer over the loss \n",
    "        # to update the params of the network by backpropagating through the network\n",
    "        # this function does not return anything\n",
    "        # you can try out other loss functions other than MSE like Huber loss, MAE, etc. \n",
    "        #\n",
    "        #Your code goes in here\n",
    "        \n",
    "        #print(\"I am here .......... in train class\")\n",
    "        curr_states, actions, rewards, new_states, terminated_s, truncated_s = self.replay_buffer.splitExperiences(experiences)\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            target_q_values      = self.target_network(new_states).detach()\n",
    "            max_target_q_values  = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "            \n",
    "            q_values  = self.policy_network(curr_states)\n",
    "            temp = torch.logical_or(terminated_s,truncated_s)\n",
    "            targets = rewards + gamma * (torch.logical_not(temp)) * max_target_q_values\n",
    "            action_q_values = torch.gather(input=q_values , dim=1 , index=actions)\n",
    "            \n",
    "            error = F.smooth_l1_loss(action_q_values , targets)  # Huber loss\n",
    "            \n",
    "            # Gradient Descent \n",
    "            self.optimizer.zero_grad()\n",
    "            error.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def updateNetwork(self, onlineNet, targetNet):\n",
    "        #this function updates the onlineNetwork with the target network\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        targetNet.load_state_dict(onlineNet.state_dict())\n",
    "\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def evaluateAgent(self):\n",
    "        #this function evaluates the agent using the value network, it evaluates agent for MAX_EVAL_EPISODES\n",
    "        #typically MAX_EVAL_EPISODES = 1\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        reward_arr = []\n",
    "        for e in range(self.max_test_ep):\n",
    "            rs = 0\n",
    "            \n",
    "            st, info = self.env.reset()\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            \n",
    "            while not (terminated or truncated):\n",
    "                act = self.exploration_eval(self.policy_network, torch.tensor([st], dtype=torch.float32))\n",
    "                st, rew, terminated, truncated, info = self.env.step(act)\n",
    "                rs += rew\n",
    "                \n",
    "            reward_arr.append(rs)\n",
    "            \n",
    "        self.test_reward = sum(reward_arr) / len(reward_arr)\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_trainRewardsList=dict()\n",
    "DQN_trainTimeList = dict()\n",
    "DQN_evalRewardsList = dict()\n",
    "DQN_wallClockTimeList=dict()\n",
    "DQN_finalEvalReward=dict()\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "\n",
    "#DQN\n",
    "agent_DQN = DQN(env, seed=i, gamma=0.99, bufferSize=1000, batchSize=32, \n",
    "        optimizerFn=optim.Adam, optimizerLR=0.00075, MAX_TRAIN_EPISODES=500, MAX_EVAL_EPISODES=1, \n",
    "        explorationStrategyTrainFn= selectEpsilonGreedyAction, explorationStrategyEvalFn= selectGreedyAction, \n",
    "        updateFrequency=25)\n",
    "DQN_trainRewardsList[1], DQN_trainTimeList[1], DQN_evalRewardsList[1], DQN_wallClockTimeList[1], DQN_finalEvalReward[1] = agent_DQN.runDQN()\n",
    "#print(DQN_trainRewardsList[1])\n",
    "\n",
    "plotQuantity(DQN_trainRewardsList, 500, ['Training Reward' , 'Training reward vs episodes' , 'DQN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Double DQN (DDQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Implement the Double DQN agent. We have studied about Double DQN agent in the Lecture. Use the function definitions (given below).\n",
    "\n",
    "This class implements the Double DQN agent, you are required to implement the various methods of this class\n",
    "as outlined below. Note this class is generic and should work with any permissible Gym environment\n",
    "\n",
    "```\n",
    "class DDQN():\n",
    "    def __init__(env, seed, gamma, \n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn, \n",
    "                 explorationStrategyEvalFn,\n",
    "                 updateFrequency)\n",
    "    def initBookKeeping(self)\n",
    "    def performBookKeeping(self, train = True)\n",
    "    def runDDQN(self)\n",
    "    def trainAgent(self)\n",
    "    def trainNetwork(self, experiences)\n",
    "    def updateNetwork(self, onlineNet, targetNet)\n",
    "    def evaluateAgent(self)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DDQN():\n",
    "    def __init__(self,env, seed, gamma, \n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn, \n",
    "                 explorationStrategyEvalFn,\n",
    "                 updateFrequency):\n",
    "        #this DDQN method \n",
    "        # 1. creates and initializes (with seed) the environment, train/eval episodes, gamma, etc. \n",
    "        # 2. creates and intializes all the variables required for book-keeping values via the initBookKeeping method\n",
    "        # 3. creates tareget and online Q-networks using the createValueNetwork above\n",
    "        # 4. creates and initializes (with network params) the optimizer function\n",
    "        # 5. sets the explorationStartegy variables/functions for train and evaluation\n",
    "        # 6. sets the batchSize for the number of experiences \n",
    "        # 7. Creates the replayBuffer\n",
    "        \n",
    "        #Your code goes in here\n",
    "        \n",
    "        # 1 , 6\n",
    "        self.gamma = gamma \n",
    "        self.buffer_size = bufferSize\n",
    "        self.batch_size = batchSize \n",
    "        self.max_train_ep = MAX_TRAIN_EPISODES\n",
    "        self.max_test_ep = MAX_EVAL_EPISODES\n",
    "        self.update_frequency = updateFrequency\n",
    "        \n",
    "        # 2\n",
    "        self.train_reward = 0\n",
    "        self.test_reward = 0\n",
    "        self.total_steps = 0 \n",
    "        self.cpu_time = 0\n",
    "        self.wall_time = 0\n",
    "        self.episode = 0\n",
    "        self.initBookKeeping()\n",
    "        \n",
    "        \n",
    "        # 5\n",
    "        self.exploration_train = explorationStrategyTrainFn\n",
    "        self.exploration_eval  = explorationStrategyEvalFn\n",
    "        \n",
    "        self.state = env.reset(seed = seed)\n",
    "        self.seed = seed \n",
    "        self.env = env \n",
    "        \n",
    "        # 3\n",
    "        inDim = int(np.prod(env.observation_space.shape))\n",
    "        outDim = env.action_space.n\n",
    "        self.policy_network = createPolicyNetwork(inDim, outDim, hDim = [512,128], activation = nn.ReLU())\n",
    "        self.target_network = copy.deepcopy(self.policy_network)\n",
    "        \n",
    "        # 4\n",
    "        self.optimizer = optimizerFn(self.policy_network.parameters(), lr=optimizerLR)\n",
    "        \n",
    "        # 7\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size, bufferType = 'DDQN')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DDQN(DDQN):\n",
    "    def initBookKeeping(self):\n",
    "        #this method creates and intializes all the variables required for book-keeping values and it is called\n",
    "        #init method\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        \n",
    "        self.train_reward_array = []\n",
    "        self.test_reward_array = []\n",
    "        self.total_steps_array = []\n",
    "        self.total_cpu_array = []\n",
    "        self.total_wall_array = []\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DDQN(DDQN):\n",
    "    def performBookKeeping(self, train = True):\n",
    "        #this method updates relevant variables for the bookKeeping, this can be called \n",
    "        #multiple times during training\n",
    "        #if you want you can print information using this, so it may help to monitor progress and also help to debug\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        \n",
    "        if train == False :\n",
    "            self.test_reward_array.append(self.test_reward)\n",
    "        else :\n",
    "            self.train_reward_array.append(self.train_reward)\n",
    "            self.total_steps_array.append(self.total_steps)\n",
    "            self.total_cpu_array.append(self.cpu_time)\n",
    "            self.total_wall_array.append(self.wall_time)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DDQN(DDQN):\n",
    "    def runDDQN(self):\n",
    "        #this is the main method, it trains the agent, performs bookkeeping while training and finally evaluates\n",
    "        #the agent and returns the following quantities:\n",
    "        #1. episode wise mean train rewards\n",
    "        #2. epsiode wise mean eval rewards \n",
    "        #2. episode wise trainTime (in seconds): time elapsed during training since the start of the first episode \n",
    "        #3. episode wise wallClockTime (in seconds): actual time elapsed since the start of training, \n",
    "        #                               note this will include time for BookKeeping and evaluation \n",
    "        # Note both trainTime and wallClockTime get accumulated as episodes proceed. \n",
    "        \n",
    "        #Your code goes in here\n",
    "        \n",
    "        self.trainAgent()\n",
    "        \n",
    "        return self.train_reward_array, self.total_cpu_array, self.total_steps_array, self.total_wall_array, self.test_reward_array\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DDQN(DDQN):\n",
    "    def trainAgent(self):\n",
    "        #this method collects experiences and trains the agent and does BookKeeping while training. \n",
    "        #this calls the trainNetwork() method internally, it also evaluates the agent per episode\n",
    "        #it trains the agent for MAX_TRAIN_EPISODES\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        self.updateNetwork(self.policy_network, self.target_network)\n",
    "        cpu_start = time.time()\n",
    "        wall_start = time.perf_counter()\n",
    "        \n",
    "        for episode in range(self.max_train_ep):\n",
    "            \n",
    "            state , info = self.env.reset(seed = self.seed)\n",
    "            \n",
    "            self.replay_buffer.collectExperiences(self.env, state, self.exploration_train, countExperiences = -1, net = self.policy_network)\n",
    "            \n",
    "            if self.replay_buffer.length() < self.batch_size :\n",
    "                continue \n",
    "                \n",
    "            experience_buffer = self.replay_buffer.sample(self.batch_size)\n",
    "            self.trainNetwork(experience_buffer , 5 )\n",
    "            \n",
    "            self.train_reward = self.replay_buffer.episode_reward \n",
    "            self.total_steps += self.replay_buffer.episode_total_steps\n",
    "            self.cpu_time = time.time()-cpu_start\n",
    "            self.wall_time = time.perf_counter() - wall_start\n",
    "            \n",
    "            self.episode = episode\n",
    "            \n",
    "            self.performBookKeeping(train=True)\n",
    "            self.evaluateAgent()\n",
    "            self.performBookKeeping(train=False)\n",
    "            \n",
    "            if episode % self.update_frequency == 0 :\n",
    "                self.updateNetwork(self.policy_network, self.target_network)\n",
    "        \n",
    "        \n",
    "        return self.train_reward_array, self.total_cpu_array, self.total_steps_array, self.total_wall_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DDQN(DDQN):\n",
    "    def trainNetwork(self, experiences, epochs):\n",
    "        # This method trains the value network epoch number of times and is called by the trainAgent function.\n",
    "        # It uses experiences to calculate targets, then calculates the error for updating the network parameters.\n",
    "        # It does not return anything.\n",
    "\n",
    "        curr_states, actions, rewards, new_states, terminated_s, truncated_s = self.replay_buffer.splitExperiences(experiences)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "\n",
    "            new_q_values_policy = self.policy_network(new_states)\n",
    "            new_actions = new_q_values_policy.argmax(dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "            new_q_values_target = self.target_network(new_states).detach()\n",
    "            max_new_q_values_target = new_q_values_target.gather(1, new_actions)\n",
    "\n",
    "            temp = torch.logical_or(terminated_s, truncated_s)\n",
    "            targets = rewards + self.gamma * (torch.logical_not(temp)) * max_new_q_values_target\n",
    "\n",
    "            q_values = self.policy_network(curr_states)\n",
    "            action_q_values = q_values.gather(1, actions)\n",
    "\n",
    "            error = F.smooth_l1_loss(action_q_values, targets)\n",
    "\n",
    "            # gradient descent\n",
    "            self.optimizer.zero_grad()\n",
    "            error.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DDQN(DDQN):\n",
    "    def updateNetwork(self, onlineNet, targetNet):\n",
    "        #this function updates the onlineNetwork with the target network\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        targetNet.load_state_dict(onlineNet.state_dict())\n",
    "\n",
    "        \n",
    "        return\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DDQN(DDQN):\n",
    "    def evaluateAgent(self):\n",
    "        #this function evaluates the agent using the value network, it evaluates agent for MAX_EVAL_EPISODES\n",
    "        #typcially MAX_EVAL_EPISODES = 1\n",
    "        \n",
    "        #Your code goes in here\n",
    "        reward_arr = []\n",
    "        for e in range(self.max_test_ep):\n",
    "            rs = 0\n",
    "            \n",
    "            st, info = self.env.reset()\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            \n",
    "            while not (terminated or truncated):\n",
    "                act = self.exploration_eval(self.policy_network, torch.tensor([st], dtype=torch.float32))\n",
    "                st, rew, terminated, truncated, info = self.env.step(act)\n",
    "                rs += rew\n",
    "                \n",
    "            reward_arr.append(rs)\n",
    "            \n",
    "        self.test_reward = sum(reward_arr) / len(reward_arr)\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dueling DDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Implement the Dueling Double Deep Q algorithm. We have studied about Dueling Double DQN agent in the Lecture. Use the function definitions (given below).\n",
    "\n",
    "This class implements the Dueling Double DQN agent, you are required to implement the various methods of this class\n",
    "as outlined below. Note this class is generic and should work with any permissible Gym environment\n",
    "\n",
    "```\n",
    "class D3QN():\n",
    "    def __init__(env, seed, gamma, tau, \n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn, \n",
    "                 explorationStrategyEvalFn,\n",
    "                 updateFrequency)\n",
    "    def initBookKeeping(self)\n",
    "    def performBookKeeping(self, train = True)\n",
    "    def runD3QN(self)\n",
    "    def trainAgent(self)\n",
    "    def trainNetwork(self, experiences)\n",
    "    def updateNetwork(self, onlineNet, targetNet)\n",
    "    def evaluateAgent(self)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class D3QN():\n",
    "    def __init__(self , env, seed, gamma, tau, \n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn, \n",
    "                 explorationStrategyEvalFn,\n",
    "                 updateFrequency):\n",
    "        #this D3QN method \n",
    "        # 1. creates and initializes (with seed) the environment, train/eval episodes, gamma, etc. \n",
    "        # 2. creates and intializes all the variables required for book-keeping values via the initBookKeeping method\n",
    "        # 3. creates tareget and online Q-networks using the createValueNetwork above\n",
    "        # 4. creates and initializes (with network params) the optimizer function\n",
    "        # 5. sets the explorationStartegy variables/functions for train and evaluation\n",
    "        # 6. sets the batchSize for the number of experiences \n",
    "        # 7. Creates the replayBuffer\n",
    "        \n",
    "        #Your code goes in here\n",
    "        # 1 , 6\n",
    "        self.gamma = gamma \n",
    "        self.buffer_size = bufferSize\n",
    "        self.batch_size = batchSize \n",
    "        self.max_train_ep = MAX_TRAIN_EPISODES\n",
    "        self.max_test_ep = MAX_EVAL_EPISODES\n",
    "        self.update_frequency = updateFrequency\n",
    "        self.tau = tau\n",
    "        \n",
    "        # 2\n",
    "        self.train_reward = 0\n",
    "        self.test_reward = 0\n",
    "        self.total_steps = 0 \n",
    "        self.cpu_time = 0\n",
    "        self.wall_time = 0\n",
    "        self.episode = 0\n",
    "        self.initBookKeeping()\n",
    "        \n",
    "        \n",
    "        # 5\n",
    "        self.exploration_train = explorationStrategyTrainFn\n",
    "        self.exploration_eval  = explorationStrategyEvalFn\n",
    "        \n",
    "        self.state = env.reset(seed = seed)\n",
    "        self.seed = seed \n",
    "        self.env = env \n",
    "        \n",
    "        # 3\n",
    "        inDim = int(np.prod(env.observation_space.shape))\n",
    "        outDim = env.action_space.n\n",
    "        self.policy_network = createDuelingNetwork(inDim, outDim, hDim = [512,128], activation = nn.ReLU())\n",
    "        self.target_network = copy.deepcopy(self.policy_network)\n",
    "        \n",
    "        # 4\n",
    "        self.optimizer = optimizerFn(self.policy_network.parameters(), lr=optimizerLR)\n",
    "        \n",
    "        # 7\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size, bufferType = 'D3QN')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class D3QN(D3QN):\n",
    "    def initBookKeeping(self):\n",
    "        #this method creates and intializes all the variables required for book-keeping values and it is called\n",
    "        #init method\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        \n",
    "        self.train_reward_array = []\n",
    "        self.test_reward_array = []\n",
    "        self.total_steps_array = []\n",
    "        self.total_cpu_array = []\n",
    "        self.total_wall_array = []\n",
    "        return\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class D3QN(D3QN):\n",
    "    def performBookKeeping(self, train = True):\n",
    "        #this method updates relevant variables for the bookKeeping, this can be called \n",
    "        #multiple times during training\n",
    "        #if you want you can print information using this, so it may help to monitor progress and also help to debug\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        \n",
    "        if train == False :\n",
    "            self.test_reward_array.append(self.test_reward)\n",
    "        else :\n",
    "            self.train_reward_array.append(self.train_reward)\n",
    "            self.total_steps_array.append(self.total_steps)\n",
    "            self.total_cpu_array.append(self.cpu_time)\n",
    "            self.total_wall_array.append(self.wall_time)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class D3QN(D3QN):\n",
    "    def runD3QN(self):\n",
    "        #this is the main method, it trains the agent, performs bookkeeping while training and finally evaluates\n",
    "        #the agent and returns the following quantities:\n",
    "        #1. episode wise mean train rewards\n",
    "        #2. epsiode wise mean eval rewards \n",
    "        #2. episode wise trainTime (in seconds): time elapsed during training since the start of the first episode \n",
    "        #3. episode wise wallClockTime (in seconds): actual time elapsed since the start of training, \n",
    "        #                               note this will include time for BookKeeping and evaluation \n",
    "        # Note both trainTime and wallClockTime get accumulated as episodes proceed. \n",
    "        \n",
    "        #Your code goes in here\n",
    "        #Your code goes in here\n",
    "        \n",
    "        self.trainAgent()\n",
    "        \n",
    "        return self.train_reward_array, self.total_cpu_array, self.total_steps_array, self.total_wall_array, self.test_reward_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class D3QN(D3QN):\n",
    "    def trainAgent(self):\n",
    "        #this method collects experiences and trains the agent and does BookKeeping while training. \n",
    "        #this calls the trainNetwork() method internally, it also evaluates the agent per episode\n",
    "        #it trains the agent for MAX_TRAIN_EPISODES\n",
    "        \n",
    "        #Your code goes in here\n",
    "        self.updateNetwork(self.policy_network, self.target_network)\n",
    "        cpu_start = time.time()\n",
    "        wall_start = time.perf_counter()\n",
    "        \n",
    "        for episode in range(self.max_train_ep):\n",
    "            \n",
    "            state , info = self.env.reset(seed = self.seed)\n",
    "            \n",
    "            self.replay_buffer.collectExperiences(self.env, state, self.exploration_train, countExperiences = -1, net = self.policy_network)\n",
    "            \n",
    "            if self.replay_buffer.length() < self.batch_size :\n",
    "                continue \n",
    "                \n",
    "            experience_buffer = self.replay_buffer.sample(self.batch_size)\n",
    "            self.trainNetwork(experience_buffer , 5 )\n",
    "            \n",
    "            self.train_reward = self.replay_buffer.episode_reward \n",
    "            self.total_steps += self.replay_buffer.episode_total_steps\n",
    "            self.cpu_time = time.time()-cpu_start\n",
    "            self.wall_time = time.perf_counter() - wall_start\n",
    "            \n",
    "            self.episode = episode\n",
    "            \n",
    "            self.performBookKeeping(train=True)\n",
    "            self.evaluateAgent()\n",
    "            self.performBookKeeping(train=False)\n",
    "            \n",
    "            if episode % self.update_frequency == 0 :\n",
    "                self.updateNetwork(self.policy_network, self.target_network)\n",
    "        \n",
    "        \n",
    "        return self.train_reward_array, self.total_cpu_array, self.total_steps_array, self.total_wall_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class D3QN(D3QN):\n",
    "    def trainNetwork(self, experiences, epochs):\n",
    "        # this method trains the value network epoch number of times and is called by the trainAgent function\n",
    "        # it essentially uses the experiences to calculate target, using the targets it calculates the error, which\n",
    "        # is further used for calulating the loss. It then uses the optimizer over the loss \n",
    "        # to update the params of the network by backpropagating through the network\n",
    "        # this function does not return anything\n",
    "        # you can try out other loss functions other than MSE like Huber loss, MAE, etc. \n",
    "        \n",
    "        #Your code goes in here\n",
    "\n",
    "        curr_states, actions, rewards, new_states, terminated_s, truncated_s = self.replay_buffer.splitExperiences(experiences)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "\n",
    "            new_q_values_policy = self.policy_network(new_states)\n",
    "            new_actions = new_q_values_policy.argmax(dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "            new_q_values_target = self.target_network(new_states).detach()\n",
    "            max_new_q_values_target = new_q_values_target.gather(1, new_actions)\n",
    "\n",
    "            temp = torch.logical_or(terminated_s, truncated_s)\n",
    "            targets = rewards + self.gamma * (torch.logical_not(temp)) * max_new_q_values_target\n",
    "\n",
    "            q_values = self.policy_network(curr_states)\n",
    "            action_q_values = q_values.gather(1, actions)\n",
    "\n",
    "            error = F.smooth_l1_loss(action_q_values, targets)\n",
    "\n",
    "            # gradient descent\n",
    "            self.optimizer.zero_grad()\n",
    "            error.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class D3QN(D3QN):\n",
    "    def updateNetwork(self, onlineNet, targetNet):\n",
    "        #this function updates the onlineNetwork with the target network using Polyak averaging\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        with torch.no_grad():\n",
    "            for paramOnline, paramTarget in zip(onlineNet.parameters(), targetNet.parameters()):\n",
    "                paramTarget.data = self.tau * paramOnline.data + (1 - self.tau) * paramTarget.data\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class D3QN(D3QN):\n",
    "    def evaluateAgent(self):\n",
    "        #this function evaluates the agent using the value network, it evaluates agent for MAX_EVAL_EPISODES\n",
    "        #typcially MAX_EVAL_EPISODES = 1\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        reward_arr = []\n",
    "        for e in range(self.max_test_ep):\n",
    "            rs = 0\n",
    "            \n",
    "            st, info = self.env.reset()\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            \n",
    "            while not (terminated or truncated):\n",
    "                act = self.exploration_eval(self.policy_network, torch.tensor([st], dtype=torch.float32))\n",
    "                st, rew, terminated, truncated, info = self.env.step(act)\n",
    "                rs += rew\n",
    "                \n",
    "            reward_arr.append(rs)\n",
    "            \n",
    "        self.test_reward = sum(reward_arr) / len(reward_arr)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dueling Double Deep Q Network with Prioritized Experience Replay (D3QN-PER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Implement the Dueling Double DQN with Prioritized Experience Replay (D3QN-PER) agent. We have studied about D3QN-PER agent in the Lecture. Use the function definitions (given below).\n",
    "\n",
    "This class implements the D3QN-PER agent, you are required to implement the various methods of this class\n",
    "as outlined below. Note this class is generic and should work with any permissible Gym environment\n",
    "\n",
    "```\n",
    "class D3QN_PER():\n",
    "    def __init__(env, seed, gamma, tau, alpha, beta, beta_rate, \n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn, \n",
    "                 explorationStrategyEvalFn,\n",
    "                 updateFrequency)\n",
    "    def initBookKeeping(self)\n",
    "    def performBookKeeping(self, train = True)\n",
    "    def runD3QN_PER(self)\n",
    "    def trainAgent(self)\n",
    "    def trainNetwork(self, experiences)\n",
    "    def updateNetwork(self, onlineNet, targetNet)\n",
    "    def evaluateAgent(self)\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class D3QN_PER():\n",
    "    def __init__(self,env, seed, gamma, tau, alpha, beta, beta_rate, \n",
    "                 bufferSize,\n",
    "                 batchSize,\n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn, \n",
    "                 explorationStrategyEvalFn,\n",
    "                 updateFrequency):\n",
    "        #this D3QN method \n",
    "        # 1. creates and initializes (with seed) the environment, train/eval episodes, gamma, etc. \n",
    "        # 2. creates and intializes all the variables required for book-keeping values via the initBookKeeping method\n",
    "        # 3. creates tareget and online Q-networks using the createValueNetwork above\n",
    "        # 4. creates and initializes (with network params) the optimizer function\n",
    "        # 5. sets the explorationStartegy variables/functions for train and evaluation\n",
    "        # 6. sets the batchSize for the number of experiences \n",
    "        # 7. Creates the replayBuffer, \n",
    "        #    the replayBuffer takes the parameters bufferSize, alpha, beta and beta_rate\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        # 1 , 6\n",
    "        self.gamma = gamma \n",
    "        self.buffer_size = bufferSize\n",
    "        self.batch_size = batchSize \n",
    "        self.max_train_ep = MAX_TRAIN_EPISODES\n",
    "        self.max_test_ep = MAX_EVAL_EPISODES\n",
    "        self.update_frequency = updateFrequency\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha \n",
    "        self.beta = beta \n",
    "        self.beta_rate  = beta_rate\n",
    "        # 2\n",
    "        self.train_reward = 0\n",
    "        self.test_reward = 0\n",
    "        self.total_steps = 0 \n",
    "        self.cpu_time = 0\n",
    "        self.wall_time = 0\n",
    "        self.episode = 0\n",
    "        self.initBookKeeping()\n",
    "        \n",
    "        \n",
    "        # 5\n",
    "        self.exploration_train = explorationStrategyTrainFn\n",
    "        self.exploration_eval  = explorationStrategyEvalFn\n",
    "        \n",
    "        self.state = env.reset(seed = seed)\n",
    "        self.seed = seed \n",
    "        self.env = env \n",
    "        \n",
    "        # 3\n",
    "        inDim = int(np.prod(env.observation_space.shape))\n",
    "        outDim = env.action_space.n\n",
    "        self.policy_network = createDuelingNetwork(inDim, outDim, hDim = [512,128], activation = nn.ReLU())\n",
    "        self.target_network = copy.deepcopy(self.policy_network)\n",
    "        \n",
    "        # 4\n",
    "        self.optimizer = optimizerFn(self.policy_network.parameters(), lr=optimizerLR)\n",
    "        \n",
    "        # 7\n",
    "        self.replay_buffer = ReplayBuffer(bufferSize, bufferType = 'PER-D3QN', priority_alpha=alpha,priority_beta=beta, priority_beta_rate=beta_rate)\n",
    "                                          \n",
    "                                          \n",
    "                                          \n",
    "                                          \n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class D3QN_PER(D3QN_PER):\n",
    "    def initBookKeeping(self):\n",
    "        #this method creates and intializes all the variables required for book-keeping values and it is called\n",
    "        #init method\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        self.train_reward_array = []\n",
    "        self.test_reward_array = []\n",
    "        self.total_steps_array = []\n",
    "        self.total_cpu_array = []\n",
    "        self.total_wall_array = []\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class D3QN_PER(D3QN_PER):\n",
    "    def performBookKeeping(self, train = True):\n",
    "        #this method updates relevant variables for the bookKeeping, this can be called \n",
    "        #multiple times during training\n",
    "        #if you want you can print information using this, so it may help to monitor progress and also help to debug\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        \n",
    "        if train == False :\n",
    "            self.test_reward_array.append(self.test_reward)\n",
    "        else :\n",
    "            self.train_reward_array.append(self.train_reward)\n",
    "            self.total_steps_array.append(self.total_steps)\n",
    "            self.total_cpu_array.append(self.cpu_time)\n",
    "            self.total_wall_array.append(self.wall_time)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class D3QN_PER(D3QN_PER):\n",
    "    def runD3QN_PER(self):\n",
    "        #this is the main method, it trains the agent, performs bookkeeping while training and finally evaluates\n",
    "        #the agent and returns the following quantities:\n",
    "        #1. episode wise mean train rewards\n",
    "        #2. epsiode wise mean eval rewards \n",
    "        #2. episode wise trainTime (in seconds): time elapsed during training since the start of the first episode \n",
    "        #3. episode wise wallClockTime (in seconds): actual time elapsed since the start of training, \n",
    "        #                               note this will include time for BookKeeping and evaluation \n",
    "        # Note both trainTime and wallClockTime get accumulated as episodes proceed. \n",
    "        #\n",
    "        # Your code goes in here\n",
    "        self.trainAgent()\n",
    "        \n",
    "        return self.train_reward_array, self.total_cpu_array, self.total_steps_array, self.total_wall_array, self.test_reward_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class D3QN_PER(D3QN_PER):\n",
    "    def trainAgent(self):\n",
    "        #this method collects experiences and trains the agent and does BookKeeping while training. \n",
    "        #this calls the trainNetwork() method internally, it also evaluates the agent per episode\n",
    "        #it trains the agent for MAX_TRAIN_EPISODES\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        self.updateNetwork(self.policy_network, self.target_network)\n",
    "        cpu_start = time.time()\n",
    "        wall_start = time.perf_counter()\n",
    "        \n",
    "        for episode in range(self.max_train_ep):\n",
    "            \n",
    "            state , info = self.env.reset(seed = self.seed)\n",
    "            \n",
    "            self.replay_buffer.collectExperiences(self.env, state, self.exploration_train, countExperiences = -1, net = self.policy_network)\n",
    "            \n",
    "            if self.replay_buffer.length() < self.batch_size :\n",
    "                continue \n",
    "                \n",
    "            experience_buffer = self.replay_buffer.sample(self.batch_size)\n",
    "            self.trainNetwork(experience_buffer , 5 )\n",
    "            \n",
    "            self.train_reward = self.replay_buffer.episode_reward \n",
    "            self.total_steps += self.replay_buffer.episode_total_steps\n",
    "            self.cpu_time = time.time()-cpu_start\n",
    "            self.wall_time = time.perf_counter() - wall_start\n",
    "            \n",
    "            self.episode = episode\n",
    "            \n",
    "            self.performBookKeeping(train=True)\n",
    "            self.evaluateAgent()\n",
    "            self.performBookKeeping(train=False)\n",
    "            \n",
    "            if episode % self.update_frequency == 0 :\n",
    "                self.updateNetwork(self.policy_network, self.target_network)\n",
    "        \n",
    "        \n",
    "        return self.train_reward_array, self.total_cpu_array, self.total_steps_array, self.total_wall_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class D3QN_PER(D3QN_PER):\n",
    "    def trainNetwork(self, experiences , epochs):\n",
    "        # this method trains the value network epoch number of times and is called by the trainAgent function\n",
    "        # it essentially uses the experiences to calculate target, using the targets it calculates the error, which\n",
    "        # is further used for calulating the loss. It then uses the optimizer over the loss \n",
    "        # to update the params of the network by backpropagating through the network\n",
    "        # this function does not return anything\n",
    "        # you can try out other loss functions other than MSE like Huber loss, MAE, etc. \n",
    "        #\n",
    "        #Your code goes in here\n",
    "\n",
    "        curr_states, actions, rewards, new_states, terminated_s, truncated_s , weights , index= self.replay_buffer.splitExperiences(experiences)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "\n",
    "            new_q_values_policy = self.policy_network(new_states)\n",
    "            new_actions = new_q_values_policy.argmax(dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "            new_q_values_target = self.target_network(new_states).detach()\n",
    "            max_new_q_values_target = new_q_values_target.gather(1, new_actions)\n",
    "\n",
    "            temp = torch.logical_or(terminated_s, truncated_s)\n",
    "            targets = rewards + self.gamma * (torch.logical_not(temp)) * max_new_q_values_target\n",
    "\n",
    "            q_values = self.policy_network(curr_states)\n",
    "            action_q_values = q_values.gather(1, actions)\n",
    "\n",
    "            error = F.smooth_l1_loss(action_q_values, targets)\n",
    "            error = (error*weights).mean()\n",
    "            \n",
    "            # gradient descent\n",
    "            self.optimizer.zero_grad()\n",
    "            error.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            self.replay_buffer.update(index, targets-q_values)\n",
    "\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class D3QN_PER(D3QN_PER):\n",
    "    def updateNetwork(self, onlineNet, targetNet):\n",
    "        #this function updates the onlineNetwork with the target network using Polyak averaging \\\n",
    "        #\n",
    "        # Your code goes in here\n",
    "        #\n",
    "        with torch.no_grad():\n",
    "            for paramOnline, paramTarget in zip(onlineNet.parameters(), targetNet.parameters()):\n",
    "                paramTarget.data = self.tau * paramOnline.data + (1 - self.tau) * paramTarget.data\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class D3QN_PER(D3QN_PER):\n",
    "    def evaluateAgent(self):\n",
    "        #this function evaluates the agent using the value network, it evaluates agent for MAX_EVAL_EPISODES\n",
    "        #typcially MAX_EVAL_EPISODES = 1\n",
    "        #\n",
    "        #Your code goes in here\n",
    "        reward_arr = []\n",
    "        for e in range(self.max_test_ep):\n",
    "            rs = 0\n",
    "            \n",
    "            st, info = self.env.reset()\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            \n",
    "            while not (terminated or truncated):\n",
    "                act = self.exploration_eval(self.policy_network, torch.tensor([st], dtype=torch.float32))\n",
    "                st, rew, terminated, truncated, info = self.env.step(act)\n",
    "                rs += rew\n",
    "                \n",
    "            reward_arr.append(rs)\n",
    "            \n",
    "        self.test_reward = sum(reward_arr) / len(reward_arr)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env.reset()\n",
    "\n",
    "# #D3QN\n",
    "# agent_D3QN_PER = D3QN_PER(env, seed=i,alpha=0.6 , beta=0.1,beta_rate=0.9992 ,tau =0.1 , gamma=0.99, bufferSize=1000, batchSize=64, \n",
    "#         optimizerFn=optim.Adam, optimizerLR=0.0005, MAX_TRAIN_EPISODES=500, MAX_EVAL_EPISODES=1, \n",
    "#         explorationStrategyTrainFn= selectEpsilonGreedyAction, explorationStrategyEvalFn= selectGreedyAction, \n",
    "#         updateFrequency=15)\n",
    "# D3QN_PER_trainRewardsList, D3QN_PER_trainTimeList, D3QN_PER_evalRewardsList, D3QN_PER_wallClockTimeList, D3QN_PER_finalEvalReward = agent_D3QN_PER.runD3QN_PER()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Policy Based RL agents.\n",
    "<a id=\"deep-policy-based\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose of this part is to learn about different Deep Policy Based RL agents.\n",
    "\n",
    "In this part of the assignment you will be implementing Deep Policy based RL algorithms we learnt in Lectures. Namely, we will be implementing REINFORCE and VPG. \n",
    "\n",
    "For all the algorithms below, this time we will not be specifying the hyper-parameters, please play with the hyper-params to come up with the best values. This way you will learn to tune the model. Some of the values were specified in the lecture, that would be a good starting point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the REINFORCE algorithm. We have studied about REINFORCE algorithm in the Lecture. Use the function definitions (given below).\n",
    "\n",
    "This class implements the REINFORCE Agent, you are required to implement the various methods of this class\n",
    "as outlined below. Note this class is generic and should work with any permissible Gym environment\n",
    "\n",
    "```\n",
    "class REINFORCE():\n",
    "    def __init__(env, seed, gamma, \n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn, \n",
    "                 explorationStrategyEvalFn)\n",
    "    def initBookKeeping(self)\n",
    "    def performBookKeeping(self, train = True)\n",
    "    def runREINFORCE(self)\n",
    "    def trainAgent(self)\n",
    "    def trainPolicyNetwork(self, experiences)\n",
    "    def evaluateAgent(self)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the methods for the REINFORCE class below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initBookKeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def performBookKeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def runREINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trainAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trainPolicyNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluateAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Policy Gradient (VPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the VPG algorithm. We have studied about VPG algorithm in the Lecture. Use the function definitions (given below).\n",
    "\n",
    "This class implements the VPG Agent, you are required to implement the various methods of this class\n",
    "as outlined below. Note this class is generic and should work with any permissible Gym environment\n",
    "\n",
    "```\n",
    "class VPG():\n",
    "    def __init__(env, seed, gamma, beta, \n",
    "                 optimizerFn,\n",
    "                 optimizerLR,\n",
    "                 MAX_TRAIN_EPISODES, MAX_EVAL_EPISODES,\n",
    "                 explorationStrategyTrainFn, \n",
    "                 explorationStrategyEvalFn)\n",
    "    def initBookKeeping(self)\n",
    "    def performBookKeeping(self, train = True)\n",
    "    def runVPG(self)\n",
    "    def trainAgent(self)\n",
    "    def trainPolicyNetwork(self, experiences)\n",
    "    def evaluateAgent(self)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initBookKeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def performBookKeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def runVPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trainAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trainPolicyNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluateAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments and Plots\n",
    "<a id=\"experiments\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the NFQ, DQN, Double DQN, Dueling Double DQN, Dueling Double Deep Q Network with Prioritized Experience Replay, REINFORCE and VPG agent on CartPole environment and MountainCar enviroment.\n",
    "\n",
    "Plot the following for each of the environment separately. Note based on different hyper-parameters and stratgies you use, can you have multiple plots for each of the below. \n",
    "\n",
    "As you are aware from your past experience, single run of the agent over the environment results in plots that have lot of variance and look very noisy. One way to overcome this is to create several different instances of the environment using different seeds and then average out the results across these and plot these. For all the plots below, you this strategy. You need to run 5 different instances of the environment for each agent. As you have seen in the lecture slides, we plot the maximum and minimum values around the mean in the plots, so this gives us the shaded plot with the mean curve in the between. In this assignment, you are required to do the same. Generate plots with envelop between maximum and minimum value (check the plotQuantity() function in the helper functions).\n",
    "\n",
    "For each of the quantity of interest, plot each of the agent within the same plot using different colors for the envelop. Choose colors such that that there is clear contrast between the plots corresponding to different agents.\n",
    "\n",
    "1. Plot mean train rewards vs episodes for Cartpole environment.\n",
    "2. Plot mean train rewards vs episodes for MountatinCar environment.\n",
    "3. Plot mean evaluation rewards vs episodes \n",
    "4. Plot mean evaluation rewards vs episodes \n",
    "5. Plot total steps vs episode for Cartpole environment.\n",
    "6. Plot total steps vs episode for MountatinCar environment.\n",
    "7. Plot train time vs episode for Cartpole environment.\n",
    "8. Plot train time vs episode for MountatinCar environment.\n",
    "9. Plot wall clock time vs episode for Cartpole environment.\n",
    "10. Plot wall clock time vs episode for MountatinCar environment.\n",
    "11. Based on plots for CartPole environment, what are your observations about different agents. Compare different agents.  \n",
    "12. Based on plots for MountainCar environment, what are your observations about different agents. Compare different agents. Do these observations concur with the ones for CartPole environment? \n",
    "13. Based on both the environments, can you generalize some of the findings for the value-based agents? If yes what are those findings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Cartpole env #####################################################\n",
    "def runDeepValueBasedAgents():\n",
    "    # this function will initialize 5 different instances of the env (using different seeds), run all the agents\n",
    "    # over these different instances. Collects results and generate the plots state above.\n",
    "    # generate your plots in the cells below\n",
    "    # write the answers to part 11, 12 and 13 in the cells below the plot-cells.\n",
    "    \n",
    "    DQN_trainRewardsList=dict()\n",
    "    DQN_trainTimeList = dict()\n",
    "    DQN_evalRewardsList = dict()\n",
    "    DQN_wallClockTimeList=dict()\n",
    "    DQN_finalEvalReward=dict()\n",
    "    \n",
    "    NFQ_trainRewardsList=dict()\n",
    "    NFQ_trainTimeList = dict()\n",
    "    NFQ_evalRewardsList = dict()\n",
    "    NFQ_wallClockTimeList=dict()\n",
    "    NFQ_finalEvalReward=dict()\n",
    "    \n",
    "    DDQN_trainRewardsList=dict()\n",
    "    DDQN_trainTimeList = dict()\n",
    "    DDQN_evalRewardsList = dict()\n",
    "    DDQN_wallClockTimeList=dict()\n",
    "    DDQN_finalEvalReward=dict()\n",
    "    \n",
    "    D3QN_trainRewardsList=dict()\n",
    "    D3QN_trainTimeList = dict()\n",
    "    D3QN_evalRewardsList = dict()\n",
    "    D3QN_wallClockTimeList=dict()\n",
    "    D3QN_finalEvalReward=dict()\n",
    "    \n",
    "    D3QN_PER_trainRewardsList=dict()\n",
    "    D3QN_PER_trainTimeList = dict()\n",
    "    D3QN_PER_evalRewardsList = dict()\n",
    "    D3QN_PER_wallClockTimeList=dict()\n",
    "    D3QN_PER_finalEvalReward=dict()\n",
    "    \n",
    "    for i in range(1,6):\n",
    "        env = gym.make('CartPole-v1')\n",
    "        env.reset(seed=i)\n",
    "        \n",
    "        #DQN\n",
    "        agent_DQN = DQN(env, seed=i, gamma=0.99, bufferSize=1000, batchSize=64, \n",
    "                optimizerFn=optim.Adam, optimizerLR=0.0005, MAX_TRAIN_EPISODES=1000, MAX_EVAL_EPISODES=1, \n",
    "                explorationStrategyTrainFn= selectEpsilonGreedyAction, explorationStrategyEvalFn= selectGreedyAction, \n",
    "                updateFrequency=15)\n",
    "        DQN_trainRewardsList[i], DQN_trainTimeList[i], DQN_evalRewardsList[i], DQN_wallClockTimeList[i], DQN_finalEvalReward[i] = agent_DQN.runDQN()\n",
    "        #print(agent_DQN.runDQN())\n",
    "        \n",
    "        #NFQ\n",
    "        agent_NFQ = NFQ(env, seed=i, gamma=0.99, bufferSize=1000, batchSize=64, \n",
    "                optimizerFn=optim.Adam, optimizerLR=0.0005, MAX_TRAIN_EPISODES=1000, MAX_EVAL_EPISODES=1, \n",
    "                explorationStrategyTrainFn= selectEpsilonGreedyAction, explorationStrategyEvalFn= selectGreedyAction)\n",
    "        NFQ_trainRewardsList[i], NFQ_trainTimeList[i], NFQ_evalRewardsList[i], NFQ_wallClockTimeList[i], NFQ_finalEvalReward[i] = agent_NFQ.runNFQ()\n",
    "        \n",
    "        \n",
    "        #DDQN\n",
    "        agent_DDQN = DDQN(env, seed=i, gamma=0.99, bufferSize=1000, batchSize=64, \n",
    "                optimizerFn=optim.Adam, optimizerLR=0.0005, MAX_TRAIN_EPISODES=1000, MAX_EVAL_EPISODES=1, \n",
    "                explorationStrategyTrainFn= selectEpsilonGreedyAction, explorationStrategyEvalFn= selectGreedyAction, \n",
    "                updateFrequency=15)\n",
    "        DDQN_trainRewardsList[i], DDQN_trainTimeList[i], DDQN_evalRewardsList[i], DDQN_wallClockTimeList[i], DDQN_finalEvalReward[i] = agent_DDQN.runDDQN()\n",
    "        \n",
    "        \n",
    "        #D3QN\n",
    "        agent_D3QN = D3QN(env, seed=i, tau =0.1 , gamma=0.99, bufferSize=1000, batchSize=64, \n",
    "                optimizerFn=optim.Adam, optimizerLR=0.0005, MAX_TRAIN_EPISODES=1000, MAX_EVAL_EPISODES=1, \n",
    "                explorationStrategyTrainFn= selectEpsilonGreedyAction, explorationStrategyEvalFn= selectGreedyAction, \n",
    "                updateFrequency=15)\n",
    "        D3QN_trainRewardsList[i], D3QN_trainTimeList[i], D3QN_evalRewardsList[i], D3QN_wallClockTimeList[i], D3QN_finalEvalReward[i] = agent_D3QN.runD3QN()\n",
    "        \n",
    "        #PER-D3QN\n",
    "        agent_D3QN_PER = D3QN_PER(env, seed=i,alpha=0.6 , beta=0.1,beta_rate=0.9992 ,tau =0.1 , gamma=0.99, bufferSize=1000, batchSize=64, \n",
    "        optimizerFn=optim.Adam, optimizerLR=0.0005, MAX_TRAIN_EPISODES=1000, MAX_EVAL_EPISODES=1, \n",
    "        explorationStrategyTrainFn= selectEpsilonGreedyAction, explorationStrategyEvalFn= selectGreedyAction, \n",
    "        updateFrequency=15)\n",
    "        D3QN_PER_trainRewardsList[i], D3QN_PER_trainTimeList[i], D3QN_PER_evalRewardsList[i], D3QN_PER_wallClockTimeList[i], D3QN_PER_finalEvalReward[i] = agent_D3QN_PER.runD3QN_PER()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    #print(finalEvalReward)\n",
    "    plotQuantity(DQN_trainRewardsList, 1000, ['Training Reward' , 'Training reward vs episodes' , 'DQN'])\n",
    "    plotQuantity(DDQN_trainRewardsList, 1000, ['Training Reward' , 'Training reward vs episodes' , 'DDQN'])\n",
    "    plotQuantity(D3QN_trainRewardsList, 1000, ['Training Reward' , 'Training reward vs episodes' , 'D3QN'])\n",
    "    plotQuantity(D3QN_PER_trainRewardsList, 1000, ['Training Reward' , 'Training reward vs episodes' , 'D3QN_PER'])\n",
    "    plotQuantity(NFQ_trainRewardsList, 1000, ['Training Reward' , 'Training reward vs episodes' , 'NFQ'])\n",
    "    plt.show()\n",
    "    plotQuantity(DQN_trainTimeList, 1000, ['Training time' , 'Training time vs episodes' , 'DQN'])\n",
    "    plotQuantity(DDQN_trainTimeList, 1000, ['Training time' , 'Training time vs episodes' , 'DDQN'])\n",
    "    plotQuantity(D3QN_trainTimeList, 1000, ['Training time' , 'Training time vs episodes' , 'D3QN'])\n",
    "    plotQuantity(D3QN_PER_trainTimeList, 1000, ['Training time' , 'Training time vs episodes' , 'D3QN_PER'])\n",
    "    plotQuantity(NFQ_trainTimeList, 1000, ['Training time' , 'Training time vs episodes' , 'NFQ'])\n",
    "    plt.show()\n",
    "    plotQuantity(DQN_evalRewardsList, 1000, ['Total steps' , 'Total steps vs episodes', 'DQN'])\n",
    "    plotQuantity(DDQN_evalRewardsList, 1000, ['Total steps' , 'Total steps vs episodes', 'DDQN'])\n",
    "    plotQuantity(D3QN_evalRewardsList, 1000, ['Total steps' , 'Total steps vs episodes', 'D3QN'])\n",
    "    plotQuantity(D3QN_PER_evalRewardsList, 1000, ['Total steps' , 'Total steps vs episodes', 'D3QN_PER'])\n",
    "    plotQuantity(NFQ_evalRewardsList, 1000, ['Total steps' , 'Total steps vs episodes', 'NFQ'])\n",
    "    plt.show()\n",
    "    plotQuantity(DQN_wallClockTimeList, 1000, ['Wall clock' , 'Wall clock vs episodes', 'DQN'])\n",
    "    plotQuantity(DDQN_wallClockTimeList, 1000, ['Wall clock' , 'Wall clock vs episodes', 'DDQN'])\n",
    "    plotQuantity(D3QN_wallClockTimeList, 1000, ['Wall clock' , 'Wall clock vs episodes', 'D3QN'])\n",
    "    plotQuantity(D3QN_PER_wallClockTimeList, 1000, ['Wall clock' , 'Wall clock vs episodes', 'D3QN_PER'])\n",
    "    plotQuantity(NFQ_wallClockTimeList, 1000, ['Wall clock' , 'Wall clock vs episodes', 'NFQ'])\n",
    "    plt.show()\n",
    "    plotQuantity(DQN_finalEvalReward,1000, ['Eval Reward' , 'Eval Reward vs episodes', 'DQN'])\n",
    "    plotQuantity(DDQN_finalEvalReward, 1000, ['Eval Reward' , 'Eval Reward vs episodes', 'DDQN'])\n",
    "    plotQuantity(D3QN_finalEvalReward, 1000, ['Eval Reward' , 'Eval Reward vs episodes', 'D3QN'])\n",
    "    plotQuantity(D3QN_PER_finalEvalReward, 1000, ['Eval Reward' , 'Eval Reward vs episodes', 'D3QN_PER'])\n",
    "    plotQuantity(NFQ_finalEvalReward, 1000, ['Eval Reward' , 'Eval Reward vs episodes', 'NFQ'])\n",
    "    plt.show()\n",
    "\n",
    "runDeepValueBasedAgents()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11 - \n",
    "\n",
    "Based on the plots above we can see that the D3QN with prioritized sampling performs the best in long run . In the plot training reward vs episodes we can see how better it is in the start but then eventually other better algorithm catches up. Also we can observe how slow NFQ is compared to other algorithm making it inefficient for normal use. While Double DQN clears the maximization bias did by DQN algo , it is more stable and converges quite quickly. Also we can see the speed of D3QN and D3QN PER and how good the algorithm performs , They have superior network structure which leads to better learning and performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Mountain Car env #####################################################\n",
    "def runDeepValueBasedAgents_MC():\n",
    "    # this function will initialize 5 different instances of the env (using different seeds), run all the agents\n",
    "    # over these different instances. Collects results and generate the plots state above.\n",
    "    # generate your plots in the cells below\n",
    "    # write the answers to part 11, 12 and 13 in the cells below the plot-cells.\n",
    "    \n",
    "    DQN_trainRewardsList=dict()\n",
    "    DQN_trainTimeList = dict()\n",
    "    DQN_evalRewardsList = dict()\n",
    "    DQN_wallClockTimeList=dict()\n",
    "    DQN_finalEvalReward=dict()\n",
    "    \n",
    "    NFQ_trainRewardsList=dict()\n",
    "    NFQ_trainTimeList = dict()\n",
    "    NFQ_evalRewardsList = dict()\n",
    "    NFQ_wallClockTimeList=dict()\n",
    "    NFQ_finalEvalReward=dict()\n",
    "    \n",
    "    DDQN_trainRewardsList=dict()\n",
    "    DDQN_trainTimeList = dict()\n",
    "    DDQN_evalRewardsList = dict()\n",
    "    DDQN_wallClockTimeList=dict()\n",
    "    DDQN_finalEvalReward=dict()\n",
    "    \n",
    "    D3QN_trainRewardsList=dict()\n",
    "    D3QN_trainTimeList = dict()\n",
    "    D3QN_evalRewardsList = dict()\n",
    "    D3QN_wallClockTimeList=dict()\n",
    "    D3QN_finalEvalReward=dict()\n",
    "    \n",
    "    D3QN_PER_trainRewardsList=dict()\n",
    "    D3QN_PER_trainTimeList = dict()\n",
    "    D3QN_PER_evalRewardsList = dict()\n",
    "    D3QN_PER_wallClockTimeList=dict()\n",
    "    D3QN_PER_finalEvalReward=dict()\n",
    "    \n",
    "    for i in range(1,6):\n",
    "        print(i)\n",
    "        env = gym.make('MountainCar-v0')\n",
    "        env.reset(seed=i)\n",
    "        \n",
    "        #DQN\n",
    "        agent_DQN = DQN(env, seed=i, gamma=0.99, bufferSize=1000, batchSize=64, \n",
    "                optimizerFn=optim.Adam, optimizerLR=0.0005, MAX_TRAIN_EPISODES=1000, MAX_EVAL_EPISODES=1, \n",
    "                explorationStrategyTrainFn= selectEpsilonGreedyAction, explorationStrategyEvalFn= selectGreedyAction, \n",
    "                updateFrequency=15)\n",
    "        DQN_trainRewardsList[i], DQN_trainTimeList[i], DQN_evalRewardsList[i], DQN_wallClockTimeList[i], DQN_finalEvalReward[i] = agent_DQN.runDQN()\n",
    "        #print(agent_DQN.runDQN())\n",
    "        \n",
    "        #NFQ\n",
    "        agent_NFQ = NFQ(env, seed=i, gamma=0.99, bufferSize=1000, batchSize=64, \n",
    "                optimizerFn=optim.Adam, optimizerLR=0.0005, MAX_TRAIN_EPISODES=1000, MAX_EVAL_EPISODES=1, \n",
    "                explorationStrategyTrainFn= selectEpsilonGreedyAction, explorationStrategyEvalFn= selectGreedyAction)\n",
    "        NFQ_trainRewardsList[i], NFQ_trainTimeList[i], NFQ_evalRewardsList[i], NFQ_wallClockTimeList[i], NFQ_finalEvalReward[i] = agent_NFQ.runNFQ()\n",
    "        \n",
    "        \n",
    "        #DDQN\n",
    "        agent_DDQN = DDQN(env, seed=i, gamma=0.99, bufferSize=1000, batchSize=64, \n",
    "                optimizerFn=optim.Adam, optimizerLR=0.0005, MAX_TRAIN_EPISODES=1000, MAX_EVAL_EPISODES=1, \n",
    "                explorationStrategyTrainFn= selectEpsilonGreedyAction, explorationStrategyEvalFn= selectGreedyAction, \n",
    "                updateFrequency=15)\n",
    "        DDQN_trainRewardsList[i], DDQN_trainTimeList[i], DDQN_evalRewardsList[i], DDQN_wallClockTimeList[i], DDQN_finalEvalReward[i] = agent_DDQN.runDDQN()\n",
    "        \n",
    "        \n",
    "        #D3QN\n",
    "        agent_D3QN = D3QN(env, seed=i, tau =0.1 , gamma=0.99, bufferSize=1000, batchSize=64, \n",
    "                optimizerFn=optim.Adam, optimizerLR=0.0005, MAX_TRAIN_EPISODES=1000, MAX_EVAL_EPISODES=1, \n",
    "                explorationStrategyTrainFn= selectEpsilonGreedyAction, explorationStrategyEvalFn= selectGreedyAction, \n",
    "                updateFrequency=15)\n",
    "        D3QN_trainRewardsList[i], D3QN_trainTimeList[i], D3QN_evalRewardsList[i], D3QN_wallClockTimeList[i], D3QN_finalEvalReward[i] = agent_D3QN.runD3QN()\n",
    "        \n",
    "        #PER-D3QN\n",
    "        agent_D3QN_PER = D3QN_PER(env, seed=i,alpha=0.6 , beta=0.1,beta_rate=0.9992 ,tau =0.1 , gamma=0.99, bufferSize=1000, batchSize=64, \n",
    "        optimizerFn=optim.Adam, optimizerLR=0.0005, MAX_TRAIN_EPISODES=1000, MAX_EVAL_EPISODES=1, \n",
    "        explorationStrategyTrainFn= selectEpsilonGreedyAction, explorationStrategyEvalFn= selectGreedyAction, \n",
    "        updateFrequency=15)\n",
    "        D3QN_PER_trainRewardsList[i], D3QN_PER_trainTimeList[i], D3QN_PER_evalRewardsList[i], D3QN_PER_wallClockTimeList[i], D3QN_PER_finalEvalReward[i] = agent_D3QN_PER.runD3QN_PER()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    #print(finalEvalReward)\n",
    "    plotQuantity(DQN_trainRewardsList, 1000, ['Training Reward' , 'Training reward vs episodes' , 'DQN'])\n",
    "    plotQuantity(DDQN_trainRewardsList, 1000, ['Training Reward' , 'Training reward vs episodes' , 'DDQN'])\n",
    "    plotQuantity(D3QN_trainRewardsList, 1000, ['Training Reward' , 'Training reward vs episodes' , 'D3QN'])\n",
    "    plotQuantity(D3QN_PER_trainRewardsList, 1000, ['Training Reward' , 'Training reward vs episodes' , 'D3QN_PER'])\n",
    "    plotQuantity(NFQ_trainRewardsList, 1000, ['Training Reward' , 'Training reward vs episodes' , 'NFQ'])\n",
    "    plt.show()\n",
    "    plotQuantity(DQN_trainTimeList, 1000, ['Training time' , 'Training time vs episodes' , 'DQN'])\n",
    "    plotQuantity(DDQN_trainTimeList, 1000, ['Training time' , 'Training time vs episodes' , 'DDQN'])\n",
    "    plotQuantity(D3QN_trainTimeList, 1000, ['Training time' , 'Training time vs episodes' , 'D3QN'])\n",
    "    plotQuantity(D3QN_PER_trainTimeList, 1000, ['Training time' , 'Training time vs episodes' , 'D3QN_PER'])\n",
    "    plotQuantity(NFQ_trainTimeList, 1000, ['Training time' , 'Training time vs episodes' , 'NFQ'])\n",
    "    plt.show()\n",
    "    plotQuantity(DQN_evalRewardsList, 1000, ['Total steps' , 'Total steps vs episodes', 'DQN'])\n",
    "    plotQuantity(DDQN_evalRewardsList, 1000, ['Total steps' , 'Total steps vs episodes', 'DDQN'])\n",
    "    plotQuantity(D3QN_evalRewardsList, 1000, ['Total steps' , 'Total steps vs episodes', 'D3QN'])\n",
    "    plotQuantity(D3QN_PER_evalRewardsList, 1000, ['Total steps' , 'Total steps vs episodes', 'D3QN_PER'])\n",
    "    plotQuantity(NFQ_evalRewardsList, 1000, ['Total steps' , 'Total steps vs episodes', 'NFQ'])\n",
    "    plt.show()\n",
    "    plotQuantity(DQN_wallClockTimeList, 1000, ['Wall clock' , 'Wall clock vs episodes', 'DQN'])\n",
    "    plotQuantity(DDQN_wallClockTimeList, 1000, ['Wall clock' , 'Wall clock vs episodes', 'DDQN'])\n",
    "    plotQuantity(D3QN_wallClockTimeList, 1000, ['Wall clock' , 'Wall clock vs episodes', 'D3QN'])\n",
    "    plotQuantity(D3QN_PER_wallClockTimeList, 1000, ['Wall clock' , 'Wall clock vs episodes', 'D3QN_PER'])\n",
    "    plotQuantity(NFQ_wallClockTimeList, 1000, ['Wall clock' , 'Wall clock vs episodes', 'NFQ'])\n",
    "    plt.show()\n",
    "    plotQuantity(DQN_finalEvalReward,1000, ['Eval Reward' , 'Eval Reward vs episodes', 'DQN'])\n",
    "    plotQuantity(DDQN_finalEvalReward, 1000, ['Eval Reward' , 'Eval Reward vs episodes', 'DDQN'])\n",
    "    plotQuantity(D3QN_finalEvalReward, 1000, ['Eval Reward' , 'Eval Reward vs episodes', 'D3QN'])\n",
    "    plotQuantity(D3QN_PER_finalEvalReward, 1000, ['Eval Reward' , 'Eval Reward vs episodes', 'D3QN_PER'])\n",
    "    plotQuantity(NFQ_finalEvalReward, 1000, ['Eval Reward' , 'Eval Reward vs episodes', 'NFQ'])\n",
    "    plt.show()\n",
    "\n",
    "runDeepValueBasedAgents_MC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12 \n",
    "\n",
    "Mountain car is a special kind of environment in which the rewards are very sparse and you only get reward at the end when the car climbs up the hill. This is the reason why every algorithm fails to train the agent in a mere 1000 episodes. There are two options : \n",
    "1) Either you increase the number of episodes or \n",
    "2) You modify the reward based on position and velocity\n",
    "Both of the above will help in efficient learning of the environment. However the second option is preferred due to computational complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13 \n",
    "\n",
    "From both of the environments we can say that the value based method do a good job in predictiong the policy when the reward is dense but when the reward is sparse as seen in the mountain car environment the value based method fails and policy based method is better choice for them as it directly evaluates the policy without calculating the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runDeepPolicyBasedAgents():\n",
    "    # this function will initialize 5 different instances of the env (using different seeds), run all the agents\n",
    "    # over these different instances. Collects results and generate the plots state above.\n",
    "    # generate your plots in the cells below\n",
    "    # write the answers to part 11, 12 and 13 in the cells below the plot-cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
