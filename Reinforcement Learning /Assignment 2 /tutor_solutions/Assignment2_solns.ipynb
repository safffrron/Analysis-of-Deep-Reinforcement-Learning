{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from gymnasium import Env, spaces, register, make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'Random_Maze'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RandomMaze(Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Left = 0\n",
    "        Up = 1\n",
    "        Right = 2\n",
    "        Down = 3\n",
    "        \"\"\"\n",
    "        self.P = {\n",
    "            0: {\n",
    "                0: [(0.8, 0, -0.04, False),(0.1, 0, -0.04, False),(0.1, 4, -0.04, False)],\n",
    "                1: [(0.8, 0, -0.04, False),(0.1, 0, -0.04, False),(0.1, 1, -0.04, False)],\n",
    "                2: [(0.8, 1, -0.04, False),(0.1, 0, -0.04, False),(0.1, 4, -0.04, False)],\n",
    "                3: [(0.8, 4, -0.04, False),(0.1, 0, -0.04, False),(0.1, 1, -0.04, False)]\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.8, 0, -0.04, False),(0.1, 1, -0.04, False),(0.1, 1, -0.04, False)],\n",
    "                1: [(0.8, 1, -0.04, False),(0.1, 0, -0.04, False),(0.1, 2, -0.04, False)],\n",
    "                2: [(0.8, 2, -0.04, False),(0.1, 1, -0.04, False),(0.1, 1, -0.04, False)],\n",
    "                3: [(0.8, 1, -0.04, False),(0.1, 0, -0.04, False),(0.1, 2, -0.04, False)]\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.8, 1, -0.04, False),(0.1, 2, -0.04, False),(0.1, 6, -0.04, False)],                \n",
    "                1: [(0.8, 2, -0.04, False),(0.1, 1, -0.04, False),(0.1, 3,  1.00, True)],\n",
    "                2: [(0.8, 3,  1.00, True ),(0.1, 2, -0.04, False),(0.1, 6, -0.04, False)],\n",
    "                3: [(0.8, 6, -0.04, False),(0.1, 1, -0.04, False),(0.1, 3,  1.00, True)]\n",
    "            },\n",
    "            3: {\n",
    "                0: [(1.0, 3, 0.00, True)],                \n",
    "                1: [(1.0, 3, 0.00, True)],\n",
    "                2: [(1.0, 3, 0.00, True)],\n",
    "                3: [(1.0, 3, 0.00, True)]\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.8, 4, -0.04, False),(0.1, 0, -0.04, False),(0.1, 8, -0.04, False)],\n",
    "                1: [(0.8, 0, -0.04, False),(0.1, 4, -0.04, False),(0.1, 4, -0.04, False)],\n",
    "                2: [(0.8, 4, -0.04, False),(0.1, 0, -0.04, False),(0.1, 8, -0.04, False)],\n",
    "                3: [(0.8, 8, -0.04, False),(0.1, 4, -0.04, False),(0.1, 4, -0.04, False)]\n",
    "            },\n",
    "            5: {\n",
    "                0: [(1.0, 5, 0.00, True)],                \n",
    "                1: [(1.0, 5, 0.00, True)],\n",
    "                2: [(1.0, 5, 0.00, True)],\n",
    "                3: [(1.0, 5, 0.00, True)]\n",
    "            },\n",
    "            6: {\n",
    "                0: [(0.8, 6, -0.04, False),(0.1, 2, -0.04, False),(0.1, 10, -0.04, False)],                \n",
    "                1: [(0.8, 2, -0.04, False),(0.1, 6, -0.04, False),(0.1, 7, -1.00,  True)],\n",
    "                2: [(0.8, 7, -1.00, True ),(0.1, 2, -0.04, False),(0.1, 10, -0.04, False)],\n",
    "                3: [(0.8, 10, -0.04,False),(0.1, 6, -0.04, False),(0.1, 7, -1.00,  True)]\n",
    "            },\n",
    "            7: {\n",
    "                0: [(1.0, 7, 0.00, True)],                \n",
    "                1: [(1.0, 7, 0.00, True)],\n",
    "                2: [(1.0, 7, 0.00, True)],\n",
    "                3: [(1.0, 7, 0.00, True)]\n",
    "            },\n",
    "            8: {\n",
    "                0: [(0.8, 8, -0.04, False),(0.1, 4, -0.04, False),(0.1, 8, -0.04, False)],                \n",
    "                1: [(0.8, 4, -0.04, False),(0.1, 8, -0.04, False),(0.1, 9, -0.04, False)],\n",
    "                2: [(0.8, 9, -0.04, False),(0.1, 4, -0.04, False),(0.1, 8, -0.04, False)],\n",
    "                3: [(0.8, 8, -0.04, False),(0.1, 8, -0.04, False),(0.1, 9, -0.04, False)]\n",
    "            },\n",
    "            9: {\n",
    "                0: [(0.8, 8, -0.04, False),(0.1, 9, -0.04, False),(0.1, 9, -0.04, False)],\n",
    "                1: [(0.8, 9, -0.04, False),(0.1, 8, -0.04, False),(0.1, 10, -0.04, False)],\n",
    "                2: [(0.8, 10, -0.04, False),(0.1, 9, -0.04, False),(0.1, 9, -0.04, False)],\n",
    "                3: [(0.8, 9, -0.04, False),(0.1, 8, -0.04, False),(0.1, 10, -0.04, False)]\n",
    "            },\n",
    "            10: {\n",
    "                0: [(0.8, 9, -0.04, False),(0.1, 6, -0.04, False),(0.1, 10, -0.04, False)],\n",
    "                1: [(0.8, 6, -0.04, False),(0.1, 9, -0.04, False),(0.1, 11, -0.04, False)],\n",
    "                2: [(0.8, 11, -0.04, False),(0.1, 6, -0.04, False),(0.1, 10, -0.04, False)],\n",
    "                3: [(0.8, 10, -0.04, False),(0.1, 9, -0.04, False),(0.1, 11, -0.04, False)]\n",
    "            },\n",
    "            11: {\n",
    "                0: [(0.8, 10, -0.04,False),(0.1, 7, -1, True),(0.1, 11, -1.00,  False)],\n",
    "                1: [(0.8, 7, -1.00, True ),(0.1, 10, -0.04, False),(0.1, 11, -0.04, False)],\n",
    "                2: [(0.8, 11, -0.04, False),(0.1, 7, -1, True),(0.1, 11, -1.00,  False)],\n",
    "                3: [(0.8, 11, -0.04, False),(0.1, 10, -0.04, False),(0.1, 11, -0.04, False)]\n",
    "\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space=spaces.Discrete(12)\n",
    "        self.starting_state = 8\n",
    "        self.dead_state = 7\n",
    "        self.goal_state = 3\n",
    "        self.previous_state=-1\n",
    "        self.curr_action = 0\n",
    "        self.seed = 121\n",
    "\n",
    "    def get_obs(self):\n",
    "        return dict(agent=self.agent_location, target=self.target_location)\n",
    "    \n",
    "    def get_info(self):\n",
    "        return dict(current_state=self.previous_state,action=self.curr_action, next_state=self.agent_location)\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \n",
    "        if seed is None:\n",
    "            seed = self.seed\n",
    "        else:\n",
    "            self.seed = seed\n",
    "        \n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        self.previous_state = self.starting_state\n",
    "        self.agent_location = self.starting_state\n",
    "        self.target_location = self.goal_state\n",
    "        self.dead_state = self.dead_state\n",
    "        \n",
    "        observation = self.get_obs()\n",
    "        info = self.get_info()\n",
    "\n",
    "        return observation,info\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        self.curr_action = action\n",
    "        self.previous_state = self.agent_location\n",
    "        transitions = self.P[self.previous_state][action]\n",
    "        probabilities, next_states, rewards, terminals = zip(*transitions)\n",
    "        index = random.choices(range(len(probabilities)), weights=probabilities, k=1)[0]\n",
    "        self.agent_location, reward, terminated = next_states[index], rewards[index], terminals[index]\n",
    "        \n",
    "        observation = self.get_obs()  \n",
    "        info = self.get_info()\n",
    "\n",
    "        return observation, reward, terminated, False, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "register(id=ENV_NAME, entry_point=RandomMaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_steps = 100\n",
    "num_episodes = 5000\n",
    "\n",
    "num_actions = 4\n",
    "num_states = 12\n",
    "starting_state = 8\n",
    "goal_state = 3\n",
    "hole_state = 7\n",
    "\n",
    "num_envs = 10\n",
    "psr_gap = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "psr = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to plot the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot(state_value, q_value, policy_success_rate):\n",
    "    # Plot for state-value function\n",
    "    fig_state_value = go.Figure()\n",
    "    for s in range(num_states):\n",
    "        fig_state_value.add_trace(go.Scatter(x=np.arange(len(state_value)), y=state_value[:, s], mode='lines', name=f'State {s}'))\n",
    "    fig_state_value.update_layout(title='Plot showing evolution of state-value function with time',\n",
    "                                  xaxis_title='Episodes',\n",
    "                                  yaxis_title='State-value')\n",
    "    fig_state_value.show()\n",
    "\n",
    "    # Plot for Q function\n",
    "    fig_q_value = go.Figure()\n",
    "    linestyles = ['solid', 'dash', 'dot', 'dashdot']\n",
    "    colors = px.colors.qualitative.Plotly\n",
    "    for s in range(num_states):\n",
    "        for a in range(num_actions):\n",
    "            fig_q_value.add_trace(go.Scatter(x=np.arange(len(q_value)), y=q_value[:, s, a],\n",
    "                                             mode='lines',\n",
    "                                             line=dict(color=colors[s % len(colors)], dash=linestyles[a % len(linestyles)]),\n",
    "                                             name=f'State {s}, Action {a}'))\n",
    "    fig_q_value.update_layout(title='Plot showing evolution of Q function with time',\n",
    "                              xaxis_title='Episodes',\n",
    "                              yaxis_title='State-action value')\n",
    "    fig_q_value.show()\n",
    "\n",
    "    # Plot for Policy Success Rate\n",
    "    fig_policy_success_rate = go.Figure()\n",
    "    fig_policy_success_rate.add_trace(go.Scatter(x=np.arange(len(policy_success_rate)), y=policy_success_rate,\n",
    "                                                 mode='lines', name='Policy Success Rate'))\n",
    "    fig_policy_success_rate.update_layout(title='Plot showing evolution of Policy Success Rate with time',\n",
    "                                         xaxis_title=f'Episodes/{psr_gap}',\n",
    "                                         yaxis_title='Policy Success Rate')\n",
    "    fig_policy_success_rate.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process(func, *args, **kwargs):\n",
    "\n",
    "    V_s = np.zeros((num_episodes, num_states))\n",
    "    Q_s = np.zeros((num_episodes, num_states, num_actions))\n",
    "    PSR = np.zeros(num_episodes//psr_gap + 1)\n",
    "\n",
    "    for seed in range(num_envs):\n",
    "        env.reset(seed=seed)\n",
    "        state_value, q_value, policy_success_rate, optimal_policy = func(*args)\n",
    "        V_s += state_value\n",
    "        Q_s += q_value\n",
    "        PSR += policy_success_rate\n",
    "        print(f\"Policy for Seed = {seed} is = {optimal_policy}\")\n",
    "\n",
    "    V_s /= num_envs\n",
    "    Q_s /= num_envs\n",
    "    PSR /= num_envs\n",
    "    \n",
    "    psr_key = func.__name__\n",
    "    \n",
    "    if kwargs.get(\"type_of\") is not None:\n",
    "        psr_key += f\"_{kwargs['type_of']}\"\n",
    "        \n",
    "    psr[psr_key] = PSR\n",
    "    \n",
    "    plot(V_s, Q_s, PSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_policy_success_rate(env, current_policy, goal_state, max_episodes = 100, max_steps = 200):\n",
    "    num_successes = 0\n",
    "    \n",
    "    for _ in range(max_episodes):\n",
    "        t = generate_trajectory(env, current_policy, 0, max_steps)\n",
    "        if t:\n",
    "            if t[-1][2] == goal_state:\n",
    "                num_successes += 1\n",
    "    \n",
    "    policy_success_rate = num_successes*100/max_episodes\n",
    "    \n",
    "    return policy_success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_policy_success_rate(algo_list):\n",
    "    data = np.zeros((num_episodes//psr_gap + 1, len(algo_list)))\n",
    "\n",
    "    for i, algo in enumerate(algo_list):\n",
    "        data[:, i] = psr[algo]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for i in range(len(algo_list)):\n",
    "        fig.add_trace(go.Scatter(x=np.arange(0, num_episodes + 1, psr_gap), y=data[:, i],\n",
    "                                 mode='lines',\n",
    "                                 name=algo_list[i]))\n",
    "\n",
    "    fig.update_layout(title='Plot showing evolution of PSR with time',\n",
    "                      xaxis_title='Episodes',\n",
    "                      yaxis_title='Policy Success Rate')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decay(initial_value, final_value, num_steps, decay_type):\n",
    "    if decay_type != 'linear' and decay_type != 'exponential':\n",
    "        raise Exception('Invalid decay_type')\n",
    "    \n",
    "    if decay_type == 'linear':\n",
    "        slope = (initial_value - final_value)/(num_steps - 1)\n",
    "        return [initial_value - i*slope for i in range(num_steps)]\n",
    "    elif decay_type == 'exponential':\n",
    "        rate = np.power((initial_value/final_value), (1/(num_steps-1)))\n",
    "        return [(initial_value/np.power(rate, i)) for i in range(num_steps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_trajectory(env, Q, epsilon, max_steps):\n",
    "    env.reset()\n",
    "    n_iterations = 0\n",
    "    s = starting_state\n",
    "    experience = []\n",
    "    \n",
    "    while n_iterations < max_steps:\n",
    "        a = np.argmax(Q[s])\n",
    "        if np.random.random() < epsilon:\n",
    "            a = np.random.randint(0, num_actions)\n",
    "        \n",
    "        observation, reward, terminated, _, info = env.step(a)\n",
    "\n",
    "        experience.append([info[\"current_state\"], a, \n",
    "                           info[\"next_state\"], reward])\n",
    "        s = info[\"next_state\"]\n",
    "\n",
    "        n_iterations += 1\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "    \n",
    "    return experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def monte_carlo_control(env, gamma, alpha_initial, epsilon_initial, max_steps, num_episodes, first_visit = True):\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    Q_s = np.zeros((num_episodes, num_states, num_actions))\n",
    "    policy_success_rate = np.zeros(num_episodes//psr_gap + 1)\n",
    "    \n",
    "    alpha_array = decay(alpha_initial, 0.01, num_episodes, 'exponential')\n",
    "    epsilon_array = decay(epsilon_initial, 0.01, num_episodes, 'exponential')\n",
    "    \n",
    "    for e in range(num_episodes):\n",
    "        alpha = alpha_array[e]\n",
    "        epsilon = epsilon_array[e]\n",
    "        \n",
    "        t = generate_trajectory(env, Q, epsilon, max_steps)\n",
    "        \n",
    "        visited = np.zeros((num_states, num_actions))\n",
    "        \n",
    "        for i, (s, a, next_state, r) in enumerate(t):\n",
    "            if (visited[s, a] == 1) and first_visit:\n",
    "                continue\n",
    "            \n",
    "            visited[s, a] = 1\n",
    "            \n",
    "            G = 0\n",
    "            for j in range(i, len(t)):\n",
    "                G += np.power(gamma, j-i)*t[j][3]\n",
    "            \n",
    "            Q[s, a] += alpha*(G - Q[s, a])\n",
    "        \n",
    "        if e%psr_gap == 0:\n",
    "            policy_success_rate[int(e/psr_gap)] = get_policy_success_rate(env, Q, goal_state)\n",
    "\n",
    "        Q_s[e] = Q\n",
    "    \n",
    "    if num_episodes%psr_gap == 0:\n",
    "        policy_success_rate[-1] = get_policy_success_rate(env, Q, goal_state)\n",
    "    \n",
    "    V = np.max(Q, axis=1)\n",
    "    V_s = np.max(Q_s, axis=2)\n",
    "    optimal_policy = Q.argmax(axis=1).tolist()\n",
    "\n",
    "    return V_s, Q_s, policy_success_rate, optimal_policy\n",
    "\n",
    "process(monte_carlo_control, env, 0.99, 0.5, 0.99, max_steps, num_episodes, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA (TD Control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_action(s, Q, epsilon):\n",
    "    a = np.argmax(Q[s])\n",
    "    if np.random.random() < epsilon:\n",
    "        a = np.random.randint(0, num_actions)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sarsa(env, gamma, alpha_initial, epsilon_initial, num_episodes):\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    Q_s = np.zeros((num_episodes, num_states, num_actions))\n",
    "    policy_success_rate = np.zeros(num_episodes//psr_gap + 1)\n",
    "    \n",
    "    alpha_array = decay(alpha_initial, 0.01, num_episodes, 'exponential')\n",
    "    epsilon_array = decay(epsilon_initial, 0.01, num_episodes, 'exponential')\n",
    "    \n",
    "    for e in range(num_episodes):\n",
    "        alpha = alpha_array[e]\n",
    "        epsilon = epsilon_array[e]\n",
    "        \n",
    "        observation, info = env.reset()\n",
    "        s = observation[\"agent\"]\n",
    "        terminated = False\n",
    "        \n",
    "        a = select_action(s, Q, epsilon)\n",
    "        \n",
    "        while not terminated:\n",
    "            observation, reward, terminated, _, info = env.step(a)\n",
    "            next_state = info[\"next_state\"]\n",
    "                           \n",
    "            next_action = select_action(s, Q, epsilon)\n",
    "            \n",
    "            td_target = reward\n",
    "            if not terminated:\n",
    "                td_target += gamma*Q[next_state, next_action]\n",
    "            Q[s, a] += alpha*(td_target - Q[s, a])\n",
    "            \n",
    "            s = next_state\n",
    "            a = next_action\n",
    "        \n",
    "        if e%psr_gap == 0:\n",
    "            policy_success_rate[int(e/psr_gap)] = get_policy_success_rate(env, Q, goal_state)\n",
    "        \n",
    "        Q_s[e] = Q\n",
    "    \n",
    "    if num_episodes%psr_gap == 0:\n",
    "        policy_success_rate[-1] = get_policy_success_rate(env, Q, goal_state)\n",
    "\n",
    "    V = np.max(Q, axis=1)\n",
    "    V_s = np.max(Q_s, axis=2)\n",
    "    optimal_policy = Q.argmax(axis=1).tolist()\n",
    "\n",
    "    return V_s, Q_s, policy_success_rate, optimal_policy\n",
    "\n",
    "process(sarsa, env, 0.96, 0.6, 0.99, num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q_learning(env, gamma, alpha_initial, epsilon_initial, num_episodes):\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    Q_s = np.zeros((num_episodes, num_states, num_actions))\n",
    "    policy_success_rate = np.zeros(num_episodes//psr_gap + 1)\n",
    "    \n",
    "    alpha_array = decay(alpha_initial, 0.01, num_episodes, 'exponential')\n",
    "    epsilon_array = decay(epsilon_initial, 0.01, num_episodes, 'exponential')\n",
    "    \n",
    "    for e in range(num_episodes):\n",
    "        alpha = alpha_array[e]\n",
    "        epsilon = epsilon_array[e]\n",
    "        \n",
    "        observation, info = env.reset()\n",
    "        s = observation[\"agent\"]\n",
    "        terminated = False\n",
    "        \n",
    "        while not terminated:\n",
    "            a = select_action(s, Q, epsilon)\n",
    "            observation, reward, terminated, _, info = env.step(a)\n",
    "            next_state = info[\"next_state\"]\n",
    "            \n",
    "            td_target = reward\n",
    "            if not terminated:\n",
    "                td_target += gamma*np.max(Q[next_state])\n",
    "            \n",
    "            Q[s, a] += alpha*(td_target - Q[s, a])\n",
    "            \n",
    "            s = next_state\n",
    "        \n",
    "        if e%psr_gap == 0:\n",
    "            policy_success_rate[int(e/psr_gap)] = get_policy_success_rate(env, Q, goal_state)\n",
    "        \n",
    "        Q_s[e] = Q\n",
    "    \n",
    "    if num_episodes%psr_gap == 0:\n",
    "        policy_success_rate[-1] = get_policy_success_rate(env, Q, goal_state)\n",
    "    \n",
    "    V = np.max(Q, axis=1)\n",
    "    V_s = np.max(Q_s, axis=2)\n",
    "    optimal_policy = Q.argmax(axis=1).tolist()\n",
    "\n",
    "    return V_s, Q_s, policy_success_rate, optimal_policy\n",
    "\n",
    "process(q_learning, env, 0.98, 0.6, 0.99, num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def double_q_learning(env, gamma, alpha_initial, epsilon_initial, num_episodes):\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    Q_s = np.zeros((num_episodes, num_states, num_actions))\n",
    "    policy_success_rate = np.zeros(num_episodes//psr_gap + 1)\n",
    "    \n",
    "    Q1 = np.zeros((num_states, num_actions))\n",
    "    Q2 = np.zeros((num_states, num_actions))\n",
    "    Q_s1 = np.zeros((num_episodes, num_states, num_actions))\n",
    "    Q_s2 = np.zeros((num_episodes, num_states, num_actions))\n",
    "    \n",
    "    alpha_array = decay(alpha_initial, 0.01, num_episodes, 'exponential')\n",
    "    epsilon_array = decay(epsilon_initial, 0.01, num_episodes, 'exponential')\n",
    "    \n",
    "    for e in range(num_episodes):\n",
    "        alpha = alpha_array[e]\n",
    "        epsilon = epsilon_array[e]\n",
    "        \n",
    "        observation, info = env.reset()\n",
    "        s = observation[\"agent\"]\n",
    "        terminated = False\n",
    "        \n",
    "        while not terminated:\n",
    "            a = select_action(s, Q, epsilon)\n",
    "            observation, reward, terminated, _, info = env.step(a)\n",
    "            next_state = info[\"next_state\"]\n",
    "            \n",
    "            if np.random.randint(2) == 0:\n",
    "                a1 = np.argmax(Q1[next_state])\n",
    "                \n",
    "                td_target = reward\n",
    "                if not terminated:\n",
    "                    td_target += gamma*Q2[next_state, a1]\n",
    "                \n",
    "                Q1[s, a] += alpha*(td_target - Q1[s, a])\n",
    "            else:\n",
    "                a2 = np.argmax(Q2[next_state])\n",
    "                \n",
    "                td_target = reward\n",
    "                if not terminated:\n",
    "                    td_target += gamma*Q1[next_state, a2]\n",
    "                \n",
    "                Q2[s, a] += alpha*(td_target - Q2[s, a])\n",
    "            \n",
    "            s = next_state\n",
    "\n",
    "        if e%psr_gap == 0:\n",
    "            policy_success_rate[int(e/psr_gap)] = get_policy_success_rate(env, Q, goal_state)\n",
    "        \n",
    "        Q_s1[e] = Q1\n",
    "        Q_s2[e] = Q2\n",
    "        Q = (Q1 + Q2)/2\n",
    "        Q_s = (Q_s1 + Q_s2)/2\n",
    "    \n",
    "    if num_episodes%psr_gap == 0:\n",
    "        policy_success_rate[-1] = get_policy_success_rate(env, Q, goal_state)\n",
    "    \n",
    "    V = np.max(Q, axis=1)\n",
    "    V_s = np.max(Q_s, axis=2)\n",
    "    optimal_policy = Q.argmax(axis=1).tolist()\n",
    "\n",
    "    return V_s, Q_s, policy_success_rate, optimal_policy\n",
    "\n",
    "process(double_q_learning, env, 0.95, 0.5, 0.99, num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Control Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_policy_success_rate(['monte_carlo_control', 'sarsa', 'q_learning', 'double_q_learning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA(位) Replacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clip(E, s, a):\n",
    "    E[s, :] = 0\n",
    "    E[s, a] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sarsa_lambda(env, gamma, alpha_initial, epsilon_initial, lda, num_episodes, replace_trace = True):\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    E = np.zeros((num_states, num_actions))\n",
    "    Q_s = np.zeros((num_episodes, num_states, num_actions))\n",
    "    policy_success_rate = np.zeros(num_episodes//psr_gap + 1)\n",
    "    \n",
    "    alpha_array = decay(alpha_initial, 0.01, num_episodes, 'exponential')\n",
    "    epsilon_array = decay(epsilon_initial, 0.01, num_episodes, 'exponential')\n",
    "    \n",
    "    for e in range(num_episodes):\n",
    "        alpha = alpha_array[e]\n",
    "        epsilon = epsilon_array[e]\n",
    "        E = E*0\n",
    "        \n",
    "        observation, info = env.reset()\n",
    "        s = observation[\"agent\"]\n",
    "        terminated = False\n",
    "        a = select_action(s, Q, epsilon)\n",
    "        \n",
    "        while not terminated:\n",
    "            observation, reward, terminated, _, info = env.step(a)\n",
    "            next_state = info[\"next_state\"]            \n",
    "            next_action = select_action(next_state, Q, epsilon)\n",
    "            \n",
    "            td_target = reward\n",
    "            if not terminated:\n",
    "                td_target += gamma*Q[next_state, next_action]\n",
    "            td_error = td_target - Q[s, a]\n",
    "            \n",
    "            E[s, a] += 1\n",
    "            \n",
    "            if replace_trace:\n",
    "                clip(E, s, a)\n",
    "            \n",
    "            Q += alpha*td_error*E\n",
    "            \n",
    "            E = gamma*lda*E\n",
    "            \n",
    "            s = next_state\n",
    "            a = next_action\n",
    "        \n",
    "        if e%psr_gap == 0:\n",
    "            policy_success_rate[int(e/psr_gap)] = get_policy_success_rate(env, Q, goal_state)\n",
    "        \n",
    "        Q_s[e] = Q\n",
    "    \n",
    "    if num_episodes%psr_gap == 0:\n",
    "        policy_success_rate[-1] = get_policy_success_rate(env, Q, goal_state)\n",
    "    \n",
    "    V = np.max(Q, axis=1)\n",
    "    V_s = np.max(Q_s, axis=2)\n",
    "    optimal_policy = Q.argmax(axis=1).tolist()\n",
    "\n",
    "    return V_s, Q_s, policy_success_rate, optimal_policy\n",
    "\n",
    "process(sarsa_lambda, env, 0.98, 0.65, 0.99, 0.5, num_episodes, True, type_of=\"replacing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA(位) Accumulating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process(sarsa_lambda, env, 0.98, 0.65, 0.99, 0.5, num_episodes, False, type_of=\"accumulating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q(位) Replacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q_lambda(env, gamma, alpha_initial, epsilon_initial, lda, num_episodes, replace_trace = True):\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    E = np.zeros((num_states, num_actions))\n",
    "    Q_s = np.zeros((num_episodes, num_states, num_actions))\n",
    "    policy_success_rate = np.zeros(num_episodes//psr_gap + 1)\n",
    "    \n",
    "    alpha_array = decay(alpha_initial, 0.01, num_episodes, 'exponential')\n",
    "    epsilon_array = decay(epsilon_initial, 0.01, num_episodes, 'exponential')\n",
    "    \n",
    "    for e in range(num_episodes):\n",
    "        alpha = alpha_array[e]\n",
    "        epsilon = epsilon_array[e]\n",
    "        E = E*0\n",
    "        \n",
    "        observation, info = env.reset()\n",
    "        s = observation[\"agent\"]\n",
    "        terminated = False\n",
    "        a = select_action(s, Q, epsilon)\n",
    "        \n",
    "        while not terminated:\n",
    "            observation, reward, terminated, _, info = env.step(a)\n",
    "            next_state = info[\"next_state\"]   \n",
    "            next_action = select_action(next_state, Q, epsilon)\n",
    "            \n",
    "            is_next_action_greedy = False\n",
    "            if Q[next_state, next_action] == np.max(Q[next_state, :]):\n",
    "                is_next_action_greedy = True\n",
    "            \n",
    "            td_target = reward\n",
    "            if not terminated:\n",
    "                td_target += gamma*np.max(Q[next_action, :])\n",
    "            td_error = td_target - Q[s, a]\n",
    "            \n",
    "            if replace_trace:\n",
    "                E[s] = 0\n",
    "            \n",
    "            E[s, a] += 1\n",
    "            \n",
    "            Q += alpha*td_error*E\n",
    "            \n",
    "            if is_next_action_greedy:\n",
    "                E = gamma*lda*E\n",
    "            else:\n",
    "                E = E*0\n",
    "            \n",
    "            s = next_state\n",
    "            a = next_action\n",
    "        \n",
    "        if e%psr_gap == 0:\n",
    "            policy_success_rate[int(e/psr_gap)] = get_policy_success_rate(env, Q, goal_state)\n",
    "        \n",
    "        Q_s[e] = Q\n",
    "    \n",
    "    if num_episodes%psr_gap == 0:\n",
    "        policy_success_rate[-1] = get_policy_success_rate(env, Q, goal_state)\n",
    "    \n",
    "    V = np.max(Q, axis=1)\n",
    "    V_s = np.max(Q_s, axis=2)\n",
    "    optimal_policy = Q.argmax(axis=1).tolist()\n",
    "\n",
    "    return V_s, Q_s, policy_success_rate, optimal_policy\n",
    "\n",
    "process(q_lambda, env, 0.95, 0.5, 0.99, 0.55, num_episodes, True, type_of=\"replacing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q(位) Accumulating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process(q_lambda, env, 0.95, 0.5, 0.99, 0.55, num_episodes, False, type_of=\"accumulating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10: Dyna-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_visited_states_and_actions_taken(T):\n",
    "    states_visited = np.zeros(num_states)\n",
    "    actions_taken = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    for s in range(num_states):\n",
    "        for a in range(num_actions):\n",
    "            if np.sum(T[s, a]) != 0:\n",
    "                states_visited[s] = 1\n",
    "                actions_taken[s, a] = 1\n",
    "    \n",
    "    states = []\n",
    "    actions_in_state = []\n",
    "    \n",
    "    for i in range(num_states):\n",
    "        actions = []\n",
    "        if states_visited[i] == 1:\n",
    "            states.append(i)\n",
    "            \n",
    "            for j in range(num_actions):\n",
    "                if actions_taken[i, j] == 1:\n",
    "                    actions.append(j)\n",
    "        actions_in_state.append(actions)\n",
    "    \n",
    "    return states, actions_in_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dyna_q(env, gamma, alpha_initial, epsilon_initial, num_episodes, num_planning):\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    E = np.zeros((num_states, num_actions))\n",
    "    Q_s = np.zeros((num_episodes, num_states, num_actions))\n",
    "    T = np.zeros((num_states, num_actions, num_states))\n",
    "    R = np.zeros((num_states, num_actions, num_states))\n",
    "    policy_success_rate = np.zeros(num_episodes//psr_gap + 1)\n",
    "    \n",
    "    alpha_array = decay(alpha_initial, 0.01, num_episodes, 'exponential')\n",
    "    epsilon_array = decay(epsilon_initial, 0.01, num_episodes, 'exponential')\n",
    "    \n",
    "    for e in range(num_episodes):\n",
    "        alpha = alpha_array[e]\n",
    "        epsilon = epsilon_array[e]\n",
    "        E = E*0\n",
    "        \n",
    "        observation, info = env.reset()\n",
    "        s = observation[\"agent\"]\n",
    "        terminated=False\n",
    "        \n",
    "        while not terminated:\n",
    "            a = select_action(s, Q, epsilon)\n",
    "            observation, reward, terminated, _, info = env.step(a)\n",
    "            next_state = info[\"next_state\"]  \n",
    "            \n",
    "            T[s, a, next_state] += 1\n",
    "            \n",
    "            r_diff = reward  - R[s, a, next_state]\n",
    "            \n",
    "            R[s, a, next_state] += (r_diff/T[s, a, next_state])\n",
    "            \n",
    "            td_target = reward\n",
    "            if not terminated:\n",
    "                td_target += gamma*np.max(Q[next_state, :])\n",
    "            td_error = td_target - Q[s, a]\n",
    "            \n",
    "            Q[s, a] += alpha*td_error\n",
    "            \n",
    "            s_backup = next_state\n",
    "            \n",
    "            for _ in range(num_planning):\n",
    "                if np.sum(Q) == 0:\n",
    "                    break\n",
    "                \n",
    "                s_visited, a_taken = get_visited_states_and_actions_taken(T)\n",
    "                \n",
    "                s = np.random.choice(s_visited)\n",
    "                a = np.random.choice(a_taken[s])\n",
    "                \n",
    "                prob_next_state = T[s, a]/np.sum(T[s, a])\n",
    "                \n",
    "                next_state = np.random.choice(range(num_states), p=prob_next_state)\n",
    "                r = R[s, a, next_state]\n",
    "                \n",
    "                td_target = r + gamma*np.max(Q[next_state])\n",
    "                td_error = td_target - Q[s, a]\n",
    "                Q[s, a] += alpha*td_error\n",
    "            \n",
    "            s = s_backup\n",
    "        \n",
    "        if e%psr_gap == 0:\n",
    "            policy_success_rate[int(e/psr_gap)] = get_policy_success_rate(env, Q, goal_state)\n",
    "        \n",
    "        Q_s[e] = Q\n",
    "        \n",
    "                \n",
    "        \n",
    "    if num_episodes%psr_gap == 0:\n",
    "        policy_success_rate[-1] = get_policy_success_rate(env, Q, goal_state)\n",
    "    \n",
    "    V = np.max(Q, axis=1)\n",
    "    V_s = np.max(Q_s, axis=2)\n",
    "    optimal_policy = Q.argmax(axis=1).tolist()\n",
    "\n",
    "    return V_s, Q_s, policy_success_rate, optimal_policy\n",
    "\n",
    "process(dyna_q, env, 0.98, 0.7, 0.99, num_episodes, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11: Trajectory Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trajectory_sampling(env, gamma, alpha_initial, epsilon_initial, num_episodes, max_trajectory):\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    E = np.zeros((num_states, num_actions))\n",
    "    Q_s = np.zeros((num_episodes, num_states, num_actions))\n",
    "    T = np.zeros((num_states, num_actions, num_states))\n",
    "    R = np.zeros((num_states, num_actions, num_states))\n",
    "    policy_success_rate = np.zeros(num_episodes//psr_gap + 1)\n",
    "    \n",
    "    alpha_array = decay(alpha_initial, 0.01, num_episodes, 'exponential')\n",
    "    epsilon_array = decay(epsilon_initial, 0.01, num_episodes, 'exponential')\n",
    "    \n",
    "    for e in range(num_episodes):\n",
    "        alpha = alpha_array[e]\n",
    "        epsilon = epsilon_array[e]\n",
    "        E = E*0\n",
    "        \n",
    "        observation, info = env.reset()\n",
    "        s = observation[\"agent\"]\n",
    "        terminated=False\n",
    "        \n",
    "        while not terminated:\n",
    "            a = select_action(s, Q, epsilon)\n",
    "            observation, reward, terminated, _, info = env.step(a)\n",
    "            next_state = info[\"next_state\"]  \n",
    "            \n",
    "            T[s, a, next_state] += 1\n",
    "            \n",
    "            r_diff = reward  - R[s, a, next_state]\n",
    "            \n",
    "            R[s, a, next_state] += (r_diff/T[s, a, next_state])\n",
    "            \n",
    "            td_target = reward\n",
    "            if not terminated:\n",
    "                td_target += gamma*np.max(Q[next_state, :])\n",
    "            td_error = td_target - Q[s, a]\n",
    "            \n",
    "            Q[s, a] += alpha*td_error\n",
    "            \n",
    "            s_backup = next_state\n",
    "            \n",
    "            for _ in range(max_trajectory):\n",
    "                if np.sum(Q) == 0:\n",
    "                    break\n",
    "                \n",
    "                a = select_action(s, Q, epsilon)\n",
    "                \n",
    "                if np.sum(T[s, a]) == 0:\n",
    "                    break\n",
    "                \n",
    "                prob_next_state = T[s, a]/np.sum(T[s, a])\n",
    "                \n",
    "                next_state = np.random.choice(range(num_states), p=prob_next_state)\n",
    "                r = R[s, a, next_state]\n",
    "                \n",
    "                td_target = r + gamma*np.max(Q[next_state])\n",
    "                td_error = td_target - Q[s, a]\n",
    "                Q[s, a] += alpha*td_error\n",
    "                \n",
    "                s = next_state\n",
    "            \n",
    "            s = s_backup\n",
    "        \n",
    "        if e%psr_gap == 0:\n",
    "            policy_success_rate[int(e/psr_gap)] = get_policy_success_rate(env, Q, goal_state)\n",
    "        \n",
    "        Q_s[e] = Q\n",
    "        \n",
    "                \n",
    "        \n",
    "    if num_episodes%psr_gap == 0:\n",
    "        policy_success_rate[-1] = get_policy_success_rate(env, Q, goal_state)\n",
    "    \n",
    "    V = np.max(Q, axis=1)\n",
    "    V_s = np.max(Q_s, axis=2)\n",
    "    optimal_policy = Q.argmax(axis=1).tolist()\n",
    "\n",
    "    return V_s, Q_s, policy_success_rate, optimal_policy\n",
    "\n",
    "process(trajectory_sampling, env, 0.99, 0.5, 0.99, num_episodes, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 12: Comparing Control Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_policy_success_rate(['sarsa_lambda_replacing', 'sarsa_lambda_accumulating', 'q_lambda_replacing', 'q_lambda_accumulating', \n",
    "                          'dyna_q', \n",
    "                          'trajectory_sampling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
