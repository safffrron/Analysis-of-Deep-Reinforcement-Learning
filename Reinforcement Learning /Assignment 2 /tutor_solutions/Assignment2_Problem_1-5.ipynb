{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bee0dd-19cd-4cc0-b66d-681d83b9c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3539ac-12d6-4de3-9154-86eb6825d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from gymnasium import Env, spaces\n",
    "from gymnasium.spaces import  Discrete\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import bernoulli\n",
    "import yaml\n",
    "from matplotlib import colors\n",
    "from tqdm import trange\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432101c4-a080-4951-8f49-f6be763fbfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomMaze(Env):\n",
    "    def __init__(self):\n",
    "        self.start_state = int(8)\n",
    "        self.goal = int(3)\n",
    "        self.hole = int(7)\n",
    "        self.state = int(8)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Discrete(12)\n",
    "        self.P = {\n",
    "            0: {\n",
    "                0: [(0.8, 0, -0.04, False), (0.1, 0, -0.04, False), (0.1, 4, -0.04, False)],\n",
    "                1: [(0.8, 0, -0.04, False), (0.1, 0, -0.04, False), (0.1, 1, -0.04, False)],\n",
    "                2: [(0.8, 1, -0.04, False), (0.1, 0, -0.04, False), (0.1, 4, -0.04, False)],\n",
    "                3: [(0.8, 4, -0.04, False), (0.1, 0, -0.04, False), (0.1, 1, -0.04, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.8, 0, -0.04, False), (0.1, 1, -0.04, False), (0.1, 1, -0.04, False)],\n",
    "                1: [(0.8, 1, -0.04, False), (0.1, 0, -0.04, False), (0.1, 2, -0.04, False)],\n",
    "                2: [(0.8, 2, -0.04, False), (0.1, 1, -0.04, False), (0.1, 1, -0.04, False)],\n",
    "                3: [(0.8, 1, -0.04, False), (0.1, 0, -0.04, False), (0.1, 2, -0.04, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.8, 1, -0.04, False), (0.1, 2, -0.04, False), (0.1, 6, -0.04, False)],\n",
    "                1: [(0.8, 2, -0.04, False), (0.1, 1, -0.04, False), (0.1, 3, +1, True)],\n",
    "                2: [(0.8, 3, +1, True), (0.1, 2, -0.04, False), (0.1, 6, -0.04, False)],\n",
    "                3: [(0.8, 6, -0.04, False), (0.1, 1, -0.04, False), (0.1, 3, +1, True)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                1: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                2: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                3: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.8, 4, -0.04, False), (0.1, 0, -0.04, False), (0.1, 8, -0.04, False)],\n",
    "                1: [(0.8, 0, -0.04, False), (0.1, 4, -0.04, False), (0.1, 4, -0.04, False)],\n",
    "                2: [(0.8, 4, -0.04, False), (0.1, 0, -0.04, False), (0.1, 8, -0.04, False)],\n",
    "                3: [(0.8, 8, -0.04, False), (0.1, 4, -0.04, False), (0.1, 4, -0.04, False)],\n",
    "            },\n",
    "            6: {\n",
    "                0: [(0.8, 6, -0.04, False), (0.1, 2, -0.04, False), (0.1, 10, -0.04, False)],\n",
    "                1: [(0.8, 2, -0.04, False), (0.1, 6, -0.04, False), (0.1, 7, -1, True)],\n",
    "                2: [(0.8, 7, -1, True), (0.1, 2, -0.04, False), (0.1, 10, -0.04, False)],\n",
    "                3: [(0.8, 10, -0.04, False), (0.1, 6, -0.04, False), (0.1, 7, -1, True)],\n",
    "            },\n",
    "            7: {\n",
    "                0: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                1: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                2: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                3: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "            },\n",
    "            8: {\n",
    "                0: [(0.8, 8, -0.04, False), (0.1, 8, -0.04, False), (0.1, 4, -0.04, False)],\n",
    "                1: [(0.8, 4, -0.04, False), (0.1, 8, -0.04, False), (0.1, 9, -0.04, False)],\n",
    "                2: [(0.8, 9, -0.04, False), (0.1, 8, -0.04, False), (0.1, 4, -0.04, False)],\n",
    "                3: [(0.8, 8, -0.04, False), (0.1, 8, -0.04, False), (0.1, 9, -0.04, False)],\n",
    "            },\n",
    "            9: {\n",
    "                0: [(0.8, 8, -0.04, False), (0.1, 9, -0.04, False), (0.1, 9, -0.04, False)],\n",
    "                1: [(0.8, 9, -0.04, False), (0.1, 8, -0.04, False), (0.1, 10, -0.04, False)],\n",
    "                2: [(0.8, 10, -0.04, False), (0.1, 9, -0.04, False), (0.1, 9, -0.04, False)],\n",
    "                3: [(0.8, 9, -0.04, False), (0.1, 8, -0.04, False), (0.1, 10, -0.04, False)]\n",
    "            },\n",
    "            10: {\n",
    "                0: [(0.8, 9, -0.04, False), (0.1, 6, -0.04, False), (0.1, 10, -0.04, False)],\n",
    "                1: [(0.8, 6, -0.04, False), (0.1, 9, -0.04, False), (0.1, 11, -0.04, False)],\n",
    "                2: [(0.8, 11, -0.04, False), (0.1, 6, -0.04, False), (0.1, 10, -0.04, False)],\n",
    "                3: [(0.8, 10, -0.04, False), (0.1, 9, -0.04, False), (0.1, 11, -0.04, False)]\n",
    "            },\n",
    "            11: {\n",
    "                0: [(0.8, 10, -0.04, False), (0.1, 7, -1, True), (0.1, 11, -0.04, False)],\n",
    "                1: [(0.8, 7, -1, True), (0.1, 10, -0.04, False), (0.1, 11, -0.04, False)],\n",
    "                2: [(0.8, 11, -0.04, False), (0.1, 7, -1, True), (0.1, 11, -0.04, False)],\n",
    "                3: [(0.8, 11, -0.04, False), (0.1, 11, -0.04, False), (0.1, 10, -0.04, False)]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def seed(self, seed):\n",
    "        np.random.seed(seed)\n",
    "        return seed\n",
    "\n",
    "    def step(self, action):\n",
    "        d = np.random.choice([0, 1, 2], p=[0.8, 0.1, 0.1])\n",
    "        tr = self.P[int(self.state)][int(action)][int(d)]\n",
    "        prob = tr[0]\n",
    "        self.state = int(tr[1])\n",
    "        reward = tr[2]\n",
    "        done = tr[3]\n",
    "        return self.state, reward, done,False, {}\n",
    "\n",
    "\n",
    "    def  reset(self, seed=None, options=None):\n",
    "        # reset state\n",
    "        super().reset(seed=seed)\n",
    "        self.state=self.start_state \n",
    "        done=False\n",
    "        info = {}\n",
    "        return self.state,info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fbdae-9a75-4a02-b3d4-a208e24ae04e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c4682-afc0-4e55-88d1-4eda1538a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env, register\n",
    "\n",
    "register(id='RME-v0', entry_point=RandomMaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2d558-ac6f-441a-bdfa-d74934d5b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('RME-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b3b508-37a6-4600-b75e-f8fcf34d48c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "alpha = 0.05\n",
    "alpha_final = 0.01\n",
    "epsilon = 0.37\n",
    "decay_rate = 0.05\n",
    "maxSteps = 50\n",
    "noEpisodes = 500\n",
    "noPlanning=5\n",
    "maxTrajectory = 10\n",
    "\n",
    "nStates = 12\n",
    "nActions = 4\n",
    "smooth_iters = 10\n",
    "interesting_states = [0, 1, 2, 4, 6, 8, 9, 10, 11]\n",
    "action_dict = {'0':\"LEFT\", '1':\"UP\", '2':\"RIGHT\", '3':\"DOWN\"}\n",
    "action_list = [\"LEFT\", \"UP\", \"RIGHT\", \"DOWN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e4401-b312-43c0-a3c9-09a3ab6d713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_state_values(V, episodes=None, prefix=''):\n",
    "    states_to_plot = [0, 1, 2, 4, 6, 8, 9, 10, 11]\n",
    "    true_v_values = np.array([0.824, 0.893, 0.955, 0.0, 0.764, 0.0, 0.688, 0.0, 0.698, 0.639, 0.606, 0.382])\n",
    "    if episodes is None:\n",
    "        episodes = np.arange(V.shape[0])\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(states_to_plot)))\n",
    "\n",
    "    for idx, state in enumerate(states_to_plot):\n",
    "        plt.plot(episodes, V[:, state], label=f'State {state}', color=colors[idx])\n",
    "        if true_v_values is not None:\n",
    "            plt.plot(episodes, [true_v_values[state]]*len(episodes), linestyle='--', label=f'True Value State {state}', color=colors[idx], alpha=0.5)\n",
    "\n",
    "    plt.title(\"State-Value (V) Function vs. Episodes\", fontsize=16)\n",
    "    plt.xlabel(\"Episodes\", fontsize=14)\n",
    "    plt.ylabel(\"State Values\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{prefix}_state_values.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def plot_action_values(Q, episodes=None, prefix=''):\n",
    "    if episodes is None:\n",
    "        episodes = np.arange(Q.shape[0])\n",
    "\n",
    "    states_to_plot = [0, 1, 2, 4, 6, 8, 9, 10, 11]\n",
    "    actions = [\"LEFT\", \"UP\", \"RIGHT\", \"DOWN\"]\n",
    "    num_actions = len(actions)\n",
    "    # Adjust the subplot layout based on the number of actions\n",
    "    cols = 2  # You can adjust this to change how many columns of subplots you want\n",
    "    rows = (num_actions + cols - 1) // cols  # Calculate rows needed based on the number of actions\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "    if num_actions == 1:\n",
    "        axs = [axs]  # Make axs iterable if there's only one plot\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for action_idx, action in enumerate(actions):\n",
    "        ax = axs[action_idx]\n",
    "        for state in states_to_plot:\n",
    "            ax.plot(episodes, Q[:, state, action_idx], label=f'State {state}, Action {action}')\n",
    "        ax.set_title(f\"Action {action}\")\n",
    "        ax.set_xlabel(\"Episodes\")\n",
    "        ax.set_ylabel(\"Q Values\")\n",
    "        ax.legend()\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for ax in axs[num_actions:]:\n",
    "        ax.set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{prefix}_action_values.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_state_action_values(Q, episodes=None, prefix=''):\n",
    "    if episodes is None:\n",
    "        episodes = np.arange(Q.shape[0])\n",
    "\n",
    "    interesting_states = [0, 1, 2, 4, 6, 8, 9, 10, 11]\n",
    "    actions = [\"LEFT\", \"UP\", \"RIGHT\", \"DOWN\"]\n",
    "\n",
    "    for state in interesting_states:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        for action_idx, action in enumerate(actions):\n",
    "            ax.plot(episodes, Q[:, state, action_idx], label=f'Action {action}')\n",
    "        ax.set_title(f\"State {state}: Q-Values across Actions\")\n",
    "        ax.set_xlabel(\"Episodes\")\n",
    "        ax.set_ylabel(\"Q-Values\")\n",
    "        ax.legend()\n",
    "        plt.savefig(f\"{prefix}_state{state}_action_values.pdf\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def visualize_state_and_action_values(state_values, action_values):\n",
    "    # Define optimal state values for comparison\n",
    "    benchmark_values = np.array([0.824, 0.893, 0.955, 0.0, 0.764, 0.0, 0.688, 0.0, 0.698, 0.639, 0.606, 0.382])\n",
    "    \n",
    "    # Define a new palette of colors using HEX codes for uniqueness\n",
    "    color_palette = ['#483D8B', '#32CD32', '#FF4500', '#008080', '#9932CC', '#2E8B57', '#FFA500', '#8B4513', '#6B8E23', '#4682B4', '#696969', '#FF69B4']\n",
    "    \n",
    "    episode_range = np.arange(len(state_values))\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = (15, 10)\n",
    "    plt.cla()\n",
    "    for state_index in range(len(benchmark_values)):\n",
    "        if state_index == 5:  # Skipping a specific state if needed\n",
    "            continue\n",
    "        plt.plot(episode_range, state_values[:, state_index], label=f'State {state_index}', color=color_palette[state_index])\n",
    "        plt.plot(episode_range, [benchmark_values[state_index]]*len(state_values), color=color_palette[state_index], linestyle='--')\n",
    "    plt.legend(loc=(1.01,0))\n",
    "    plt.title(\"State-Value (V) Function vs. Episodes Across States\")\n",
    "    plt.xlabel('Episode Count')\n",
    "    plt.ylabel('State Values')\n",
    "    plt.show()\n",
    "    \n",
    "    for selected_state in interesting_states:\n",
    "        episode_range = np.arange(len(state_values))\n",
    "        legend = []\n",
    "        \n",
    "        plt.cla()\n",
    "        for action_index in range(action_values.shape[2]):  # Assuming action_values shape is [episodes, states, actions]\n",
    "            plt.plot(episode_range, action_values[:, selected_state, action_index], label=f'Action {action_index}')\n",
    "            legend.append(f\"Action {action_index}\")\n",
    "        plt.legend(legend, loc='best')\n",
    "        plt.title(f\"Action-Value (Q) Function vs. Episodes for State {selected_state}\")\n",
    "        plt.xlabel('Episode Count')\n",
    "        plt.ylabel('Action Values')\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc3ca1-fe88-4e0a-9276-ebe56857f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generateTrajectory(env, Q, epsilon, max_steps=1000):\n",
    "    steps = 0\n",
    "    done = False\n",
    "    traj = []\n",
    "\n",
    "    # Assuming the reset method returns the initial state which is ignored here\n",
    "    # If needed, capture the initial state as: state = env.reset()\n",
    "    env.reset()\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        old_state = env.state\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > epsilon:\n",
    "            action = np.argmax(Q[old_state])\n",
    "        else:\n",
    "            action = np.random.randint(0, env.action_space.n)  # Assuming env.action_space.n is valid\n",
    "\n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "        traj.append((old_state, action, reward, new_state))\n",
    "        \n",
    "        steps += 1\n",
    "\n",
    "    return traj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae31b36-19af-4433-bef7-7b275a640723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decayAlpha(initialValue, finalValue, maxSteps, decayType):\n",
    "    step_size=np.zeros(maxSteps)\n",
    "    if decayType==\"linear\":\n",
    "        decayRate=(initialValue-finalValue)/(maxSteps -1)\n",
    "        for i in range(maxSteps):\n",
    "            step_size[i]=initialValue-i*decayRate\n",
    "\n",
    "    else:\n",
    "        decayRate=(np.log(initialValue/finalValue))/maxSteps\n",
    "        step_size[0]=initialValue\n",
    "        for i in range(maxSteps-1):\n",
    "            step_size[i+1]=step_size[i]*np.exp(-decayRate)\n",
    "\n",
    "    return step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c15cd-049a-4fbe-857e-e1ada2004e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_epsilon(epsilon, decay_rate, decay_type):\n",
    "    min_value = 0.0\n",
    "    if decay_type == 'linear':\n",
    "        epsilon = max(min_value, epsilon-decay_rate*epsilon)\n",
    "    else:\n",
    "        epsilon = max(min_value, epsilon*np.exp(-1*decay_rate))\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd3411f-d633-40ba-b7cd-e1823c2c79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_control(env, gamma, alpha, epsilon, maxSteps, noEpisodes, firstVisit = True):\n",
    "    Q = np.zeros((nStates, nActions))\n",
    "    Qe = np.zeros((noEpisodes, nStates, nActions))\n",
    "    alphas = decayAlpha(alpha, alpha_final, noEpisodes, 'linear')\n",
    "    for ep in range(noEpisodes):\n",
    "        alpha = alphas[ep]\n",
    "        epsilon = decay_epsilon(epsilon, decay_rate, 'linear')\n",
    "        traj = generateTrajectory(env, Q, epsilon, maxSteps)\n",
    "        visited = np.zeros((nStates, nActions))\n",
    "        for i, (old_state, action, reward, new_state) in enumerate(traj):\n",
    "            if visited[old_state, action] != 0 and firstVisit:\n",
    "                continue\n",
    "\n",
    "            visited[old_state, action] = 1\n",
    "\n",
    "            g_t = 0\n",
    "            for j in range(i, len(traj)):\n",
    "                reward = traj[j][2]\n",
    "                g_t += (gamma**(j-i)) * reward \n",
    "            G = g_t\n",
    "            Q[old_state, action] += alpha * (G - Q[old_state, action])\n",
    "        Qe[ep] = Q\n",
    "    state_value = np.max(Qe, axis=-1)\n",
    "    q_value = Qe\n",
    "    optimal_policy = np.argmax(Q, axis=-1)\n",
    "        \n",
    "    return state_value, q_value, optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd4b444-7327-4b4a-a393-928dd0ec225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_value_all = np.zeros((smooth_iters, noEpisodes, nStates))\n",
    "q_value_all = np.zeros((smooth_iters, noEpisodes, nStates, nActions))\n",
    "for it in trange(smooth_iters, desc='Smoothing'):\n",
    "    env.seed(it)\n",
    "    state_value, q_value, optimal_policy = monte_carlo_control(env, gamma, alpha, epsilon, maxSteps, noEpisodes, firstVisit = True)\n",
    "    state_value_all[it] = state_value\n",
    "    q_value_all[it] = q_value\n",
    "state_value = np.average(state_value_all, axis=0)\n",
    "q_value = np.average(q_value_all, axis=0)\n",
    "'''\n",
    "plot_state_values(state_value,  episodes=range(noEpisodes))\n",
    "plot_action_values(q_value,  episodes=range(noEpisodes))\n",
    "plot_state_action_values(q_value,  episodes=range(noEpisodes))\n",
    "'''\n",
    "plot_state_values(state_value,  episodes=range(noEpisodes), prefix='mcc-')\n",
    "plot_action_values(q_value,  episodes=range(noEpisodes), prefix='mcc-')\n",
    "plot_state_action_values(q_value,  episodes=range(noEpisodes), prefix='mcc-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43824e5a-5e28-44b3-a009-0194dd2e4152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def visualize_policy(grid_policy):\n",
    "    reshaped_policy = np.array(grid_policy).reshape((3, 4))\n",
    "    \n",
    "    # Define a modern color map\n",
    "    cmap = mcolors.ListedColormap(['#f0f0f0', '#f0f0f0'])  # Light grey for the grid\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))  # Adjust for a more aesthetic aspect ratio\n",
    "    ax.imshow(reshaped_policy, cmap=cmap)\n",
    "    \n",
    "    # Set up grid aesthetics\n",
    "    ax.set_xticks(np.arange(4))\n",
    "    ax.set_yticks(np.arange(3))\n",
    "    ax.set_xticks(np.arange(-.5, 4, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-.5, 3, 1), minor=True)\n",
    "    ax.grid(which='minor', color='#333333', linestyle='-', linewidth=2)  # Darker grid lines for contrast\n",
    "    ax.tick_params(which='both', bottom=False, left=False, labelleft=False, labelbottom=False)  # Hide ticks\n",
    "    \n",
    "    # Annotate cells with directions or special markers\n",
    "    directions = {0: '←', 1: '↑', 2: '→', 3: '↓'}\n",
    "    special_states = {5: 'W', 3: 'G', 7: 'H'}  # Define special states separately for clarity\n",
    "    for y in range(3):\n",
    "        for x in range(4):\n",
    "            state = y*4 + x\n",
    "            cell_value = reshaped_policy[y, x]\n",
    "            text = special_states.get(state, directions.get(cell_value, ''))\n",
    "            ax.text(x, y, text, ha='center', va='center', color='#007acc', fontsize=15, weight='bold')  # Use a bold, blue font for visibility\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e726640-b904-499a-889c-8ec01b5d641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_optimal_policy = np.argmax(q_value[-1], axis=-1)\n",
    "visualize_policy(smooth_optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3120e859-3a0c-49e8-a924-aaf15cd4f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actionSelect(state, Q, epsilon):\n",
    "    u = np.random.uniform()\n",
    "    if u > epsilon:\n",
    "        action = np.argmax(Q[env.state])\n",
    "    else:\n",
    "        action = np.random.randint(0, nActions)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e82a3-a435-4f1f-af42-cb36557c2d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, gamma, alpha, epsilon, noEpisodes):\n",
    "    Q = np.zeros((nStates, nActions))\n",
    "    Qe = np.zeros((noEpisodes, nStates, nActions))\n",
    "    alphas = decayAlpha(alpha, alpha_final, noEpisodes, 'linear')\n",
    "    for ep in range(noEpisodes):\n",
    "        alpha = alphas[ep]\n",
    "        epsilon = decay_epsilon(epsilon, decay_rate, 'linear')\n",
    "        \n",
    "        old_state,done = env.reset()\n",
    "        old_action = actionSelect(old_state, Q, epsilon)        \n",
    "        \n",
    "        while not done:            \n",
    "            new_state, reward, done,truncated,info = env.step(old_action)\n",
    "            new_action = actionSelect(new_state, Q, epsilon)\n",
    "            td_target = reward\n",
    "            \n",
    "            if not done:\n",
    "                td_target = td_target + gamma * Q[new_state, new_action]\n",
    "            \n",
    "            td_error = td_target - Q[old_state, old_action]\n",
    "\n",
    "            Q[old_state, old_action] = Q[old_state, old_action] + alpha * td_error\n",
    "            old_state = new_state\n",
    "            old_action = new_action                \n",
    "        Qe[ep, :] = Q        \n",
    "    \n",
    "    V = np.max(Qe, axis = 2)\n",
    "    optimal_policy = np.amax(Qe, axis = 2)\n",
    "        \n",
    "    state_value = V\n",
    "    q_value = Qe\n",
    "    return state_value, q_value, optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a0db8-591a-45b0-aed8-386646a81dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_value_all = np.zeros((smooth_iters, noEpisodes, nStates))\n",
    "q_value_all = np.zeros((smooth_iters, noEpisodes, nStates, nActions))\n",
    "for it in trange(smooth_iters, desc='Smoothing'):\n",
    "    env.seed(it)\n",
    "    state_value, q_value, optimal_policy = sarsa(env, gamma, alpha, epsilon, noEpisodes)\n",
    "    state_value_all[it] = state_value\n",
    "    q_value_all[it] = q_value\n",
    "state_value = np.average(state_value_all, axis=0)\n",
    "q_value = np.average(q_value_all, axis=0)\n",
    "plot_state_values(state_value,  episodes=range(noEpisodes), prefix= 'sarsa-')\n",
    "plot_action_values(q_value,  episodes=range(noEpisodes), prefix='sarsa-')\n",
    "plot_state_action_values(q_value,  episodes=range(noEpisodes), prefix='sarsa-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecda3b3-42e8-4f8a-bf2c-18e61f6cc36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_optimal_policy = np.argmax(q_value[-1], axis=-1)\n",
    "visualize_policy(smooth_optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87003919-e9ef-484b-8b91-f7f3ff340cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, gamma, alpha, epsilon, noEpisodes):\n",
    "    Q = np.zeros((nStates, nActions))\n",
    "    Qe = np.zeros((noEpisodes, nStates, nActions))\n",
    "    alphas = decayAlpha(alpha, alpha_final, noEpisodes, 'linear')\n",
    "    for ep in range(noEpisodes):\n",
    "        alpha = alphas[ep]\n",
    "        epsilon = decay_epsilon(epsilon, decay_rate, 'linear')\n",
    "        \n",
    "        old_state,done = env.reset()\n",
    "        \n",
    "        while not done:\n",
    "            old_action = actionSelect(old_state, Q, epsilon)\n",
    "            new_state, reward, done, truncated, info= env.step(old_action)\n",
    "            td_target = reward\n",
    "            \n",
    "            if not done:\n",
    "                td_target = td_target + gamma * Q[new_state].max()\n",
    "            \n",
    "            td_error = td_target - Q[old_state, old_action]\n",
    "\n",
    "            Q[old_state, old_action] = Q[old_state, old_action] + alpha * td_error\n",
    "            old_state = new_state                \n",
    "        Qe[ep, :] = Q\n",
    "        \n",
    "    \n",
    "    V = np.max(Qe, axis = 2)\n",
    "    optimal_policy = np.amax(Qe, axis = 2)\n",
    "        \n",
    "    state_value = V\n",
    "    q_value = Qe\n",
    "    return state_value, q_value, optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a22e95-4a45-42a6-a500-76c219c3aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_value_all = np.zeros((smooth_iters, noEpisodes, nStates))\n",
    "q_value_all = np.zeros((smooth_iters, noEpisodes, nStates, nActions))\n",
    "for it in trange(smooth_iters, desc='Smoothing'):\n",
    "    env.seed(it)\n",
    "    state_value, q_value, optimal_policy = q_learning(env, gamma, alpha, epsilon, noEpisodes)\n",
    "    state_value_all[it] = state_value\n",
    "    q_value_all[it] = q_value\n",
    "state_value = np.average(state_value_all, axis=0)\n",
    "q_value = np.average(q_value_all, axis=0)\n",
    "plot_state_values(state_value,  episodes=range(noEpisodes) , prefix='ql-')\n",
    "plot_action_values(q_value,  episodes=range(noEpisodes), prefix='ql-')\n",
    "plot_state_action_values(q_value,  episodes=range(noEpisodes), prefix='ql-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb1602-7a38-4dd3-b17e-02f5450237a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_optimal_policy = np.argmax(q_value[-1], axis=-1)\n",
    "visualize_policy(smooth_optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63515805-f667-4c09-a22d-ca380007e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_q_learning(env, gamma, alpha, epsilon, noEpisodes):\n",
    "    Q = np.zeros((nStates, nActions))\n",
    "    Qe = np.zeros((noEpisodes, nStates, nActions))\n",
    "    \n",
    "    Q_1 = np.zeros((nStates, nActions))\n",
    "    Qe_1 = np.zeros((noEpisodes, nStates, nActions))\n",
    "    \n",
    "    Q_2 = np.zeros((nStates, nActions))\n",
    "    Qe_2 = np.zeros((noEpisodes, nStates, nActions))\n",
    "    \n",
    "    alphas = decayAlpha(alpha, alpha_final, noEpisodes, 'linear')\n",
    "    \n",
    "    for ep in range(noEpisodes):\n",
    "        alpha = alphas[ep]\n",
    "        epsilon = decay_epsilon(epsilon, decay_rate, 'linear')\n",
    "        \n",
    "        old_state,done = env.reset()\n",
    "\n",
    "        while not done:\n",
    "            old_action = actionSelect(old_state, Q_1, epsilon)\n",
    "            new_state, reward, done,truncated,info = env.step(old_action)\n",
    "            td_target = reward\n",
    "            \n",
    "            if np.random.randint(2):\n",
    "                action_q1 = np.argmax(Q_1[new_state])\n",
    "                td_target    = reward\n",
    "                \n",
    "                if not done:\n",
    "                    td_target    = td_target + gamma * Q_2[new_state, action_q1]\n",
    "            \n",
    "                td_error     = td_target - Q_1[old_state, old_action]\n",
    "\n",
    "                Q_1[old_state, old_action] = Q_1[old_state, old_action] + alpha * td_error\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                action_q2 = np.argmax(Q_2[new_state])\n",
    "                td_target    = reward\n",
    "                \n",
    "                if not done:\n",
    "                    td_target    = td_target + gamma * Q_1[new_state, action_q2]\n",
    "            \n",
    "                td_error     = td_target - Q_2[old_state, old_action]\n",
    "\n",
    "                Q_2[old_state, old_action] = Q_2[old_state, old_action] + alpha * td_error\n",
    "            old_state = new_state                \n",
    "        Qe_1[ep, :] = Q_1\n",
    "        Qe_2[ep, :] = Q_2\n",
    "        Q           = (Q_1 + Q_2)/2\n",
    "        Qe          = (Qe_1 + Qe_2)/2\n",
    "\n",
    "    V = np.max(Qe, axis = 2)\n",
    "    optimal_policy = np.amax(Qe, axis = 2)\n",
    "        \n",
    "    state_value = V\n",
    "    q_value = Qe\n",
    "    return state_value, q_value, optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb88a91-f698-471c-81a8-760a880de46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_value_all = np.zeros((smooth_iters, noEpisodes, nStates))\n",
    "q_value_all = np.zeros((smooth_iters, noEpisodes, nStates, nActions))\n",
    "for it in trange(smooth_iters, desc='Smoothing'):\n",
    "    env.seed(it)\n",
    "    state_value, q_value, optimal_policy = double_q_learning(env, gamma, alpha, epsilon, noEpisodes)\n",
    "    state_value_all[it] = state_value\n",
    "    q_value_all[it] = q_value\n",
    "state_value = np.average(state_value_all, axis=0)\n",
    "q_value = np.average(q_value_all, axis=0)\n",
    "plot_state_values(state_value,  episodes=range(noEpisodes), prefix='dql-')\n",
    "plot_action_values(q_value,  episodes=range(noEpisodes), prefix='dql-')\n",
    "plot_state_action_values(q_value,  episodes=range(noEpisodes), prefix='dql-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ccd63e-9202-46ea-9b7e-582c4f59870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_optimal_policy = np.argmax(q_value[-1], axis=-1)\n",
    "visualize_policy(smooth_optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78817d30-56db-4cc9-b61d-4389f2172be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_success_rate(env, current_policy, goal_state, maxEpisodes = 100, maxSteps = 200):\n",
    "    success = 0\n",
    "    for _ in range(maxEpisodes):\n",
    "        env.reset()\n",
    "        for _ in range(maxSteps):\n",
    "            env.step(current_policy[env.state])\n",
    "            if env.state == 3 or env.state == 7:\n",
    "                break\n",
    "        if env.state == goal_state:\n",
    "            success += 1\n",
    "    success /= maxEpisodes\n",
    "    policy_success_rate = success * 100\n",
    "    \n",
    "    return policy_success_rate   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933026fe-aa7d-4473-90a2-73391eefc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_success_rate():\n",
    "    q_value_all_mc = np.zeros((smooth_iters, noEpisodes, nStates, nActions))\n",
    "    q_value_all_sarsa = np.zeros((smooth_iters, noEpisodes, nStates, nActions))\n",
    "    q_value_all_q = np.zeros((smooth_iters, noEpisodes, nStates, nActions))\n",
    "    q_value_all_dq = np.zeros((smooth_iters, noEpisodes, nStates, nActions))\n",
    "    \n",
    "    for it in trange(smooth_iters, desc='Smoothing'):\n",
    "        env.seed(it)\n",
    "        _, q_value_mc, _ = monte_carlo_control(env, gamma, alpha, epsilon, maxSteps, noEpisodes, firstVisit = True)\n",
    "        _, q_value_sarsa, _ = sarsa(env, gamma, alpha, epsilon, noEpisodes)\n",
    "        _, q_value_q, _ = q_learning(env, gamma, alpha, epsilon, noEpisodes)\n",
    "        _, q_value_dq, _ = double_q_learning(env, gamma, alpha, epsilon, noEpisodes)\n",
    "        \n",
    "        q_value_all_mc[it] = q_value_mc\n",
    "        q_value_all_sarsa[it] = q_value_sarsa\n",
    "        q_value_all_q[it] = q_value_q\n",
    "        q_value_all_dq[it] = q_value_dq\n",
    "    \n",
    "    q_value_mc = np.average(q_value_all_mc, axis=0)\n",
    "    q_value_sarsa = np.average(q_value_all_sarsa, axis=0)\n",
    "    q_value_q = np.average(q_value_all_q, axis=0)\n",
    "    q_value_dq = np.average(q_value_all_dq, axis=0)\n",
    "\n",
    "    optimal_policies_mc = np.argmax(q_value_mc, axis=-1)\n",
    "    optimal_policies_sarsa = np.argmax(q_value_sarsa, axis=-1)\n",
    "    optimal_policies_q = np.argmax(q_value_q, axis=-1)\n",
    "    optimal_policies_dq = np.argmax(q_value_dq, axis=-1)\n",
    "    \n",
    "    success_rates_mc = np.zeros(noEpisodes)\n",
    "    success_rates_sarsa = np.zeros(noEpisodes)\n",
    "    success_rates_q = np.zeros(noEpisodes)\n",
    "    success_rates_dq = np.zeros(noEpisodes)\n",
    "    \n",
    "    for i in trange(noEpisodes, desc='Episodes'):\n",
    "        success_rates_mc[i] = get_policy_success_rate(env, optimal_policies_mc[i], 3)\n",
    "        success_rates_sarsa[i] = get_policy_success_rate(env, optimal_policies_sarsa[i], 3)\n",
    "        success_rates_q[i] = get_policy_success_rate(env, optimal_policies_q[i], 3)\n",
    "        success_rates_dq[i] = get_policy_success_rate(env, optimal_policies_dq[i], 3)\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = (12, 8)\n",
    "    plt.cla()\n",
    "    color_scheme = {\n",
    "    'FVMCC': '#FF5733',  # A bright orange\n",
    "    'SARSA': '#33FF57',  # A vivid green\n",
    "    'QL':    '#3357FF',  # A deep blue\n",
    "    'DQL':   '#F033FF',  # A vibrant purple\n",
    "     }\n",
    "    plt.plot(np.arange(noEpisodes), success_rates_mc, label='FVMCC',  color=color_scheme['FVMCC'])\n",
    "    plt.plot(np.arange(noEpisodes), success_rates_sarsa, label='SARSA',  color=color_scheme['SARSA'])\n",
    "    plt.plot(np.arange(noEpisodes), success_rates_q, label='QL', color=color_scheme['QL'])\n",
    "    plt.plot(np.arange(noEpisodes), success_rates_dq, label='DQL', color=color_scheme['DQL'])\n",
    "    plt.legend()\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.title(\"Success Rate vs. Episodes\")\n",
    "    plt.savefig(f\"succratevseps.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_estimated_returns_and_mae():\n",
    "    optimal_values = np.asarray([0.82442985, 0.89286374, 0.95464233, 0., 0.76427487, 0.,0.68820946, 0., 0.69763948, 0.63906542, 0.60613373, 0.38186228])\n",
    "    \n",
    "    state_value_all_mc = np.zeros((smooth_iters, noEpisodes, nStates))\n",
    "    state_value_all_sarsa = np.zeros((smooth_iters, noEpisodes, nStates))\n",
    "    state_value_all_q = np.zeros((smooth_iters, noEpisodes, nStates))\n",
    "    state_value_all_dq = np.zeros((smooth_iters, noEpisodes, nStates))\n",
    "    \n",
    "    for it in trange(smooth_iters, desc='Smoothing'):\n",
    "        env.seed(it)\n",
    "        state_value_mc, _, _ = monte_carlo_control(env, gamma, alpha, epsilon, maxSteps, noEpisodes, firstVisit = True)\n",
    "        state_value_sarsa, _, _ = sarsa(env, gamma, alpha, epsilon, noEpisodes)\n",
    "        state_value_q, _, _ = q_learning(env, gamma, alpha, epsilon, noEpisodes)\n",
    "        state_value_dq, _, _ = double_q_learning(env, gamma, alpha, epsilon, noEpisodes)\n",
    "        \n",
    "        state_value_all_mc[it] = state_value_mc\n",
    "        state_value_all_sarsa[it] = state_value_sarsa\n",
    "        state_value_all_q[it] = state_value_q\n",
    "        state_value_all_dq[it] = state_value_dq\n",
    "    \n",
    "    state_value_mc = np.average(state_value_all_mc, axis=0)\n",
    "    state_value_sarsa = np.average(state_value_all_sarsa, axis=0)\n",
    "    state_value_q = np.average(state_value_all_q, axis=0)\n",
    "    state_value_dq = np.average(state_value_all_dq, axis=0)\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = (12, 8)\n",
    "    x = np.arange(noEpisodes)\n",
    "    plt.cla()\n",
    "\n",
    "    plt.plot(x, state_value_mc[:, 8], label='FVMCC', color=color_scheme['FVMCC'])\n",
    "    plt.plot(x, state_value_sarsa[:, 8], label='SARSA', color=color_scheme['SARSA'])\n",
    "    plt.plot(x, state_value_q[:, 8], label='QL', color=color_scheme['QL'])\n",
    "    plt.plot(x, state_value_dq[:, 8], label='DQL', color=color_scheme['DQL'])\n",
    "    plt.legend()\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(\"Estimated Expected Return (from the Start State) vs. Episodes\")\n",
    "    plt.savefig(f\"expectedreturnvseps.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    mae_mc = np.average(np.abs(state_value_mc - optimal_values.reshape(1, -1)), axis=-1)\n",
    "    mae_sarsa = np.average(np.abs(state_value_sarsa - optimal_values.reshape(1, -1)), axis=-1)\n",
    "    mae_q = np.average(np.abs(state_value_q - optimal_values.reshape(1, -1)), axis=-1)\n",
    "    mae_dq = np.average(np.abs(state_value_dq - optimal_values.reshape(1, -1)), axis=-1)\n",
    "\n",
    "    plt.cla()\n",
    "    plt.plot(x, mae_mc, label='FVMCC', color=color_scheme['FVMCC'])\n",
    "    plt.plot(x, mae_sarsa, label='SARSA', color=color_scheme['SARSA'])\n",
    "    plt.plot(x, mae_q, label='QL', color=color_scheme['QL'])\n",
    "    plt.plot(x, mae_dq, label='DQL', color=color_scheme['DQL'])\n",
    "    plt.legend()\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title(\"State-Value Function Estimation Error vs. Episodes\")\n",
    "    plt.savefig(f\"statevaluefvseps.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_success_rate()\n",
    "plot_estimated_returns_and_mae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb17404-23bd-4f59-a6b2-427f858bca0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
