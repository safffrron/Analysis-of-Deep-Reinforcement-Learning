{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a891074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all useful functions \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces, register, make\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5751f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliBandit(Env):\n",
    "    \n",
    "    \n",
    "    #----- 1 -----\n",
    "    #constructor for initialization and some helper functions\n",
    "    \n",
    "    \n",
    "    def __init__(self, render_mode=None, alpha = 1.0 , beta = 1.0):\n",
    "        \n",
    "        #P is basically State: Action: [ Transition Probability , Next state , Reward , isTerminated?]\n",
    "        self.P = {\n",
    "            0: {\n",
    "                0: [(1.0, 0, 0.0, True)],\n",
    "                1: [(1.0, 0, 0.0, True)]\n",
    "            },\n",
    "            1: {\n",
    "                0: [(alpha, 0, 1.0, True), (1-alpha, 2, 0.0, True)],\n",
    "                1: [(beta, 2, 1.0, True), (1-beta, 0, 0.0, True)]\n",
    "            },\n",
    "            2: {\n",
    "                0: [(1.0, 2, 0.0, True)],\n",
    "                1: [(1.0, 2, 0.0, True)]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.size = 3 # The size of the 1D grid\n",
    "#         self.window_size = 512  # The size of the PyGame window\n",
    "        \n",
    "        # We have 3 observations, corresponding to each position in the 1-D grid\n",
    "        self.observation_space = spaces.Discrete(self.size)\n",
    "\n",
    "        # We have 2 actions, corresponding to \"left\" & \"right\"\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.action_space_size = 2 \n",
    "        \n",
    "    \n",
    "    #return the locations of agent and target\n",
    "    def _get_obs(self):\n",
    "        return {   \n",
    "            \"agent\" : self._agent_location, \n",
    "            \"target\": self._target_location  \n",
    "        }\n",
    "    \n",
    "    #returns the distance between agent and target \n",
    "    def _get_info(self):\n",
    "        return {  \n",
    "            \"distance\": abs(self._agent_location - self._target_location)   \n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #----- 2 ------\n",
    "    # The reset function to initiate \n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        self._agent_location = 1             #location of agent in the begining\n",
    "        self._target_location = self.size-1  #starting location of target in this case 2 \n",
    "        self._dead_state = 0                 #dead location\n",
    "        \n",
    "        \n",
    "        observation = self._get_obs()        #getting useful information\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation,info\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #------- 3 ---------\n",
    "    # The step function \n",
    "    \n",
    "    def step(self, action):  # takes action as a parameter\n",
    "\n",
    "        # gets the current location and stores the values from P set \n",
    "        prev_location = self._agent_location                                #gets location\n",
    "        transitions = self.P[prev_location][action]                         #gets the corresponding action tuple\n",
    "        probabilities, next_states, rewards, terminals = zip(*transitions)  #stores the value for use \n",
    "        \n",
    "        # Randomly select a transition based on the probabilities\n",
    "        # gives you random state based on your probabilities \n",
    "        index = random.choices(range(len(probabilities)), weights=probabilities, k=1)[0]\n",
    "        # stores the values \n",
    "        self._agent_location, reward, terminated = next_states[index], rewards[index], terminals[index]\n",
    "        \n",
    "        truncated = False\n",
    "        observation = self._get_obs()  \n",
    "        info = self._get_info()\n",
    "\n",
    "        info[\"log\"] = {\"current_state\": prev_location, \n",
    "                       \"action\":action,  \n",
    "                        \"next_state\": self._agent_location}\n",
    "\n",
    "        # Return the required 5-tuple\n",
    "        return observation, reward, terminated, truncated, info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906cf9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the custom environment\n",
    "register(id='BBandit', entry_point=BernoulliBandit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa0a6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# a=float(input(\"Enter the value of alpha -  \"))\n",
    "# b=float(input(\"Enter the value of beta -  \"))\n",
    "\n",
    "# Create and use the environment\n",
    "environment = make('BBandit', alpha = 0.8 , beta = 0.8)\n",
    "\n",
    "reward_sum = 0\n",
    "observation = environment.reset(seed=0)\n",
    "for _ in range(10):\n",
    "    action = environment.action_space.sample()  # this is where you would insert your policy\n",
    "    observation, reward, terminated, truncated, info = environment.step(action)\n",
    "    reward_sum += reward\n",
    "    print(info['log'])\n",
    "\n",
    "    if terminated:\n",
    "        print(\"Terminated\", \"\\n\")\n",
    "\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation = environment.reset(seed=0)\n",
    "        \n",
    "        \n",
    "print(\" Average Reward over 10 episode = \" , int(reward_sum * 0.1))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab9128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Stratergy - Pure Exploitation \n",
    "\n",
    "def PureExploitation(Env , maxEpisodes):\n",
    "    \n",
    "    Q = np.zeros(Env.action_space_size)\n",
    "    N = np.zeros(Env.action_space_size)\n",
    "    e=1\n",
    "    Q_est = np.zeros((maxEpisodes,Env.action_space_size))\n",
    "    R_est = np.zeros(maxEpisodes)\n",
    "    obs = Env.reset(seed=0)\n",
    "    sum =0\n",
    "    opti=0\n",
    "    \n",
    "    while e < maxEpisodes :\n",
    "        a = Q.argmax()\n",
    "        R = Env.step(a)[1]\n",
    "        sum = sum + R\n",
    "        N[a]= N[a]+1\n",
    "        Q[a]= Q[a] + ((R - Q[a])/N[a])\n",
    "        Q_est[e]=Q\n",
    "        if max(Q) == Q[a]:\n",
    "            opti+=1\n",
    "        #change R_est according to need of question \n",
    "        R_est[e]= (opti/e)*100\n",
    "        e=e+1\n",
    "        obs = Env.reset(seed=0)\n",
    "    \n",
    "#     print(\" left-action | right-action  | reward\")\n",
    "#     for i in range(maxEpisodes):\n",
    "#         print(f\"{Q_est[i][0]:12.6f} | {Q_est[i][1]:13.6f} | {R_est[i]:6.2f}\")\n",
    "        \n",
    "    return R_est     \n",
    "\n",
    "    \n",
    "ev = make('BBandit', alpha = 0.5 , beta = 0.5)\n",
    "PureExploitation(ev , 10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration Stratergy - Pure Exploration \n",
    "\n",
    "def PureExploration(Env , maxEpisodes):\n",
    "    \n",
    "    Q = np.zeros(Env.action_space_size)\n",
    "    N = np.zeros(Env.action_space_size)\n",
    "    e=1\n",
    "    Q_est = np.zeros((maxEpisodes,Env.action_space_size))\n",
    "    R_est = np.zeros(maxEpisodes)\n",
    "    observation, info = Env.reset(seed=0)\n",
    "    sum=0\n",
    "    opti=0\n",
    "    \n",
    "    while e < maxEpisodes :\n",
    "        a = np.random.randint(0,len(Q))\n",
    "        R = Env.step(a)[1]\n",
    "        sum += R\n",
    "        N[a]= N[a]+1\n",
    "        Q[a]= Q[a] + ((R - Q[a])/N[a])\n",
    "        Q_est[e]=Q\n",
    "        if max(Q)==Q[a]:\n",
    "            opti+=1\n",
    "        R_est[e]=(opti/e)*100\n",
    "        e=e+1\n",
    "        observation, info = Env.reset(seed=0)\n",
    "    \n",
    "#     print(\" left-action | right-action  | reward\")\n",
    "#     for i in range(maxEpisodes):\n",
    "#         print(f\"{Q_est[i][0]:12.6f} | {Q_est[i][1]:13.6f} | {R_est[i]:6.2f}\")\n",
    "\n",
    "    return R_est\n",
    "    \n",
    "ev = make('BBandit', alpha = 0.5 , beta = 0.5)\n",
    "PureExploration(ev , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be70beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon - Greedy stratergy\n",
    "\n",
    "def EpsilonGreedy(Env , maxEpisodes , epsilon = 0.5):\n",
    "    \n",
    "    Q = np.zeros(Env.action_space_size)\n",
    "    N = np.zeros(Env.action_space_size)\n",
    "    e=1\n",
    "    Q_est = np.zeros((maxEpisodes,Env.action_space_size))\n",
    "    R_est = np.zeros(maxEpisodes)\n",
    "    observation, info = Env.reset(seed=0)\n",
    "    sum=0\n",
    "    opti=0\n",
    "    while e < maxEpisodes :\n",
    "        temp = np.random.randint(0,1)\n",
    "        if temp > epsilon :\n",
    "            a=Q.argmax()\n",
    "        else :\n",
    "            a=np.random.randint(0,len(Q))\n",
    "        R = Env.step(a)[1]\n",
    "        sum += R\n",
    "        N[a]= N[a]+1\n",
    "        Q[a]= Q[a] + ((R - Q[a])/N[a])\n",
    "        Q_est[e]=Q\n",
    "        if max(Q)==Q[a]:\n",
    "            opti+=1\n",
    "        R_est[e]=(opti/e)*100\n",
    "        e=e+1\n",
    "        observation, info = Env.reset(seed=0)\n",
    "    \n",
    "#     print(\" left-action | right-action  | reward\")\n",
    "#     for i in range(maxEpisodes):\n",
    "#         print(f\"{Q_est[i][0]:12.6f} | {Q_est[i][1]:13.6f} | {R_est[i]:6.2f}\")\n",
    "    \n",
    "    return R_est \n",
    "\n",
    "\n",
    "ev = make('BBandit', alpha = 0.5 , beta = 0.5)\n",
    "#ep = float(input(\"Enter the value of epsilon between 0 and 1 = \"))\n",
    "EpsilonGreedy(ev , 10 , 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decaying epsilon - Greedy stratergy\n",
    "\n",
    "def decayingEpsilonGreedy(Env , maxEpisodes , choice = 0):\n",
    "    \n",
    "    Q = np.zeros(Env.action_space_size)\n",
    "    N = np.zeros(Env.action_space_size)\n",
    "    e=1\n",
    "    sum=0\n",
    "    opti=0\n",
    "    Q_est = np.zeros((maxEpisodes,Env.action_space_size))\n",
    "    R_est = np.zeros(maxEpisodes)\n",
    "    observation, info = Env.reset(seed=0)\n",
    "    epsilon = 1 \n",
    "    while e < maxEpisodes :\n",
    "        if choice == 0:\n",
    "            #linear decay\n",
    "            epsilon = epsilon - (e/maxEpisodes)\n",
    "        else :\n",
    "            #exponential decay\n",
    "            epsilon = epsilon * math.exp(-(e/maxEpisodes))\n",
    "            \n",
    "            \n",
    "        temp = np.random.randint(0,1)\n",
    "        if temp > epsilon :\n",
    "            a=Q.argmax()\n",
    "        else :\n",
    "            a=np.random.randint(0,len(Q))\n",
    "        R = Env.step(a)[1]\n",
    "        sum += R\n",
    "        N[a]= N[a]+1\n",
    "        Q[a]= Q[a] + ((R - Q[a])/N[a])\n",
    "        Q_est[e]=Q\n",
    "        if max(Q)==Q[a]:\n",
    "            opti+=1\n",
    "        R_est[e]=(opti/e)*100\n",
    "        e=e+1\n",
    "        observation, info = Env.reset(seed=0)\n",
    "    \n",
    "#     print(\" left-action | right-action  | reward\")\n",
    "#     for i in range(maxEpisodes):\n",
    "#         print(f\"{Q_est[i][0]:12.6f} | {Q_est[i][1]:13.6f} | {R_est[i]:6.2f}\")\n",
    "    \n",
    "    return R_est\n",
    "\n",
    "\n",
    "ev = make('BBandit', alpha = 0.5 , beta = 0.5)\n",
    "#ep = float(input(\" Choose the type of decay , 0 -> linear , 1 -> exponential = \"))\n",
    "decayingEpsilonGreedy(ev , 10 , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd667769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SoftMax stratergy\n",
    "def softmax(x):\n",
    "    #Compute softmax values for each sets of scores in x.\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "def Softmax(Env , maxEpisodes , t=10):\n",
    "    \n",
    "    Q = np.zeros(Env.action_space_size)\n",
    "    N = np.zeros(Env.action_space_size)\n",
    "    e=1\n",
    "    sum=0\n",
    "    opti=0\n",
    "    Q_est = np.zeros((maxEpisodes,Env.action_space_size))\n",
    "    R_est = np.zeros(maxEpisodes)\n",
    "    observation, info = Env.reset(seed=0)\n",
    "    t_not = copy.copy(t)\n",
    "    \n",
    "    while e < maxEpisodes :\n",
    "        probs = softmax(Q/t)\n",
    "        a= random.choices(range(Env.action_space_size), weights=probs, k=1)[0]\n",
    "        R = Env.step(a)[1]\n",
    "        sum+=R\n",
    "        N[a]= N[a]+1\n",
    "        Q[a]= Q[a] + ((R - Q[a])/N[a])\n",
    "        t = t - ((e/maxEpisodes)*t_not)   #linearly decaying temperature\n",
    "        Q_est[e]=Q\n",
    "        if max(Q)==Q[a]:\n",
    "            opti+=1\n",
    "        R_est[e]=(opti/e)*100\n",
    "        e=e+1\n",
    "        observation, info = Env.reset(seed=0)\n",
    "    \n",
    "#     print(\" left-action | right-action  | reward\")\n",
    "#     for i in range(maxEpisodes):\n",
    "#         print(f\"{Q_est[i][0]:12.6f} | {Q_est[i][1]:13.6f} | {R_est[i]:6.2f}\")\n",
    "    \n",
    "    return R_est\n",
    "\n",
    "\n",
    "ev = make('BBandit', alpha = 0.5 , beta = 0.5)\n",
    "#ep = float(input(\" Choose the value of Temperature = \"))\n",
    "Softmax(ev , 10 , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb0d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCB stratergy\n",
    "\n",
    "def UCB(Env , maxEpisodes , c=10):\n",
    "    \n",
    "    Q = np.zeros(Env.action_space_size)\n",
    "    N = np.zeros(Env.action_space_size)\n",
    "    e=1\n",
    "    sum=0\n",
    "    Q_est = np.zeros((maxEpisodes,Env.action_space_size))\n",
    "    R_est = np.zeros(maxEpisodes)\n",
    "    observation, info = Env.reset(seed=0)\n",
    "    opti=0\n",
    "    \n",
    "    while e < maxEpisodes :\n",
    "        if e < len(Q) :\n",
    "            a=e\n",
    "        else :\n",
    "            U = c * math.sqrt(math.log(e,10)/N[a])\n",
    "            a= (Q+U).argmax()\n",
    "        R = Env.step(a)[1]\n",
    "        sum+=R\n",
    "        N[a]= N[a]+1\n",
    "        Q[a]= Q[a] + ((R - Q[a])/N[a])\n",
    "        if max(Q) == Q[a]:\n",
    "            opti+=1\n",
    "        Q_est[e]=Q\n",
    "        R_est[e]= (((opti)/e)*100)\n",
    "        e=e+1\n",
    "        observation, info = Env.reset(seed=0)\n",
    "    \n",
    "#     print(\" left-action | right-action  | reward\")\n",
    "#     for i in range(maxEpisodes):\n",
    "#         print(f\"{Q_est[i][0]:12.6f} | {Q_est[i][1]:13.6f} | {R_est[i]:6.2f}\")\n",
    "    \n",
    "    return R_est\n",
    "\n",
    "\n",
    "ev = make('BBandit', alpha = 0.5 , beta = 0.5)\n",
    "#ep = float(input(\" Choose the value of c = \"))\n",
    "UCB(ev , 10 , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d605824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1 . 4 \n",
    "y_axis1 = np.zeros(1000)\n",
    "y_axis2 = np.zeros(1000)\n",
    "y_axis3 = np.zeros(1000)\n",
    "y_axis4 = np.zeros(1000)\n",
    "y_axis5 = np.zeros(1000)\n",
    "y_axis6 = np.zeros(1000)\n",
    "for j in range(50):\n",
    "    a = np.random.normal(loc=0, scale=1)\n",
    "    b = np.random.normal(loc=0, scale=1)\n",
    "\n",
    "    ev = make('BBandit', alpha = a , beta = b)\n",
    "\n",
    "    y_axis1 = y_axis1 + PureExploitation(ev,1000)\n",
    "    y_axis2 = y_axis2 + PureExploration(ev,1000)\n",
    "    y_axis3 = y_axis3 + EpsilonGreedy(ev,1000)\n",
    "    y_axis4 = y_axis4 + decayingEpsilonGreedy(ev,1000)\n",
    "    y_axis5 = y_axis5 + Softmax(ev,1000)\n",
    "    y_axis6 = y_axis6 + UCB(ev,1000)\n",
    "        \n",
    "# Average the accumulated data\n",
    "y_axis1 /= 50\n",
    "y_axis2 /= 50\n",
    "y_axis3 /= 50\n",
    "y_axis4 /= 50\n",
    "y_axis5 /= 50\n",
    "y_axis6 /= 50\n",
    "   \n",
    "x_axis = np.linspace(1,1000,1000)    \n",
    "\n",
    "# Plot the data\n",
    "plt.plot(x_axis, y_axis1, label='PureExploitation')\n",
    "plt.plot(x_axis, y_axis2, label='PureExploration')\n",
    "plt.plot(x_axis, y_axis3, label='EpsilonGreedy')\n",
    "plt.plot(x_axis, y_axis4, label='DecayingEpsilonGreedy')\n",
    "plt.plot(x_axis, y_axis5, label='Softmax')\n",
    "plt.plot(x_axis, y_axis6, label='UCB')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('Optimal action vs. Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Optimal action %')\n",
    "plt.legend()\n",
    "\n",
    "plt.xscale('log')\n",
    "\n",
    "#plt.savefig(\"opti3.png\", format=\"png\", dpi=1200)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d0c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df77c45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
