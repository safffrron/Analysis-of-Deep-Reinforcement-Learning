{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c6a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all useful functions \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces, register, make\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15efc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWalk(Env):\n",
    "    \n",
    "    \n",
    "    #----- 1 -----\n",
    "    #constructor for initialization and some helper functions\n",
    "    \n",
    "    \n",
    "    def __init__(self , start=3):\n",
    "        \n",
    "        #P is basically State: Action: [ Transition Probability , Next state , Reward , isTerminated?]\n",
    "        self.P = {\n",
    "            0: {\n",
    "                0: [(1.0, 0, 0.0, True)],\n",
    "                1: [(1.0, 0, 0.0, True)]\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.5, 0, 0.0, True), (0.5, 2, 0.0, False)],\n",
    "                1: [(0.5, 2, 0.0, False), (0.5, 0, 0.0, True)]\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.5, 1, 0.0, False), (0.5, 3, 0.0, False)],\n",
    "                1: [(0.5, 3, 0.0, False), (0.5, 1, 0.0, False)]\n",
    "            },\n",
    "            3: {\n",
    "                0: [(0.5, 2, 0.0, False), (0.5, 4, 0.0, False)],\n",
    "                1: [(0.5, 4, 0.0, False), (0.5, 2, 0.0, False)]\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.5, 3, 0.0, False), (0.5, 5, 0.0, False)],\n",
    "                1: [(0.5, 5, 0.0, False), (0.5, 3, 0.0, False)]\n",
    "            },\n",
    "            5: {\n",
    "                0: [(0.5, 6, 1.0, True), (0.5, 4, 0.0, False)],\n",
    "                1: [(0.5, 4, 0.0, False),(0.5, 6, 1.0, True)]\n",
    "            },\n",
    "            6: {\n",
    "                0: [(1.0, 6, 0.0, True)],\n",
    "                1: [(1.0, 6, 0.0, True)]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.size = 7 # The size of the 1D grid\n",
    "        #self.window_size = 512  # The size of the PyGame window\n",
    "        \n",
    "        # We have 3 observations, corresponding to each position in the 1-D grid\n",
    "        self.observation_space = spaces.Discrete(self.size)\n",
    "\n",
    "        # We have 2 actions, corresponding to \"left\" & \"right\"\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.act_space_size = 2 \n",
    "        self.starting_pos = start\n",
    "        \n",
    "    \n",
    "    #return the locations of agent and target\n",
    "    def _get_obs(self):\n",
    "        return {   \n",
    "            \"agent\" : self._agent_location, \n",
    "            \"target\": self._target_location  \n",
    "        }\n",
    "    \n",
    "    #returns the distance between agent and target \n",
    "    def _get_info(self):\n",
    "        return {  \n",
    "            \"distance\": abs(self._agent_location - self._target_location)   \n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #----- 2 ------\n",
    "    # The reset function to initiate \n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        self._agent_location = self.starting_pos  #location of agent in the begining\n",
    "        self._target_location = self.size-1  #starting location of target in this case 2 \n",
    "        self._dead_state = 0                 #dead location\n",
    "        \n",
    "        \n",
    "        observation = self._get_obs()        #getting useful information\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation,info\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #------- 3 ---------\n",
    "    # The step function \n",
    "    \n",
    "    def step(self, action):  # takes action as a parameter\n",
    "\n",
    "        # gets the current location and stores the values from P set \n",
    "        prev_location = self._agent_location                                #gets location\n",
    "        transitions = self.P[prev_location][action]                         #gets the corresponding action tuple\n",
    "        probabilities, next_states, rewards, terminals = zip(*transitions)  #stores the value for use \n",
    "        \n",
    "        # Randomly select a transition based on the probabilities\n",
    "        # gives you random state based on your probabilities \n",
    "        index = random.choices(range(len(probabilities)), weights=probabilities, k=1)[0]\n",
    "        # stores the values \n",
    "        self._agent_location, reward, terminated = next_states[index], rewards[index], terminals[index]\n",
    "        \n",
    "        truncated = False\n",
    "        observation = self._get_obs()  \n",
    "        info = self._get_info()\n",
    "\n",
    "        info[\"log\"] = {\"current_state\": prev_location, \n",
    "                       \"action\":action,  \n",
    "                        \"next_state\": self._agent_location}\n",
    "\n",
    "        # Return the required 5-tuple\n",
    "        return observation, reward, terminated, truncated, info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the custom environment\n",
    "register(id='RandomWalk', entry_point=RandomWalk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and use the environment\n",
    "environment = make('RandomWalk' )\n",
    "terminated = False\n",
    "reward_sum = 0\n",
    "observation = environment.reset(seed=0)\n",
    "while not terminated :\n",
    "    action = 1  # this is where you would insert your policy\n",
    "    observation, reward, terminated, truncated, info = environment.step(action)\n",
    "    reward_sum += reward\n",
    "    print(info['log'])\n",
    "\n",
    "    if terminated:\n",
    "        print(\"Terminated\", \"\\n\")\n",
    "\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation = environment.reset(seed=0)\n",
    "        \n",
    "        \n",
    "print(\" Reward = \" , (reward_sum )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbcaa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = make('RandomWalk')\n",
    "observation = environment.reset(seed=0)\n",
    "\n",
    "\n",
    "policy=0                #only for this particular case as anyways it will go left \n",
    "\n",
    "\n",
    "def generateTrajectory( Env , policy , maxSteps) :\n",
    "    experience = []\n",
    "    terminated = False\n",
    "    steps=1\n",
    "    while not terminated :\n",
    "        \n",
    "        action = policy\n",
    "        observation, reward, terminated, truncated, info = environment.step(action)\n",
    "        \n",
    "        experience.append( (info['log']['current_state'],info['log']['action'],reward,info['log']['next_state']) )\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            observation = environment.reset(seed=0)\n",
    "            return experience\n",
    "            \n",
    "        if steps > maxSteps :\n",
    "            observation = environment.reset(seed=0)\n",
    "            return []\n",
    "        \n",
    "        steps+=1\n",
    "\n",
    "generateTrajectory(environment , policy , 100) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ac9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# decay-type : 0 -> linear decay , 1 -> exponential decay\n",
    "\n",
    "def decayAlpha(initialValue, finalValue, maxSteps, decayType) :\n",
    "    ans=[initialValue]\n",
    "    if decayType == 0 :\n",
    "        \n",
    "        steps= abs(finalValue-initialValue)/maxSteps \n",
    "        i = 1 \n",
    "        value = initialValue\n",
    "        while i <= maxSteps :\n",
    "            value = value - steps\n",
    "            ans.append(value)\n",
    "            i+=1\n",
    "        return ans\n",
    "    \n",
    "    else :\n",
    "        \n",
    "        factor =  (finalValue / initialValue) ** (1 / maxSteps) \n",
    "        i=1\n",
    "        \n",
    "        while i <= maxSteps :\n",
    "            value = initialValue * (factor ** i)\n",
    "            ans.append(value)\n",
    "            i+=1\n",
    "        \n",
    "        return ans \n",
    "        \n",
    "y_axis1 = decayAlpha(1000, 10, 10, 1)\n",
    "y_axis2 = decayAlpha(1000, 10, 10, 0) \n",
    "\n",
    "x_axis = [i for i in range(1,12)]\n",
    "\n",
    "\n",
    "plt.plot(x_axis, y_axis1 , label='exponential')\n",
    "plt.plot(x_axis, y_axis2 , label='linear'      )\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "#plt.savefig(\"qn22.png\", format=\"png\", dpi=1200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea23a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarloPrediction( Env , maxSteps , noEpisodes ,alph=0.01 ,policy=0 , gamma=0.99,   firstVisit=True):\n",
    "    # initializations\n",
    "    v = np.zeros(Env.size)\n",
    "    visited=np.zeros(Env.size)\n",
    "    v_r = np.zeros((noEpisodes,Env.size))\n",
    "    G_t = np.zeros(noEpisodes)\n",
    "    observation = Env.reset(seed=0)\n",
    "    \n",
    "    alpha= decayAlpha(alph, 0.001, noEpisodes , 0)\n",
    "    #algorithm\n",
    "    for e in range(noEpisodes):\n",
    "        observation = Env.reset(seed=0)\n",
    "        value=0\n",
    "        t = generateTrajectory( Env , policy , maxSteps)\n",
    "        \n",
    "        visited[:]=False\n",
    "        \n",
    "        for i , (s,action,reward,nex) in enumerate(t):\n",
    "            if visited[s] and firstVisit :\n",
    "                continue\n",
    "            else :\n",
    "                visited[s]=True\n",
    "            \n",
    "            G=0\n",
    "            j=i\n",
    "            while j<len(t):\n",
    "                G += ((gamma**(j-i))*(t[j][2]))\n",
    "                j+=1\n",
    "            \n",
    "            if s==5:\n",
    "                value=G\n",
    "            \n",
    "            v[s]+= alpha[e]*(G-v[s]) \n",
    "        G_t[e]=value\n",
    "        v_r[e]=v\n",
    "        \n",
    "    return v_r\n",
    "\n",
    "\n",
    "environment = make('RandomWalk' )\n",
    "observation = environment.reset(seed=0)\n",
    "MonteCarloPrediction( environment, 100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TemporalDifference( Env , noEpisodes ,alpha ,policy=0 , gamma=0.99):\n",
    "    # initializations\n",
    "    v = np.zeros(Env.size)\n",
    "    v_r = np.zeros((noEpisodes,Env.size))\n",
    "    observation = Env.reset(seed=0)\n",
    "    G_t = np.zeros(noEpisodes)\n",
    "    alpha= decayAlpha(alpha, 0.001, noEpisodes , 1)\n",
    "    \n",
    "    \n",
    "    #algorithm\n",
    "    for e in range(noEpisodes):\n",
    "        terminated = False \n",
    "        value=0\n",
    "        \n",
    "        while not terminated :\n",
    "            action = policy\n",
    "            observation, reward, terminated, truncated, info = environment.step(action)\n",
    "            td_target = reward\n",
    "            if not terminated :\n",
    "                td_target += gamma * v[info['log']['next_state']]\n",
    "                \n",
    "            td_error=td_target-v[info['log']['current_state']] \n",
    "            if info['log']['current_state'] == 5 :\n",
    "                value=td_error\n",
    "            \n",
    "            v[info['log']['current_state']]+=(alpha[e]*td_error)\n",
    "            info['log']['current_state']=info['log']['next_state']\n",
    "            \n",
    "        G_t[e]=value\n",
    "        v_r[e]=v\n",
    "        observation = Env.reset(seed=0)\n",
    "    return G_t\n",
    "\n",
    "\n",
    "environment = make('RandomWalk' , start=4)\n",
    "observation = environment.reset(seed=0)\n",
    "TemporalDifference( environment , 10 , 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f848b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2 - TD\n",
    "\n",
    "environment = make('RandomWalk' , start=2)\n",
    "observation = environment.reset(seed=0)\n",
    "arr = TemporalDifference( environment , 500 , 0.5)\n",
    "\n",
    "y_axis1=arr[:,1]\n",
    "y_axis2=arr[:,2]\n",
    "y_axis3=arr[:,3]\n",
    "y_axis4=arr[:,4]\n",
    "y_axis5=arr[:,5]\n",
    "\n",
    "x_axis = np.linspace(1,500,500)    \n",
    "\n",
    "# Plot the data\n",
    "plt.plot(x_axis, y_axis1, label='state-1')\n",
    "plt.plot(x_axis, y_axis2, label='state-2')\n",
    "plt.plot(x_axis, y_axis3, label='state-3')\n",
    "plt.plot(x_axis, y_axis4, label='state-4')\n",
    "plt.plot(x_axis, y_axis5, label='state-5')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('Value vs. Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.axhline(y = 0.16154172, color = 'blue', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.33186436, color = 'orange', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.504596, color = 'green', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.65570089, color = 'red', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.79724225, color = 'purple', linestyle = 'dashed')\n",
    "#plt.xscale('log')\n",
    "\n",
    "#plt.savefig(\"qn29.png\", format=\"png\", dpi=1200)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515dbe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2 - MC\n",
    "\n",
    "environment = make('RandomWalk' )\n",
    "observation = environment.reset(seed=0)\n",
    "arr = MonteCarloPrediction( environment, 100,500,firstVisit=True)\n",
    "\n",
    "y_axis1=arr[:,1]\n",
    "y_axis2=arr[:,2]\n",
    "y_axis3=arr[:,3]\n",
    "y_axis4=arr[:,4]\n",
    "y_axis5=arr[:,5]\n",
    "\n",
    "x_axis = np.linspace(1,500,500)    \n",
    "\n",
    "# Plot the data\n",
    "plt.plot(x_axis, y_axis1, label='state-1')\n",
    "plt.plot(x_axis, y_axis2, label='state-2')\n",
    "plt.plot(x_axis, y_axis3, label='state-3')\n",
    "plt.plot(x_axis, y_axis4, label='state-4')\n",
    "plt.plot(x_axis, y_axis5, label='state-5')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('Value vs. Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.axhline(y = 0.16154172, color = 'blue', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.33186436, color = 'orange', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.504596, color = 'green', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.65570089, color = 'red', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.79724225, color = 'purple', linestyle = 'dashed')\n",
    "#plt.xscale('log')\n",
    "\n",
    "#plt.savefig(\"qn281.png\", format=\"png\", dpi=1200)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a3685",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = make('RandomWalk' , start=2)\n",
    "observation = environment.reset(seed=0)\n",
    "arr = TemporalDifference( environment , 500 , 0.5)\n",
    "\n",
    "y_axis=arr\n",
    "\n",
    "\n",
    "x_axis = np.linspace(1,500,500)    \n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(x_axis, y_axis, label='state')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('TD error vs. Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('TD error')\n",
    "\n",
    "#plt.xscale('log')\n",
    "\n",
    "#plt.savefig(\"qn2155.png\", format=\"png\", dpi=1200)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59af560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = make('RandomWalk' )\n",
    "observation = environment.reset(seed=0)\n",
    "arr = MonteCarloPrediction( environment, 100,500,firstVisit=False)\n",
    "\n",
    "y_axis=arr\n",
    "\n",
    "\n",
    "x_axis = np.linspace(1,500,500)    \n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(x_axis, y_axis, label='state')\n",
    "\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('MCEV Target vs. Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('MCEV')\n",
    "\n",
    "\n",
    "#plt.xscale('log')\n",
    "\n",
    "#plt.savefig(\"qn214.png\", format=\"png\", dpi=1200)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555d1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2 - TD\n",
    "\n",
    "\n",
    "arr = np.zeros((500,7))\n",
    "              \n",
    "for i in range(20): \n",
    "    environment = make('RandomWalk' , start=2)\n",
    "    observation = environment.reset(seed=i)\n",
    "    arr = arr + TemporalDifference( environment , 500 , 0.5)\n",
    "arr = arr/20\n",
    "y_axis1=arr[:,1]\n",
    "y_axis2=arr[:,2]\n",
    "y_axis3=arr[:,3]\n",
    "y_axis4=arr[:,4]\n",
    "y_axis5=arr[:,5]\n",
    "\n",
    "x_axis = np.linspace(1,500,500)    \n",
    "\n",
    "# Plot the data\n",
    "plt.plot(x_axis, y_axis1, label='state-1')\n",
    "plt.plot(x_axis, y_axis2, label='state-2')\n",
    "plt.plot(x_axis, y_axis3, label='state-3')\n",
    "plt.plot(x_axis, y_axis4, label='state-4')\n",
    "plt.plot(x_axis, y_axis5, label='state-5')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('Value vs. Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.axhline(y = 0.16154172, color = 'blue', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.33186436, color = 'orange', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.504596, color = 'green', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.65570089, color = 'red', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.79724225, color = 'purple', linestyle = 'dashed')\n",
    "#plt.xscale('log')\n",
    "\n",
    "plt.savefig(\"qn283.png\", format=\"png\", dpi=1200)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.zeros((500,7))\n",
    "              \n",
    "for i in range(20): \n",
    "    environment = make('RandomWalk' )\n",
    "    observation = environment.reset(seed=i)\n",
    "    arr += MonteCarloPrediction( environment, 100,500,firstVisit=True)\n",
    "arr = arr/20\n",
    "\n",
    "y_axis1=arr[:,1]\n",
    "y_axis2=arr[:,2]\n",
    "y_axis3=arr[:,3]\n",
    "y_axis4=arr[:,4]\n",
    "y_axis5=arr[:,5]\n",
    "\n",
    "x_axis = np.linspace(1,500,500)    \n",
    "\n",
    "# Plot the data\n",
    "plt.plot(x_axis, y_axis1, label='state-1')\n",
    "plt.plot(x_axis, y_axis2, label='state-2')\n",
    "plt.plot(x_axis, y_axis3, label='state-3')\n",
    "plt.plot(x_axis, y_axis4, label='state-4')\n",
    "plt.plot(x_axis, y_axis5, label='state-5')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('Value vs. Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.axhline(y = 0.16154172, color = 'blue', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.33186436, color = 'orange', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.504596, color = 'green', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.65570089, color = 'red', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.79724225, color = 'purple', linestyle = 'dashed')\n",
    "#plt.xscale('log')\n",
    "\n",
    "#plt.savefig(\"qn281.png\", format=\"png\", dpi=1200)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f54c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8570306d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
