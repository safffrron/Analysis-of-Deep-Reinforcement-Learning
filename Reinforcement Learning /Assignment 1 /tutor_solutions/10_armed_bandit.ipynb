{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using our Enviroment\r\n",
    "\r\n",
    "Before using our custom enviroment we must use pip to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user --upgrade --editable Assignment-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's better to restart the kernel once the pip is done installing.\r\n",
    "\r\n",
    "Initializing all the required parameters in a dictionary named params\r\n",
    "\r\n",
    "Set global seed for both random and numpy.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "random.seed(2023)\n",
    "np.random.seed(2023)\n",
    "params={}\n",
    "params[\"sigma\"]=1\n",
    "params[\"mu\"]=0\n",
    "params[\"arms\"]=10\n",
    "params[\"maxEpisodes\"]=1000\n",
    "params[\"epsilon\"]=0.1\n",
    "params[\"initial_epsilon\"]=1.0\n",
    "params[\"final_epsilon\"]={}\n",
    "params[\"decay_rate\"]={}\n",
    "params[\"final_epsilon\"]['lin']=0.0\n",
    "params[\"final_epsilon\"]['exp']=0.0001\n",
    "params[\"decay_rate\"]['lin']=(params[\"initial_epsilon\"]-params[\"final_epsilon\"]['lin'])/params[\"maxEpisodes\"]\n",
    "params[\"decay_rate\"]['exp']=np.log(params[\"initial_epsilon\"]/params[\"final_epsilon\"]['exp'])/params[\"maxEpisodes\"]\n",
    "params[\"c_UCB\"]=0.1\n",
    "params[\"init_temp\"]=10000000\n",
    "params[\"final_temp\"]=0.05\n",
    "params[\"temp_decay_rate_lin\"]=(params[\"init_temp\"]-params[\"final_temp\"])/params[\"maxEpisodes\"]\n",
    "params[\"temp_decay_rate_exp\"]=np.log(params[\"init_temp\"]/params[\"final_temp\"])/params[\"maxEpisodes\"]\n",
    "params[\"no_envs\"]=50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now simply Import gym, create our environment and run a few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2023)\n",
    "np.random.seed(2023)\n",
    "env=gym.make('Assignment_1:tenArmedGaussianBandit-v0',mu=params[\"mu\"],sigma=params[\"sigma\"],arms=params[\"arms\"])\n",
    "#env=gym.make('Assignment_1:tenArmedGaussianBandit-v0',mu=0.0,sigma=1.0,arms=params[\"arms\"])\n",
    "episodes = 10\n",
    "for episode in range(1,episodes+1):\n",
    "    state=env.reset(seed=2023)\n",
    "    done= False\n",
    "    action =random.choice(list(range(0,env.action_space.n)))\n",
    "    end_state, reward, done, truncated, info =env.step(action)\n",
    "    \n",
    "    print('Episode:{} End State: {} Reward:{} Action:{}'.format(episode,end_state,reward,action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring Initial helper function Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softMax(x):\r\n",
    "    return np.exp(x)/(np.sum(np.exp(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure Exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PureExploitation(env,params):\n",
    "    # print(\"PureExploitation\")\n",
    "    Q = np.zeros(env.action_space.n)\n",
    "    N = np.zeros(env.action_space.n)\n",
    "    e = 0\n",
    "    Q_est = np.zeros((params[\"maxEpisodes\"],env.action_space.n))\n",
    "    R=np.zeros((params[\"maxEpisodes\"]))\n",
    "    actions=np.zeros((params[\"maxEpisodes\"]))\n",
    "    env.reset(seed=2023)\n",
    "    while e < params[\"maxEpisodes\"]-1 :\n",
    "        max_indices=np.where(Q==np.amax(Q))\n",
    "        action = random.choice(max_indices[0])\n",
    "        # print(action)\n",
    "        # action = np.argmax(Q)\n",
    "        end_state, reward, done, truncated, info = env.step(action)\n",
    "        N[action] = N[action] + 1\n",
    "        Q[action] = Q[action] + (reward-Q[action])/N[action]\n",
    "        e = e+1\n",
    "        Q_est[e] = Q\n",
    "        R[e]=reward\n",
    "        actions[e]=action\n",
    "        env.reset(seed=2023)\n",
    "    return Q_est,R,actions\n",
    "# print(PureExploitation(env,params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PureExploration(env,params):\n",
    "    # print(\"PureExploration\")\n",
    "    Q = np.zeros(env.action_space.n)\n",
    "    N = np.zeros(env.action_space.n)\n",
    "    e = 0\n",
    "    Q_est = np.zeros((params[\"maxEpisodes\"],env.action_space.n))\n",
    "    R=np.zeros((params[\"maxEpisodes\"]))\n",
    "    actions=np.zeros((params[\"maxEpisodes\"]))\n",
    "    env.reset(seed=2023)\n",
    "    while e < params[\"maxEpisodes\"]-1 :\n",
    "        action= random.choice((0,len(Q)-1))\n",
    "        # print(action)\n",
    "        end_state, reward, done,truncated,info = env.step(action)\n",
    "        N[action] = N[action] + 1\n",
    "        Q[action] = Q[action] + (reward-Q[action])/N[action]\n",
    "        e = e+1\n",
    "        Q_est[e] = Q\n",
    "        R[e]=reward\n",
    "        actions[e]=action\n",
    "        env.reset(seed=2023)\n",
    "    return Q_est,R,actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon Greedy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilonGreedy(env,params):\n",
    "    # print(\"epsilonGreedy\")\n",
    "    Q = np.zeros(env.action_space.n)\n",
    "    N = np.zeros(env.action_space.n)\n",
    "    e = 0\n",
    "    Q_est = np.zeros((params[\"maxEpisodes\"],env.action_space.n))\n",
    "    R=np.zeros((params[\"maxEpisodes\"]))\n",
    "    actions=np.zeros((params[\"maxEpisodes\"]))\n",
    "    env.reset(seed=2023)\n",
    "    while e < params[\"maxEpisodes\"]-1 :\n",
    "        if random.random() > params[\"epsilon\"]:\n",
    "            max_indices=np.where(Q==np.amax(Q))\n",
    "            action = random.choice(max_indices[0])\n",
    "        else :\n",
    "            action= random.choice((0,len(Q)-1))\n",
    "        # print(action)\n",
    "        end_state, reward, done,truncated,info = env.step(action)\n",
    "        N[action] = N[action] + 1\n",
    "        Q[action] = Q[action] + (reward-Q[action])/N[action]\n",
    "        e = e+1\n",
    "        Q_est[e] = Q\n",
    "        R[e]=reward\n",
    "        actions[e]=action\n",
    "        env.reset(seed=2023)\n",
    "    return Q_est,R,actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decaying Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decayingEpsilonGreedy(env,params,type):\n",
    "    # print(\"decayingEpsilonGreedy\")\n",
    "    Q = np.zeros(env.action_space.n)\n",
    "    N = np.zeros(env.action_space.n)\n",
    "    e = 0\n",
    "    Q_est = np.zeros((params[\"maxEpisodes\"],env.action_space.n))\n",
    "    R=np.zeros((params[\"maxEpisodes\"]))\n",
    "    actions=np.zeros((params[\"maxEpisodes\"]))\n",
    "    env.reset(seed=2023)\n",
    "    epsilon=params[\"initial_epsilon\"]\n",
    "    while e < params[\"maxEpisodes\"]-1 :\n",
    "        if random.random() > epsilon:\n",
    "            max_indices=np.where(Q==np.amax(Q))\n",
    "            action = random.choice(max_indices[0])\n",
    "        else :\n",
    "            action= random.choice((0,len(Q)-1))\n",
    "        # print(action)\n",
    "        end_state, reward, done, truncated, info = env.step(action)\n",
    "        N[action] = N[action] + 1\n",
    "        Q[action] = Q[action] + (reward-Q[action])/N[action]\n",
    "        e = e+1\n",
    "        Q_est[e] = Q\n",
    "        \n",
    "        if type=='lin':\n",
    "            epsilon=epsilon-params[\"decay_rate\"]['lin']\n",
    "        else :\n",
    "            epsilon = epsilon*np.exp(-params[\"decay_rate\"]['exp'])\n",
    "            \n",
    "        R[e]=reward\n",
    "        actions[e]=action\n",
    "        env.reset(seed=2023)\n",
    "    return Q_est,R,actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SoftMax Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softMaxExploration(env,params,type):\n",
    "    # print(\"softMaxExploration\")\n",
    "    Q = np.zeros(env.action_space.n)\n",
    "    N = np.zeros(env.action_space.n)\n",
    "    e = 0\n",
    "    Q_est = np.zeros((params[\"maxEpisodes\"],env.action_space.n))\n",
    "    R=np.zeros((params[\"maxEpisodes\"]))\n",
    "    actions=np.zeros((params[\"maxEpisodes\"]))\n",
    "    env.reset(seed=2023)\n",
    "    temp=params[\"init_temp\"]\n",
    "    while e < params[\"maxEpisodes\"]-1 :\n",
    "        probs = softMax(Q/temp)\n",
    "        action=random.choices(list(range(0,env.action_space.n)),weights=probs,k=1)[0]\n",
    "        # print(action)\n",
    "        end_state, reward, done, truncated, info = env.step(action)\n",
    "        N[action] = N[action] + 1\n",
    "        Q[action] = Q[action] + (reward-Q[action])/N[action]\n",
    "        # have to decay temperature\n",
    "        e = e+1\n",
    "        Q_est[e] = Q\n",
    "        if type=='lin':\n",
    "            temp=temp-params[\"temp_decay_rate_lin\"]\n",
    "        else :\n",
    "            temp = temp*np.exp(-params[\"temp_decay_rate_exp\"])\n",
    "        R[e]=reward\n",
    "        actions[e]=action\n",
    "        env.reset(seed=2023)\n",
    "    return Q_est,R,actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upper Confidence Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCBexploration(env,params):\n",
    "    # print(\"UCBexploration\")\n",
    "    Q = np.zeros(env.action_space.n)\n",
    "    N = np.zeros(env.action_space.n)\n",
    "\n",
    "    e = 0\n",
    "    Q_est = np.zeros((params[\"maxEpisodes\"],env.action_space.n))\n",
    "    R=np.zeros((params[\"maxEpisodes\"]))\n",
    "    actions=np.zeros((params[\"maxEpisodes\"]))\n",
    "    env.reset(seed=2023)\n",
    "    while e < params[\"maxEpisodes\"] - 1:\n",
    "        if e< len(Q):\n",
    "            action = e\n",
    "        else:\n",
    "            U= params[\"c_UCB\"]* math.sqrt(math.log(e))/np.sqrt(N)\n",
    "            UCB = np.add(Q,U)\n",
    "            max_indices=np.where(UCB==np.amax(UCB))\n",
    "            action = random.choice(max_indices[0])\n",
    "        end_state, reward, done, truncated, info = env.step(action)\n",
    "        N[action] = N[action] + 1\n",
    "        Q[action] = Q[action] + (reward-Q[action])/N[action]\n",
    "        e = e+1\n",
    "        Q_est[e] = Q\n",
    "        R[e] =reward\n",
    "        actions[e]=action\n",
    "        env.reset(seed=2023)\n",
    "    return Q_est,R,actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 50 different probability distribution for creating 50 different Environments. Also simultaneously creating V* and Q* tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2023)\n",
    "np.random.seed(2023)\n",
    "envs=[]\n",
    "q_star=[]\n",
    "v_star=[]\n",
    "for i in range(50):\n",
    "        # envs is a list of environments\n",
    "    env=gym.make('Assignment_1:tenArmedGaussianBandit-v0',mu=params[\"mu\"],sigma=params[\"sigma\"],arms=params[\"arms\"])\n",
    "    envs.append(env)\n",
    "    q_star.append(envs[i].mean_rewards)\n",
    "    v_star.append(np.amax(q_star[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Reward Plot for different Agents with timesteps, Initializing decay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_type=\"exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2023)\n",
    "np.random.seed(2023)\n",
    "Rewards1=[]\n",
    "for i in range(params[\"no_envs\"]):\n",
    "        \n",
    "    Q_est,R,actions =PureExploitation(envs[i],params)\n",
    "    # print(np.shape(R))\n",
    "    Rewards1.append(R)\n",
    "    # print(i)\n",
    "Rewards1=np.transpose(Rewards1)    \n",
    "# print(Rewards[23])\n",
    "avg_rewards1=[]\n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    avg_rewards1.append(np.sum(Rewards1[i])/params[\"no_envs\"])\n",
    "# print(np.shape(avg_rewards))\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),avg_rewards1, color='r')\n",
    "# plt.ylabel('Average Rewards per timesteps')  \n",
    "# plt.xlabel('Timesteps') \n",
    "# plt.title('PureExploitation:Average rewards recieved vs timestep 10armedBandit') \n",
    "# plt.savefig(' PureExploitation-10armedBandit Average rewards recieved vs timestep .pdf') \n",
    "# # plt.close()\n",
    "# # print(np.shape(Rewards1))\n",
    "\n",
    "Rewards2=[]\n",
    "for i in range(params[\"no_envs\"]):\n",
    "        \n",
    "    Q_est,R ,actions=PureExploration(envs[i],params)\n",
    "\n",
    "    Rewards2.append(R)\n",
    "    # print(i)\n",
    "Rewards2=np.transpose(Rewards2)    \n",
    "    \n",
    "avg_rewards2=[]\n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    avg_rewards2.append(np.sum(Rewards2[i])/params[\"no_envs\"])\n",
    "# print(np.shape(avg_rewards))\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),avg_rewards2, color='blue')\n",
    "# plt.ylabel('Average Rewards per timesteps')  \n",
    "# plt.xlabel('Timesteps')  \n",
    "# plt.title('PureExploration:Average rewards recieved vs timestep 10armedBandit') \n",
    "# plt.savefig(' PureExploration-10armedBandit Average rewards recieved vs timestep .pdf') \n",
    "# # plt.close()\n",
    "\n",
    "Rewards3=[]\n",
    "for i in range(params[\"no_envs\"]):\n",
    "    \n",
    "    Q_est,R ,actions=epsilonGreedy(envs[i],params)\n",
    "    # print(np.shape(R))\n",
    "    Rewards3.append(R)\n",
    "    # print(i)\n",
    "Rewards3=np.transpose(Rewards3)    \n",
    "# print(Rewards[23])\n",
    "avg_rewards3=[]\n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    avg_rewards3.append(np.sum(Rewards3[i])/params[\"no_envs\"])\n",
    "# print(np.shape(avg_rewards))\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),avg_rewards3, color='green')\n",
    "# plt.ylabel('Average Rewards per timesteps')  \n",
    "# plt.xlabel('Timesteps')  \n",
    "# plt.title('epsilonGreedy:Average rewards recieved vs timestep 10armedBandit') \n",
    "# plt.savefig(' epsilonGreedy- 10armedBandit Average rewards recieved vs timestep.pdf') \n",
    "# # plt.close()\n",
    "\n",
    "Rewards4=[]\n",
    "for i in range(params[\"no_envs\"]):\n",
    "    \n",
    "    Q_est,R ,actions=decayingEpsilonGreedy(envs[i],params,decay_type)\n",
    "    # print(np.shape(R))\n",
    "    Rewards4.append(R)\n",
    "    # print(i)\n",
    "Rewards4=np.transpose(Rewards4)    \n",
    "# print(Rewards[23])\n",
    "avg_rewards4=[]\n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    avg_rewards4.append(np.sum(Rewards4[i])/params[\"no_envs\"])\n",
    "# print(np.shape(avg_rewards))\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),avg_rewards4, color='black')\n",
    "#plt.ylabel('Average Rewards per timesteps')  \n",
    "#plt.xlabel('Timesteps')  \n",
    "#plt.title('decayingEpsilonGreedy:Average rewards recieved vs timestep 10armedBandit') \n",
    "#plt.savefig(' decayingEpsilonGreedy-10armedBandit Average rewards recieved vs timestep .pdf') \n",
    "# # plt.close()\n",
    "\n",
    "Rewards5=[]\n",
    "for i in range(params[\"no_envs\"]):\n",
    "    \n",
    "    Q_est,R ,actions=softMaxExploration(envs[i],params,decay_type)\n",
    "    # print(np.shape(R))\n",
    "    Rewards5.append(R)\n",
    "    # print(i)\n",
    "Rewards5=np.transpose(Rewards5)    \n",
    "# print(Rewards[23])\n",
    "avg_rewards5=[]\n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    avg_rewards5.append(np.sum(Rewards5[i])/params[\"no_envs\"])\n",
    "# print(np.shape(avg_rewards))\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),avg_rewards5,color='pink')\n",
    "# plt.ylabel('Average Rewards per timesteps')  \n",
    "# plt.xlabel('Timesteps')  \n",
    "# plt.title('softMaxExploration:Average rewards recieved vs timestep 10armedBandit') \n",
    "# plt.savefig('softMaxExploration-10armedBandit Average rewards recieved vs timestep .pdf') \n",
    "# # plt.close()\n",
    "\n",
    "Rewards6=[]\n",
    "for i in range(params[\"no_envs\"]):\n",
    "    \n",
    "    Q_est,R,actions =UCBexploration(envs[i],params)\n",
    "    # print(np.shape(R))\n",
    "    Rewards6.append(R)\n",
    "    # print(i)\n",
    "Rewards6=np.transpose(Rewards6)    \n",
    "# print(Rewards[23])\n",
    "avg_rewards6=[]\n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    avg_rewards6.append(np.sum(Rewards6[i])/params[\"no_envs\"])\n",
    "# print(np.shape(avg_rewards))\n",
    "#plt.legend(['Pure Exploitation', 'Pure Exploration', 'Epsilon Greedy', 'Decaying Epsilon Greedy', 'SoftMax Exploration', 'UCB Exploration'], loc='best')\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),avg_rewards6, color='orange')\n",
    "# plt.ylabel('Average Rewards per timesteps')  \n",
    "# plt.xlabel('Timesteps')  \n",
    "# plt.title('UCBexploration:Average rewards recieved vs timestep 10armedBandit') \n",
    "# plt.savefig(' UCBexploration-10armedBandit Average rewards recieved vs timestep .pdf') \n",
    "plt.title(\"Average Rewards of Different Agents: 10 armed\")\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel(\"Average Rewards\")\n",
    "plt.savefig(\"Average Rewards of Different Agents-10 armed bandit.pdf\")\n",
    "# plt.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Regret for different Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2023)\n",
    "np.random.seed(2023)\n",
    "Actions1=np.zeros((params[\"no_envs\"],params[\"maxEpisodes\"]))\n",
    "Q_est1=np.empty((params[\"no_envs\"],envs[0].action_space.n))\n",
    "Regret1=np.zeros(params[\"maxEpisodes\"])\n",
    "for i in range(params[\"no_envs\"]):\n",
    "    \n",
    "    Q_est,R,actions=PureExploitation(envs[i],params)\n",
    "    # print(actions)\n",
    "    # print(np.shape(actions))\n",
    "    Actions1[i]=actions\n",
    "\n",
    "Actions1=np.transpose(Actions1)\n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    if i==0:\n",
    "        sum = 0\n",
    "        for j in range(params[\"no_envs\"]):\n",
    "            sum+=(v_star[j]-q_star[j][int(Actions1[i][j])])\n",
    "            # print(sum)\n",
    "        # print(np.shape(sum))\n",
    "        Regret1[i]=sum/params[\"no_envs\"]\n",
    "        # Regret1=np.average(np.amax(Q_est1,axis=1)-Q_est[Actions1[i]])\n",
    "    else:\n",
    "        sum =0\n",
    "        for j in range(params[\"no_envs\"]):\n",
    "            sum+=(v_star[j]-q_star[j][int(Actions1[i][j])])\n",
    "        Regret1[i]=Regret1[i-1]+(sum/params[\"no_envs\"])\n",
    "# print(Regret1)\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),Regret1, color='r')\n",
    "# plt.ylabel('Average Regrets per timesteps')  \n",
    "# plt.xlabel('Timesteps') \n",
    "# plt.title('PureExploitation:Average regrets recieved vs timestep 2armedBandit') \n",
    "# plt.savefig(' PureExploitation-2armedBandit Average regrets recieved vs timestep .pdf') \n",
    "# plt.close()\n",
    "\n",
    "Actions2=np.zeros((params[\"no_envs\"],params[\"maxEpisodes\"]))\n",
    "Q_est2=np.empty((params[\"no_envs\"],envs[0].action_space.n))\n",
    "Regret2=np.zeros(params[\"maxEpisodes\"])\n",
    "for i in range(params[\"no_envs\"]):\n",
    "    \n",
    "    Q_est,R,actions=PureExploration(envs[i],params)\n",
    "    # print(actions)\n",
    "    # print(np.shape(actions))\n",
    "    Actions2[i]=actions\n",
    "    # print(Q_est[-1])\n",
    "    # Q_est2[i]=Q_est[-1]\n",
    "# print(np.shape(Actions1))\n",
    "# print(np.shape(Q_est1))\n",
    "Actions2=np.transpose(Actions2)\n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    if i==0:\n",
    "        sum = 0\n",
    "        for j in range(params[\"no_envs\"]):\n",
    "            sum+=(v_star[j]-q_star[j][int(Actions2[i][j])])\n",
    "            # print(sum)\n",
    "        # print(np.shape(sum))\n",
    "        Regret2[i]=sum/params[\"no_envs\"]\n",
    "        # Regret1=np.average(np.amax(Q_est1,axis=1)-Q_est[Actions1[i]])\n",
    "    else:\n",
    "        sum =0\n",
    "        for j in range(params[\"no_envs\"]):\n",
    "            sum+=(v_star[j]-q_star[j][int(Actions2[i][j])])\n",
    "        Regret2[i]=Regret2[i-1]+(sum/params[\"no_envs\"])\n",
    "# print(Regret1)\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),Regret2, color='blue')\n",
    "# plt.ylabel('Average Regrets per timesteps')  \n",
    "# plt.xlabel('Timesteps') \n",
    "# plt.title('PureExploration:Average regrets recieved vs timestep 2armedBandit') \n",
    "# plt.savefig(' PureExploration-2armedBandit Average regrets recieved vs timestep .pdf') \n",
    "# plt.close()\n",
    "\n",
    "Actions1=np.zeros((params[\"no_envs\"],params[\"maxEpisodes\"]))\n",
    "Q_est1=np.empty((params[\"no_envs\"],envs[0].action_space.n))\n",
    "Regret1=np.zeros(params[\"maxEpisodes\"])\n",
    "for i in range(params[\"no_envs\"]):\n",
    "    \n",
    "    Q_est,R,actions=epsilonGreedy(envs[i],params)\n",
    "    # print(actions)\n",
    "    # print(np.shape(actions))\n",
    "    Actions1[i]=actions\n",
    "    # print(Q_est[-1])\n",
    "    # Q_est1[i]=Q_est[-1]\n",
    "# print(np.shape(Actions1))\n",
    "# print(np.shape(Q_est1))\n",
    "Actions1=np.transpose(Actions1)\n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    if i==0:\n",
    "        sum = 0\n",
    "        for j in range(params[\"no_envs\"]):\n",
    "            sum+=(v_star[j]-q_star[j][int(Actions1[i][j])])\n",
    "            # print(sum)\n",
    "        # print(np.shape(sum))\n",
    "        Regret1[i]=sum/params[\"no_envs\"]\n",
    "        # Regret1=np.average(np.amax(Q_est1,axis=1)-Q_est[Actions1[i]])\n",
    "    else:\n",
    "        sum =0\n",
    "        for j in range(params[\"no_envs\"]):\n",
    "            sum+=(v_star[j]-q_star[j][int(Actions1[i][j])])\n",
    "        Regret1[i]=Regret1[i-1]+sum/params[\"no_envs\"]\n",
    "# print(Regret1)\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),Regret1, color='green')\n",
    "# plt.ylabel('Average Regrets per timesteps')  \n",
    "# plt.xlabel('Timesteps') \n",
    "# plt.title('epsilonGreedy:Average regrets recieved vs timestep 2armedBandit') \n",
    "# plt.savefig(' epsilonGreedy-2armedBandit Average regrets recieved vs timestep .pdf') \n",
    "# plt.close()\n",
    "\n",
    "Actions1=np.zeros((params[\"no_envs\"],params[\"maxEpisodes\"]))\n",
    "Q_est1=np.empty((params[\"no_envs\"],envs[0].action_space.n))\n",
    "Regret1=np.zeros(params[\"maxEpisodes\"])\n",
    "for i in range(params[\"no_envs\"]):\n",
    "        \n",
    "    Q_est,R,actions=decayingEpsilonGreedy(envs[i],params,decay_type)\n",
    "    # print(actions)\n",
    "    # print(np.shape(actions))\n",
    "    Actions1[i]=actions\n",
    "    # print(Q_est[-1])\n",
    "    # Q_est1[i]=Q_est[-1]\n",
    "# print(np.shape(Actions1))\n",
    "# print(np.shape(Q_est1))\n",
    "Actions1=np.transpose(Actions1)\n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    if i==0:\n",
    "        sum = 0\n",
    "        for j in range(params[\"no_envs\"]):\n",
    "            sum+=(v_star[j]-q_star[j][int(Actions1[i][j])])\n",
    "            # print(sum)\n",
    "        # print(np.shape(sum))\n",
    "        Regret1[i]=sum/params[\"no_envs\"]\n",
    "        # Regret1=np.average(np.amax(Q_est1,axis=1)-Q_est[Actions1[i]])\n",
    "    else:\n",
    "        sum =0\n",
    "        for j in range(params[\"no_envs\"]):\n",
    "            sum+=(v_star[j]-q_star[j][int(Actions1[i][j])])\n",
    "        Regret1[i]=Regret1[i-1]+sum/params[\"no_envs\"]\n",
    "    # print(Regret1)\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),Regret1, color='black')\n",
    "# plt.ylabel('Average Regrets per timesteps')  \n",
    "# plt.xlabel('Timesteps') \n",
    "# plt.title('decayingEpsilonGreedy:Average regrets recieved vs timestep 2armedBandit') \n",
    "# plt.savefig(' decayingEpsilonGreedy-2armedBandit Average regrets recieved vs timestep .pdf') \n",
    "# plt.close()\n",
    "\n",
    "Actions1=np.zeros((params[\"no_envs\"],params[\"maxEpisodes\"]))\n",
    "Q_est1=np.empty((params[\"no_envs\"],envs[0].action_space.n))\n",
    "Regret1=np.zeros(params[\"maxEpisodes\"])\n",
    "for i in range(params[\"no_envs\"]):\n",
    "\n",
    "    Q_est,R,actions=softMaxExploration(envs[i],params,decay_type)\n",
    "    # print(actions)\n",
    "    # print(np.shape(actions))\n",
    "    Actions1[i]=actions\n",
    "    # print(Q_est[-1])\n",
    "    # Q_est1[i]=Q_est[-1]\n",
    "# print(np.shape(Actions1))\n",
    "# print(np.shape(Q_est1))\n",
    "Actions1=np.transpose(Actions1)\n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    if i==0:\n",
    "        sum = 0\n",
    "        for j in range(params[\"no_envs\"]):\n",
    "            sum+=(v_star[j]-q_star[j][int(Actions1[i][j])])\n",
    "            # print(sum)\n",
    "        # print(np.shape(sum))\n",
    "        Regret1[i]=sum/params[\"no_envs\"]\n",
    "    # Regret1=np.average(np.amax(Q_est1,axis=1)-Q_est[Actions1[i]])\n",
    "    else:\n",
    "        sum =0\n",
    "        for j in range(params[\"no_envs\"]):\n",
    "            sum+=(v_star[j]-q_star[j][int(Actions1[i][j])])\n",
    "        Regret1[i]=Regret1[i-1]+sum/params[\"no_envs\"]\n",
    "# # print(Regret1)\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),Regret1, color='pink')\n",
    "# plt.ylabel('Average Regrets per timesteps')  \n",
    "# plt.xlabel('Timesteps') \n",
    "# plt.title('softMaxExploration:Average regrets recieved vs timestep 2armedBandit') \n",
    "# plt.savefig(' softMaxExploration-2armedBandit Average regrets recieved vs timestep .pdf') \n",
    "# plt.close()\n",
    "\n",
    "Actions1=np.zeros((params[\"no_envs\"],params[\"maxEpisodes\"]))\n",
    "Q_est1=np.empty((params[\"no_envs\"],envs[0].action_space.n))\n",
    "Regret1=np.zeros(params[\"maxEpisodes\"])\n",
    "for i in range(params[\"no_envs\"]):\n",
    "    \n",
    "    Q_est,R,actions=UCBexploration(envs[i],params)\n",
    "    # print(actions)\n",
    "    # print(np.shape(actions))\n",
    "    Actions1[i]=actions\n",
    "    # print(Q_est[-1])\n",
    "    # Q_est1[i]=Q_est[-1]\n",
    "# print(np.shape(Actions1))\n",
    "# print(np.shape(Q_est1))\n",
    "Actions1=np.transpose(Actions1)\n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    if i==0:\n",
    "        sum = 0\n",
    "        for j in range(params[\"no_envs\"]):\n",
    "            sum+=(v_star[j]-q_star[j][int(Actions1[i][j])])\n",
    "            # print(sum)\n",
    "        # print(np.shape(sum))\n",
    "        Regret1[i]=sum/params[\"no_envs\"]\n",
    "            # Regret1=np.average(np.amax(Q_est1,axis=1)-Q_est[Actions1[i]])\n",
    "    else:\n",
    "        sum =0\n",
    "        for j in range(params[\"no_envs\"]):\n",
    "                sum+=(v_star[j]-q_star[j][int(Actions1[i][j])])\n",
    "        Regret1[i]=Regret1[i-1]+sum/params[\"no_envs\"]\n",
    "# print(Regret1)\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),Regret1, color='orange')\n",
    "plt.ylabel('Average Regrets per timesteps')  \n",
    "plt.xlabel('Timesteps') \n",
    "# plt.title('UCBexploration:Average regrets recieved vs timestep 2armedBandit') \n",
    "plt.legend([\"Pure Exploitation\",\"Pure Exploration\",\"Epsilon-Greedy\",\"Decaying Epsilon Greedy\",\"Softmax exploration\",\"UCB Exploration\"], loc =\"upper left\")\n",
    "plt.title(\"Regret for different Agents: 10 armed\")\n",
    "plt.savefig('Regrets 10 armed.pdf') \n",
    "# plt.index\n",
    "# plt.close()\n",
    "# Regret(envs,params,v_star,q_star,\"exp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% of Optimal Actions taken by different Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2023)\n",
    "np.random.seed(2023)\n",
    "Actions1=np.zeros((params[\"no_envs\"],params[\"maxEpisodes\"]))\n",
    "Q_est1=np.zeros((params[\"no_envs\"],envs[0].action_space.n))\n",
    "for i in range(params[\"no_envs\"]):\n",
    "    Q_est,R,actions=PureExploitation(envs[i],params)\n",
    "    # print(actions)\n",
    "    # print(np.shape(actions))\n",
    "    Actions1[i]=actions\n",
    "    # print(Q_est[-1])\n",
    "    # Q_est1[i]=Q_est[-1]\n",
    "Actions1=np.transpose(Actions1)\n",
    "# print(np.shape(Actions1))\n",
    "optimal1=np.zeros((params[\"maxEpisodes\"]))     \n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    count1=0\n",
    "    for j in range(params[\"no_envs\"]):\n",
    "        if Actions1[i][j] in np.where(q_star[j]==v_star[j]):\n",
    "            count1+=1\n",
    "    optimal1[i]=(count1*100/params[\"no_envs\"])\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),optimal1, color='r')\n",
    "# plt.ylabel('Average Regrets per timesteps')  \n",
    "# plt.xlabel('Timesteps') \n",
    "# plt.title('PureExploitation: % optimal action done vs timestep 2armedBandit') \n",
    "# plt.savefig(' PureExploitation-2armedBandit  % optimal action done  vs timestep .pdf') \n",
    "# plt.close()\n",
    "\n",
    "Actions1=np.zeros((params[\"no_envs\"],params[\"maxEpisodes\"]))\n",
    "Q_est1=np.zeros((params[\"no_envs\"],envs[0].action_space.n))\n",
    "for i in range(params[\"no_envs\"]):\n",
    "    Q_est,R,actions=PureExploration(envs[i],params)\n",
    "    # print(actions)\n",
    "    # print(np.shape(actions))\n",
    "    Actions1[i]=actions\n",
    "    # print(Q_est[-1])\n",
    "    # Q_est1[i]=Q_est[-1]\n",
    "Actions1=np.transpose(Actions1)\n",
    "# print(np.shape(Actions1))\n",
    "optimal1=np.zeros((params[\"maxEpisodes\"]))     \n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    count1=0\n",
    "    for j in range(params[\"no_envs\"]):\n",
    "        if Actions1[i][j] in np.where(q_star[j]==v_star[j]):\n",
    "            count1+=1\n",
    "    optimal1[i]=(count1*100/params[\"no_envs\"])\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),optimal1, color='blue')\n",
    "# plt.ylabel('Average Regrets per timesteps')  \n",
    "# plt.xlabel('Timesteps') \n",
    "# plt.title('PureExploration: % optimal action done vs timestep 2armedBandit') \n",
    "# plt.savefig('PureExploration-2armedBandit  % optimal action done  vs timestep .pdf') \n",
    "# plt.close()\n",
    "\n",
    "Actions1=np.zeros((params[\"no_envs\"],params[\"maxEpisodes\"]))\n",
    "Q_est1=np.zeros((params[\"no_envs\"],envs[0].action_space.n))\n",
    "for i in range(params[\"no_envs\"]):\n",
    "    Q_est,R,actions=epsilonGreedy(envs[i],params)\n",
    "    # print(actions)\n",
    "    # print(np.shape(actions))\n",
    "    Actions1[i]=actions\n",
    "    # print(Q_est[-1])\n",
    "    # Q_est1[i]=Q_est[-1]\n",
    "Actions1=np.transpose(Actions1)\n",
    "# print(np.shape(Actions1))\n",
    "optimal1=np.zeros((params[\"maxEpisodes\"]))     \n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    count1=0\n",
    "    for j in range(params[\"no_envs\"]):\n",
    "        if Actions1[i][j] in np.where(q_star[j]==v_star[j]):\n",
    "            count1+=1\n",
    "    optimal1[i]=(count1*100/params[\"no_envs\"])\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),optimal1, color='green')\n",
    "# plt.ylabel('Average Regrets per timesteps')  \n",
    "# plt.xlabel('Timesteps') \n",
    "# plt.title('epsilonGreedy: % optimal action done vs timestep 2armedBandit') \n",
    "# plt.savefig('epsilonGreedy-2armedBandit  % optimal action done  vs timestep .pdf') \n",
    "# plt.close()\n",
    "\n",
    "Actions1=np.zeros((params[\"no_envs\"],params[\"maxEpisodes\"]))\n",
    "Q_est1=np.zeros((params[\"no_envs\"],envs[0].action_space.n))\n",
    "for i in range(params[\"no_envs\"]):\n",
    "    Q_est,R,actions=decayingEpsilonGreedy(envs[i],params,decay_type)\n",
    "    # print(actions)\n",
    "    # print(np.shape(actions))\n",
    "    Actions1[i]=actions\n",
    "    # print(Q_est[-1])\n",
    "    # Q_est1[i]=Q_est[-1]\n",
    "Actions1=np.transpose(Actions1)\n",
    "# print(np.shape(Actions1))\n",
    "optimal1=np.zeros((params[\"maxEpisodes\"]))     \n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    count1=0\n",
    "    for j in range(params[\"no_envs\"]):\n",
    "        if Actions1[i][j] in np.where(q_star[j]==v_star[j]):\n",
    "            count1+=1\n",
    "    optimal1[i]=(count1*100/params[\"no_envs\"])\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),optimal1, color='black')\n",
    "# plt.ylabel('Average Regrets per timesteps')  \n",
    "# plt.xlabel('Timesteps') \n",
    "# plt.title('decayingEpsilonGreedy: % optimal action done vs timestep 2armedBandit') \n",
    "# plt.savefig('decayingEpsilonGreedy-2armedBandit  % optimal action done  vs timestep .pdf') \n",
    "# plt.close()\n",
    "\n",
    "Actions1=np.zeros((params[\"no_envs\"],params[\"maxEpisodes\"]))\n",
    "Q_est1=np.zeros((params[\"no_envs\"],envs[0].action_space.n))\n",
    "for i in range(params[\"no_envs\"]):\n",
    "    Q_est,R,actions=softMaxExploration(envs[i],params,decay_type)\n",
    "    # print(actions)\n",
    "    # print(np.shape(actions))\n",
    "    Actions1[i]=actions\n",
    "    # print(Q_est[-1])\n",
    "    # Q_est1[i]=Q_est[-1]\n",
    "Actions1=np.transpose(Actions1)\n",
    "# print(np.shape(Actions1))\n",
    "optimal1=np.zeros((params[\"maxEpisodes\"]))     \n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    count1=0\n",
    "    for j in range(params[\"no_envs\"]):\n",
    "        if Actions1[i][j] in np.where(q_star[j]==v_star[j]):\n",
    "            count1+=1\n",
    "    optimal1[i]=(count1*100/params[\"no_envs\"])\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),optimal1, color='pink')\n",
    "# plt.ylabel('Average Regrets per timesteps')  \n",
    "# plt.xlabel('Timesteps') \n",
    "# plt.title('softMaxExploration: % optimal action done vs timestep 2armedBandit') \n",
    "# plt.savefig(' softMaxExploration-2armedBandit  % optimal action done  vs timestep .pdf') \n",
    "# plt.close()\n",
    "\n",
    "Actions1=np.zeros((params[\"no_envs\"],params[\"maxEpisodes\"]))\n",
    "Q_est1=np.zeros((params[\"no_envs\"],envs[0].action_space.n))\n",
    "for i in range(params[\"no_envs\"]):\n",
    "    Q_est,R,actions=UCBexploration(envs[i],params)\n",
    "    # print(actions)\n",
    "    # print(np.shape(actions))\n",
    "    Actions1[i]=actions\n",
    "    # print(Q_est[-1])\n",
    "    # Q_est1[i]=Q_est[-1]\n",
    "Actions1=np.transpose(Actions1)\n",
    "# print(np.shape(Actions1))\n",
    "optimal1=np.zeros((params[\"maxEpisodes\"]))     \n",
    "for i in range(params[\"maxEpisodes\"]):\n",
    "    count1=0\n",
    "    for j in range(params[\"no_envs\"]):\n",
    "        if Actions1[i][j] in np.where(q_star[j]==v_star[j]):\n",
    "            count1+=1\n",
    "    optimal1[i]=(count1*100/params[\"no_envs\"])\n",
    "plt.plot(np.arange(0,params[\"maxEpisodes\"]),optimal1, color='orange')\n",
    "# plt.ylabel('Average Regrets per timesteps')  \n",
    "# plt.xlabel('Timesteps') \n",
    "# plt.title('UCBexploration: % optimal action done vs timestep 2armedBandit') \n",
    "# plt.savefig(' UCBexploration-2armedBandit  % optimal action done  vs timestep .pdf') \n",
    "# plt.close()\n",
    "plt.ylabel(\"% Optimal Actions\")\n",
    "plt.xlabel(\"Steps\") \n",
    "# plt.legend([\"Pure Exploitation\",\"Pure Exploration\",\"Epsilon-Greedy\",\"Decaying Epsilon Greedy\",\"Softmax exploration\",\"UCB Exploration\"], loc =\"upper left\")\n",
    "plt.title(\"%Optimal Actions for different Agents: 10 armed\")\n",
    "plt.savefig('%Optimal Actions- 10 armed.pdf') \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e8fbf9a983a2b3ad981a2f9c2c427da9bcf0d16346e2ead530ccf2c05c5df6b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
