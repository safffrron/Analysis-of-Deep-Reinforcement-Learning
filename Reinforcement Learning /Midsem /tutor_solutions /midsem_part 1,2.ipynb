{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEgup_6WtLpw",
    "outputId": "d68f4bab-7335-4bb8-a5ca-a862fc06a435"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9nRgoE5lORf"
   },
   "outputs": [],
   "source": [
    "from gymnasium import Env\n",
    "from gymnasium.spaces import  Discrete\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import bernoulli\n",
    "import yaml\n",
    "from matplotlib import pyplot as plt\n",
    "class RandomMaze(Env):\n",
    "    def __init__(self,params):\n",
    "        # action space left =0,up=1,right=2,down=3\n",
    "        self.action_space = Discrete(4)\n",
    "        # state spaces are 3,7-terminal, 5 wall, rest all non terminal\n",
    "        self.observation_space=Discrete(12)\n",
    "        # initial state is 8\n",
    "        self.startState=params[\"startState\"]\n",
    "        self.state=self.startState\n",
    "        # prob of 0.8 of going in the desired action direction\n",
    "        self.goInDirection=params[\"goInDirection\"]\n",
    "        self.goOrthogonal=(1-self.goInDirection)/2\n",
    "        # it incurs a livingCost for every non terminal state\n",
    "        self.livingCost=params[\"livingCost\"]\n",
    "\n",
    "    def step(self,action):\n",
    "        # stochasticity in the walk.\n",
    "        # sample from uniform (0,1) distribution using random.random()\n",
    "        random_num=np.random.random()\n",
    "        if random_num<=self.goInDirection:\n",
    "            pass\n",
    "        else:\n",
    "            bernouli_sample = np.random.binomial(1, .5)\n",
    "            if bernouli_sample == 0:\n",
    "                action= (action - 1)%4\n",
    "            else:\n",
    "                action= (action + 1)%4\n",
    "        # left action 0\n",
    "        if action == 0 :\n",
    "            if self.state not in [0,4,8,6]:\n",
    "                self.state-=1\n",
    "            else:\n",
    "                self.state=self.state\n",
    "        # right action 2\n",
    "        if action == 2 :\n",
    "            if self.state!=4 and self.state!=11:\n",
    "                self.state+=1\n",
    "            else:\n",
    "                self.state=self.state\n",
    "        # up action =1\n",
    "        if action == 1 :\n",
    "            if self.state not in [0,1,2,9]:\n",
    "                self.state-=4\n",
    "            else:\n",
    "                self.state=self.state\n",
    "        # down action = 3\n",
    "        if action == 3 :\n",
    "            if self.state not in [1,8,9,10,11]:\n",
    "                self.state+=4\n",
    "            else:\n",
    "                self.state=self.state\n",
    "\n",
    "        if self.state==3:\n",
    "            reward = 1\n",
    "        elif self.state == 7:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward=-self.livingCost\n",
    "        # check wether we reached a terminal state or not\n",
    "        if self.state==3 or self.state==7:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        # set placeholder for information\n",
    "        info={}\n",
    "        truncated = False\n",
    "        return self.state,reward,done,truncated,info\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def currState(self):\n",
    "        return self.state\n",
    "\n",
    "    def  reset(self, seed=None, options=None):\n",
    "        # reset state\n",
    "        super().reset(seed=seed)\n",
    "        self.state=self.startState\n",
    "        done=False\n",
    "        info = {}\n",
    "        return self.state,info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCWmIY4AlORi",
    "outputId": "4da09576-b89a-4206-c66b-d5119a63a4fb"
   },
   "outputs": [],
   "source": [
    "from gymnasium import Env, register\n",
    "\n",
    "register(id='RME-v0', entry_point=RandomMaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZouGnWqFlORj",
    "outputId": "732f520c-c650-4230-f50d-a0117de1602b"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "random.seed(10)\n",
    "np.random.seed(10)\n",
    "params={}\n",
    "params[\"gamma\"]=0.99          #discount factor\n",
    "params[\"goInDirection\"]=0.8       #probability of successfully moving in the intended direction\n",
    "params[\"livingCost\"]=0.04         #living cost absolute value\n",
    "params[\"startState\"]=8            #start state\n",
    "params[\"theta\"]=pow(10,-10)       # threshold for convergence\n",
    "params[\"S+\"]=[0,1,2,3,4,6,7,8,9,10,11]        #all reachable states\n",
    "params[\"S\"]=[0,1,2,4,6,8,9,10,11]    #non terminal states\n",
    "params[\"A\"]=[0,1,2,3]                #action space\n",
    "env=gym.make('RME-v0',params=params)\n",
    "env.reset(seed=10)\n",
    "done= False\n",
    "score=0\n",
    "time=0\n",
    "while not done:\n",
    "# env.render()\n",
    "    action =random.choice([0,1,2,3])\n",
    "    curr_state=env.currState()\n",
    "    next_state, reward, done, truncated, info =env.step(action)\n",
    "    #if done:\n",
    "    #   env.reset(seed=10)\n",
    "    score+=reward\n",
    "    print('timeStamp:{}  CurrState:{}  Action:{}  NextState:{}  Reward:{}  Score:{:.2f}'.format(time,curr_state,action,next_state,reward,score))\n",
    "    time+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZGqZ47flORk"
   },
   "outputs": [],
   "source": [
    "pi=np.zeros(12)\n",
    "pi[0]=3\n",
    "pi[1]=2\n",
    "pi[2]=2\n",
    "pi[3]=0   \n",
    "pi[4]=1\n",
    "pi[5]=0 \n",
    "pi[6]=3\n",
    "pi[7]=0 \n",
    "pi[8]=1\n",
    "pi[9]=2\n",
    "pi[10]=3\n",
    "pi[11]=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EbpgeB9lORl"
   },
   "outputs": [],
   "source": [
    "\n",
    "# creating the dynamics function for our random maze environment\n",
    "l=-params[\"livingCost\"]\n",
    "# left =0,up=1,right=2,down=3\n",
    "P={\n",
    "    0:{\n",
    "        0: [(0.8, 0, l, False), (0.1, 0, l, False), (0.1, 4, l, False)],\n",
    "        1: [(0.8, 0, l, False), (0.1, 0, l, False), (0.1, 1, l, False)],\n",
    "        2: [(0.8, 1, l, False), (0.1, 0, l, False), (0.1, 4, l, False)],\n",
    "        3: [(0.8, 4, l, False), (0.1, 0, l, False), (0.1, 1, l, False)]\n",
    "    },\n",
    "    1:{\n",
    "        0: [(0.8, 0, l, False), (0.1, 1, l, False), (0.1, 1, l, False)],\n",
    "        1: [(0.8, 1, l, False), (0.1, 0, l, False), (0.1, 2, l, False)],\n",
    "        2: [(0.8, 2, l, False), (0.1, 1, l, False), (0.1, 1, l, False)],\n",
    "        3: [(0.8, 1, l, False), (0.1, 0, l, False), (0.1, 2, l, False)]\n",
    "    },\n",
    "    2:{\n",
    "        0: [(0.8, 1, l, False), (0.1, 2, l, False), (0.1, 6, l, False)],\n",
    "        1: [(0.8, 2, l, False), (0.1, 1, l, False), (0.1, 3, 1, True)],\n",
    "        2: [(0.8, 3, 1, True), (0.1, 2, l, False), (0.1, 6, l, False)],\n",
    "        3: [(0.8, 6, l, False), (0.1, 1, l, False), (0.1, 3, 1, True)]\n",
    "    },\n",
    "    3:{\n",
    "        0: [(1.0, 0, 0.0, True)],\n",
    "        1: [(1.0, 0, 0.0, True)],\n",
    "        2: [(1.0, 0, 0.0, True)],\n",
    "        3: [(1.0, 0, 0.0, True)]\n",
    "    },\n",
    "    4:{\n",
    "        0: [(0.8, 4, l, False), (0.1, 0, l, False), (0.1, 8, l, False)],\n",
    "        1: [(0.8, 0, l, False), (0.1, 4, l, False), (0.1, 4, l, False)],\n",
    "        2: [(0.8, 4, l, False), (0.1, 0, l, False), (0.1, 8, l, False)],\n",
    "        3: [(0.8, 8, l, False), (0.1, 4, l, False), (0.1, 4, l, False)]\n",
    "    },\n",
    "    6:{\n",
    "        0: [(0.8, 6, l, False), (0.1, 2, l, False), (0.1, 10, l, False)],\n",
    "        1: [(0.8, 2, l, False), (0.1, 6, l, False), (0.1, 7, -1, True)],\n",
    "        2: [(0.8, 7, -1, True), (0.1, 2, l, False), (0.1, 10, l, False)],\n",
    "        3: [(0.8, 10, l, False), (0.1, 6, l, False), (0.1, 7, -1, True)]\n",
    "    },\n",
    "    7:{\n",
    "        0: [(1.0, 0, 0.0, True)],\n",
    "        1: [(1.0, 0, 0.0, True)],\n",
    "        2: [(1.0, 0, 0.0, True)],\n",
    "        3: [(1.0, 0, 0.0, True)]\n",
    "    },\n",
    "    8:{\n",
    "        0: [(0.8, 8, l, False), (0.1, 4, l, False), (0.1, 8, l, False)],\n",
    "        1: [(0.8, 4, l, False), (0.1, 8, l, False), (0.1, 9, l, False)],\n",
    "        2: [(0.8, 9, l, False), (0.1, 4, l, False), (0.1, 8, l, False)],\n",
    "        3: [(0.8, 8, l, False), (0.1, 8, l, False), (0.1, 9, l, False)]\n",
    "    },\n",
    "    9:{\n",
    "        0: [(0.8, 8, l, False), (0.1, 9, l, False), (0.1, 9, l, False)],\n",
    "        1: [(0.8, 9, l, False), (0.1, 8, l, False), (0.1, 10, l, False)],\n",
    "        2: [(0.8, 10, l, False), (0.1, 9, l, False), (0.1, 9, l, False)],\n",
    "        3: [(0.8, 9, l, False), (0.1, 8, l, False), (0.1, 10, l, False)]\n",
    "    },\n",
    "    10:{\n",
    "        0: [(0.8, 9, l, False), (0.1, 6, l, False), (0.1, 10, l, False)],\n",
    "        1: [(0.8, 6, l, False), (0.1, 9, l, False), (0.1, 11, l, False)],\n",
    "        2: [(0.8, 11, l, False), (0.1, 6, l, False), (0.1, 10, l, False)],\n",
    "        3: [(0.8, 10, l, False), (0.1, 9, l, False), (0.1, 11, l, False)]\n",
    "    },\n",
    "    11:{\n",
    "        0: [(0.8, 10, l, False), (0.1, 7, -1, True), (0.1, 11, l, False)],\n",
    "        1: [(0.8, 7, -1, True), (0.1, 10, l, False), (0.1, 11, l, False)],\n",
    "        2: [(0.8, 11, l, False), (0.1, 7, -1, True), (0.1, 11, l, False)],\n",
    "        3: [(0.8, 11, l, False), (0.1, 10, l, False), (0.1, 11, l, False)]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0MTkMWWlORm"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function evaluates a given policy pi. It iteratively calculates the state-value function v for a policy until\n",
    "the maximum change in value functions of any states is less than a small threshold theta. The input P is the transition\n",
    "model, params contains the MDP parameters, and pi is the current policy.\n",
    "The output v_new is the value function for the given policy, which estimates how good it is to be in each state following the policy.\n",
    "'''\n",
    "def PolicyEvaluation(P,params,pi):\n",
    "    v_new=np.zeros(12)\n",
    "    itr=0\n",
    "    while 1:\n",
    "        itr+=1\n",
    "        delta = 0\n",
    "        for s in params[\"S+\"]:\n",
    "            vs = 0\n",
    "            for p,s_next,r,f in P[s][pi[s]]:  #as you are taking deterministic policy\n",
    "                vs+=p*(r+params[\"gamma\"]*v_new[s_next])\n",
    "            # v_new[s]+=(pi[s]==a)*temp\n",
    "            delta = max(delta, np.abs(v_new[s] - vs))\n",
    "            v_new[s] = vs\n",
    "        if delta < params[\"theta\"]:\n",
    "            break\n",
    "    return v_new,itr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LtgA-w3lORm"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function improves a given policy based on the value function v calculated from PolicyEvaluation.\n",
    "It computes the action-value function Q for each action at each state and then updates the policy to choose\n",
    "the action with the highest action-value at each state. The output pi is the improved policy.\n",
    "'''\n",
    "\n",
    "def PolicyImprovement(v,P,params):\n",
    "    Q=np.zeros((12,len(params[\"A\"])))\n",
    "    pi=np.zeros(12)\n",
    "    for s in params[\"S+\"]:\n",
    "        for a in params[\"A\"]:\n",
    "            for p,s_next,r,f in P[s][a]:\n",
    "                Q[s][a]+=p*(r+params[\"gamma\"]*v[s_next])\n",
    "    for s in params[\"S+\"]:\n",
    "        pi[s]=np.argmax(Q[s])\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5k-TUyculORm",
    "outputId": "d0b98669-f543-40a0-fe9b-ad59edc0a5a0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function uses PolicyEvaluation and PolicyImprovement to find the optimal policy.\n",
    "It alternates between evaluating a policy and improving it until the policy is stable (i.e., it no longer changes).\n",
    "The output v is the value function for the optimal policy, pi is the optimal policy itself, and itr is the number of iterations it took to converge.\n",
    "'''\n",
    "def PolicyIteration(P,params,pi):\n",
    "    itr=0\n",
    "    while 1:\n",
    "        pi_old=pi\n",
    "        v,itrr=PolicyEvaluation(P,params,pi)\n",
    "        itr+=itrr\n",
    "        pi=PolicyImprovement(v,P,params)\n",
    "        #can also have a value convergence check.  #TODO later\n",
    "        if np.all(pi_old==pi):\n",
    "            break\n",
    "    return v,pi,itr\n",
    "v,pi,itr=PolicyIteration(P,params,pi)\n",
    "np.random.seed(3)\n",
    "print(\"PolicyIteration\")\n",
    "print(v)\n",
    "print(pi)\n",
    "print(itr)\n",
    "\n",
    "\n",
    "# left =0,up=1,right=2,down=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYqjOGNzqnp4",
    "outputId": "fbd8a28c-f8c6-4a6b-d4fa-16bd72817a37"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function initializes a value function v to zero for all states and iteratively computes\n",
    "the action-value function Q for each state-action pair. The major computations involve updating Q\n",
    "based on the expected rewards and the discounted value of subsequent states, as given by the Bellman equation.\n",
    "The function iterates until the change in v is below theta, indicating convergence.\n",
    "'''\n",
    "def ValueIteration(P,params):\n",
    "    v=np.zeros(12)\n",
    "    itr=0\n",
    "    while 1:\n",
    "        delta = 0\n",
    "        itr+=1\n",
    "        Q=np.zeros((12,4))\n",
    "        for s in params[\"S+\"]:\n",
    "            for a in params[\"A\"]:\n",
    "                for p,s_next,r,f in P[s][a]:\n",
    "                    Q[s][a]+=p*(r+params[\"gamma\"]*v[s_next])\n",
    "        for s in params[\"S+\"]:\n",
    "            vs = v[s]\n",
    "            v[s] = np.max(Q[s])\n",
    "            delta = max(delta, np.abs(vs-v[s]))\n",
    "        if delta < params[\"theta\"]:\n",
    "            break\n",
    "\n",
    "    pi = PolicyImprovement(v, P, params)\n",
    "    return v,pi,itr\n",
    "np.random.seed(3)\n",
    "v,pi,itr=ValueIteration(P,params)\n",
    "print(\"value\")\n",
    "print(v)\n",
    "print(pi)\n",
    "print(itr)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "interpreter": {
   "hash": "9e8fbf9a983a2b3ad981a2f9c2c427da9bcf0d16346e2ead530ccf2c05c5df6b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
