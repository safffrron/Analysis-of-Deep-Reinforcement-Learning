{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all useful functions \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces, register, make\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b70a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomMaze(Env):\n",
    "    \n",
    "    \n",
    "    #----- 1 -----\n",
    "    #constructor for initialization and some helper functions\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #P is basically State: Action: [ Transition Probability , Next state , Reward , isTerminated?]\n",
    "        # for actions : 0 -> up 1-> right 2->down 3-> left (clockwise)\n",
    "        self.P = {\n",
    "            0: {\n",
    "                0: [(0.8, 0, -0.04, False),(0.1, 1, -0.04, False),(0.1, 0, -0.04, False)],\n",
    "                1: [(0.8, 1, -0.04, False),(0.1, 0, -0.04, False),(0.1, 4, -0.04, False)],\n",
    "                2: [(0.8, 4, -0.04, False),(0.1, 1, -0.04, False),(0.1, 0, -0.04, False)],\n",
    "                3: [(0.8, 0, -0.04, False),(0.1, 0, -0.04, False),(0.1, 4, -0.04, False)]\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.8, 1, -0.04, False),(0.1, 2, -0.04, False),(0.1, 0, -0.04, False)],\n",
    "                1: [(0.8, 2, -0.04, False),(0.1, 1, -0.04, False),(0.1, 6, -0.04, False)],\n",
    "                2: [(0.8, 1, -0.04, False),(0.1, 2, -0.04, False),(0.1, 0, -0.04, False)],\n",
    "                3: [(0.8, 0, -0.04, False),(0.1, 1, -0.04, False),(0.1, 1, -0.04, False)]\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.8, 2, -0.04, False),(0.1, 1, -0.04, False),(0.1, 3,  1.00, True)],\n",
    "                1: [(0.8, 3,  1.00, True ),(0.1, 2, -0.04, False),(0.1, 6, -0.04, False)],\n",
    "                2: [(0.8, 6, -0.04, False),(0.1, 1, -0.04, False),(0.1, 3,  1.00, True)],\n",
    "                3: [(0.8, 1, -0.04, False),(0.1, 2, -0.04, False),(0.1, 6, -0.04, False)]\n",
    "            },\n",
    "            3: {\n",
    "                0: [(1.0, 3, 0.00, True)],         # Goal\n",
    "                1: [(1.0, 3, 0.00, True)],\n",
    "                2: [(1.0, 3, 0.00, True)],\n",
    "                3: [(1.0, 3, 0.00, True)]\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.8, 0, -0.04, False),(0.1, 4, -0.04, False),(0.1, 4, -0.04, False)],\n",
    "                1: [(0.8, 4, -0.04, False),(0.1, 0, -0.04, False),(0.1, 8, -0.04, False)],\n",
    "                2: [(0.8, 8, -0.04, False),(0.1, 4, -0.04, False),(0.1, 4, -0.04, False)],\n",
    "                3: [(0.8, 4, -0.04, False),(0.1, 0, -0.04, False),(0.1, 8, -0.04, False)]\n",
    "            },\n",
    "            5: {\n",
    "                0: [(1.0, 5, 0.00, True)],\n",
    "                1: [(1.0, 5, 0.00, True)],         # wall\n",
    "                2: [(1.0, 5, 0.00, True)],\n",
    "                3: [(1.0, 5, 0.00, True)]\n",
    "            },\n",
    "            6: {\n",
    "                0: [(0.8, 2, -0.04, False),(0.1, 6, -0.04, False),(0.1, 7, -1.00,  True)],\n",
    "                1: [(0.8, 7, -1.00, True ),(0.1, 2, -0.04, False),(0.1, 10, -0.04, False)],\n",
    "                2: [(0.8, 10, -0.04,False),(0.1, 6, -0.04, False),(0.1, 7, -1.00,  True)],\n",
    "                3: [(0.8, 6, -0.04, False),(0.1, 2, -0.04, False),(0.1, 10, -0.04, False)]\n",
    "            },\n",
    "            7: {\n",
    "                0: [(1.0, 7, 0.00, True)],\n",
    "                1: [(1.0, 7, 0.00, True)],        # hole\n",
    "                2: [(1.0, 7, 0.00, True)],\n",
    "                3: [(1.0, 7, 0.00, True)]\n",
    "            },\n",
    "            8: {\n",
    "                0: [(0.8, 4, -0.04, False),(0.1, 9, -0.04, False),(0.1, 8, -0.04, False)],\n",
    "                1: [(0.8, 9, -0.04, False),(0.1, 4, -0.04, False),(0.1, 8, -0.04, False)],\n",
    "                2: [(0.8, 8, -0.04, False),(0.1, 8, -0.04, False),(0.1, 9, -0.04, False)],\n",
    "                3: [(0.8, 8, -0.04, False),(0.1, 4, -0.04, False),(0.1, 8, -0.04, False)]\n",
    "            },\n",
    "            9: {\n",
    "                0: [(0.8, 9, -0.04, False),(0.1, 10, -0.04, False),(0.1, 8, -0.04, False)],\n",
    "                1: [(0.8, 10, -0.04, False),(0.1, 9, -0.04, False),(0.1, 9, -0.04, False)],\n",
    "                2: [(0.8, 9, -0.04, False),(0.1, 8, -0.04, False),(0.1, 10, -0.04, False)],\n",
    "                3: [(0.8, 8, -0.04, False),(0.1, 9, -0.04, False),(0.1, 9, -0.04, False)]\n",
    "            },\n",
    "            10: {\n",
    "                0: [(0.8, 6, -0.04, False),(0.1, 9, -0.04, False),(0.1, 11, -0.04, False)],\n",
    "                1: [(0.8, 11, -0.04, False),(0.1, 6, -0.04, False),(0.1, 10, -0.04, False)],\n",
    "                2: [(0.8, 10, -0.04, False),(0.1, 11, -0.04, False),(0.1, 9, -0.04, False)],\n",
    "                3: [(0.8, 9, -0.04, False),(0.1, 6, -0.04, False),(0.1, 10, -0.04, False)]\n",
    "            },\n",
    "            11: {\n",
    "                0: [(0.8, 7, -1.00, True ),(0.1, 10, -0.04, False),(0.1, 11, -0.04, False)],\n",
    "                1: [(0.8, 11, -0.04, False),(0.1, 11, -0.04, False),(0.1, 7, -1.00,  True)],\n",
    "                2: [(0.8, 11, -0.04, False),(0.1, 10, -0.04, False),(0.1, 11, -0.04, False)],\n",
    "                3: [(0.8, 10, -0.04,False),(0.1, 11, -0.04, False),(0.1, 7, -1.00,  True)]\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        self.size = 12 # The size of the grid\n",
    "        #self.window_size = 512  # The size of the PyGame window\n",
    "        \n",
    "        \n",
    "        #self.agent_location = 8 \n",
    "        \n",
    "        # We have 3 observations, corresponding to each position in the 1-D grid\n",
    "        self.observation_space = spaces.Discrete(self.size)\n",
    "\n",
    "        # We have 2 actions, corresponding to \"left\" & \"right\"\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.action_space_size_ = 4 \n",
    "        \n",
    "    \n",
    "    #return the locations of agent and target\n",
    "    def _get_obs(self):\n",
    "        return {   \n",
    "            \"agent\" : self.agent_location, \n",
    "            \"target\": self._target_location  \n",
    "        }\n",
    "    \n",
    "    #returns the distance between agent and target \n",
    "    def _get_info(self):\n",
    "        return {  \n",
    "            \"distance\": abs(self.agent_location - self._target_location)   \n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #----- 2 ------\n",
    "    # The reset function to initiate \n",
    "    \n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        self.agent_location = 8             #location of agent in the begining\n",
    "        self._target_location = 3            #location of target  \n",
    "        self._dead_state = 7                 #dead location\n",
    "        \n",
    "        \n",
    "        observation = self._get_obs()        #getting useful information\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation,info\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #------- 3 ---------\n",
    "    # The step function \n",
    "    \n",
    "    def step(self, action):  # takes action as a parameter\n",
    "\n",
    "        # gets the current location and stores the values from P set \n",
    "        prev_location = self.agent_location                                #gets location\n",
    "        transitions = self.P[prev_location][action]                         #gets the corresponding action tuple\n",
    "        probabilities, next_states, rewards, terminals = zip(*transitions)  #stores the value for use \n",
    "        \n",
    "        # Randomly select a transition based on the probabilities\n",
    "        # gives you random state based on your probabilities \n",
    "        index = random.choices(range(len(probabilities)), weights=probabilities, k=1)[0]\n",
    "        # stores the values \n",
    "        self.agent_location, reward, terminated = next_states[index], rewards[index], terminals[index]\n",
    "        \n",
    "        truncated = False\n",
    "        observation = self._get_obs()  \n",
    "        info = self._get_info()\n",
    "\n",
    "        info[\"log\"] = {\"current_state\": prev_location, \n",
    "                       \"action\":action,  \n",
    "                        \"next_state\": self.agent_location}\n",
    "\n",
    "        # Return the required 5-tuple\n",
    "        return observation, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13bdd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the custom environment\n",
    "register(id='RMaze', entry_point=RandomMaze)\n",
    "\n",
    "optimal_policy = [0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 2] # obtained in previous question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19cc385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTrajectory( Env , policy , maxSteps) :\n",
    "    observation = environment.reset(seed=0)\n",
    "    experience = []\n",
    "    terminated = False\n",
    "    steps=1\n",
    "    while not terminated :\n",
    "        \n",
    "        \n",
    "        action = policy[Env.agent_location]\n",
    "        observation, reward, terminated, truncated, info = environment.step(action)\n",
    "        \n",
    "        experience.append( (info['log']['current_state'],info['log']['action'],reward,info['log']['next_state']) )\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            observation = environment.reset(seed=0)\n",
    "            return experience\n",
    "            \n",
    "        if steps > maxSteps :\n",
    "            observation = environment.reset(seed=0)\n",
    "            return []\n",
    "        \n",
    "        steps+=1\n",
    "        \n",
    "environment = make('RMaze')\n",
    "generateTrajectory(environment , optimal_policy , 10)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decay-type : 0 -> linear decay , 1 -> exponential decay\n",
    "\n",
    "def decayAlpha(initialValue, finalValue, maxSteps, decayType) :\n",
    "    ans=[initialValue]\n",
    "    if decayType == 0 :\n",
    "        \n",
    "        steps= abs(finalValue-initialValue)/maxSteps \n",
    "        i = 1 \n",
    "        value = initialValue\n",
    "        while i <= maxSteps :\n",
    "            value = value - steps\n",
    "            ans.append(value)\n",
    "            i+=1\n",
    "        return ans\n",
    "    \n",
    "    else :\n",
    "        \n",
    "        factor =  (finalValue / initialValue) ** (1 / maxSteps) \n",
    "        i=1\n",
    "        \n",
    "        while i <= maxSteps :\n",
    "            value = initialValue * (factor ** i)\n",
    "            ans.append(value)\n",
    "            i+=1\n",
    "        \n",
    "        return ans \n",
    "        \n",
    "y_axis1 = decayAlpha(1000, 10, 10, 1)\n",
    "y_axis2 = decayAlpha(1000, 10, 10, 0) \n",
    "\n",
    "x_axis = [i for i in range(1,12)]\n",
    "\n",
    "\n",
    "plt.plot(x_axis, y_axis1 , label='exponential')\n",
    "plt.plot(x_axis, y_axis2 , label='linear'      )\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "#plt.savefig(\"qn22.png\", format=\"png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec4ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monte Carlo First Visit -> firstVisit = True\n",
    "#Monte Carlo Every Visit -> firstVisit = False \n",
    "\n",
    "def MonteCarloPrediction( Env , maxSteps , noEpisodes ,policy ,alph=0.1 , gamma=0.99,   firstVisit=False):\n",
    "    # initializations\n",
    "    v = np.zeros(Env.size)\n",
    "    visited=np.zeros(Env.size)\n",
    "    v_r = np.zeros((noEpisodes,Env.size))\n",
    "    G_t = np.zeros(noEpisodes)\n",
    "    observation = Env.reset(seed=0)\n",
    "    \n",
    "    alpha= decayAlpha(alph, 0.01, noEpisodes , 0)\n",
    "    #algorithm\n",
    "    for e in range(noEpisodes):\n",
    "        observation = Env.reset(seed=0)\n",
    "        value=0\n",
    "        t = generateTrajectory( Env , policy , maxSteps)\n",
    "        \n",
    "        visited[:]=False\n",
    "        \n",
    "        for i , (s,action,reward,nex) in enumerate(t):\n",
    "            if visited[s] and firstVisit :\n",
    "                continue\n",
    "            else :\n",
    "                visited[s]=True\n",
    "            \n",
    "            G=0\n",
    "            j=i\n",
    "            while j<len(t):\n",
    "                G += ((gamma**(j-i))*(t[j][2]))\n",
    "                j+=1\n",
    "            \n",
    "            if s==6:\n",
    "                value=G\n",
    "            \n",
    "            v[s]+= alpha[e]*(G-v[s]) \n",
    "        G_t[e]=value\n",
    "        v_r[e]=v\n",
    "        \n",
    "    return  v_r\n",
    "\n",
    "\n",
    "environment = make('RMaze' )\n",
    "observation = environment.reset(seed=0)\n",
    "MonteCarloPrediction( environment, 100,5,optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca2cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporal Difference \n",
    "\n",
    "def TemporalDifference( Env , noEpisodes ,alph ,policy , gamma=0.99):\n",
    "    # initializations\n",
    "    v = np.zeros(Env.size)\n",
    "    v_r = np.zeros((noEpisodes,Env.size))\n",
    "    observation = Env.reset(seed=0)\n",
    "    G_t = np.zeros(noEpisodes)\n",
    "    alpha= decayAlpha(alph, 0.01, noEpisodes , 1)\n",
    "    \n",
    "    \n",
    "    #algorithm\n",
    "    for e in range(1,noEpisodes):\n",
    "        #alpha= decayAlpha(alph, 0.01, e , 1)\n",
    "        terminated = False \n",
    "        value=0\n",
    "        \n",
    "        while not terminated :\n",
    "            action = policy[Env.agent_location]\n",
    "            observation, reward, terminated, truncated, info = Env.step(action)\n",
    "            td_target = reward\n",
    "            if not terminated :\n",
    "                td_target += gamma * v[info['log']['next_state']]\n",
    "                \n",
    "            td_error=td_target-v[info['log']['current_state']] \n",
    "            if info['log']['current_state'] == 6 :\n",
    "                value=td_target\n",
    "            \n",
    "            v[info['log']['current_state']]+=(alpha[e]*td_error)\n",
    "            info['log']['current_state']=info['log']['next_state']\n",
    "            \n",
    "        G_t[e]=value\n",
    "        v_r[e]=v\n",
    "        observation = Env.reset(seed=0)\n",
    "    return  v_r\n",
    "\n",
    "\n",
    "environment = make('RMaze' )\n",
    "observation = environment.reset(seed=0)\n",
    "TemporalDifference( environment , 5 , 0.1 , optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNStepTrajectory( Env , policy , s , n ) :\n",
    "    observation = environment.reset(seed=0)\n",
    "    path = []\n",
    "    terminated = False\n",
    "    flag = True\n",
    "    steps=1\n",
    "    while not terminated :\n",
    "        \n",
    "        \n",
    "        action = policy[s]\n",
    "        observation, reward, terminated, truncated, info = Env.step(action)\n",
    "#         if flag:\n",
    "#             flag=False\n",
    "#             s_dash=info['log']['next_state']\n",
    "#             t=terminated\n",
    "        path.append( reward )\n",
    "        \n",
    "        if terminated or truncated or steps > n:\n",
    "            observation = environment.reset(seed=0)\n",
    "            return path,info['log']['next_state'],terminated\n",
    "        \n",
    "        steps+=1\n",
    "\n",
    "def calculateReturn(gamma , path , n ):\n",
    "    sum =0\n",
    "    c=0\n",
    "    for i in path:\n",
    "        if c>=n:\n",
    "            break\n",
    "        sum+= ((gamma**c) * i)\n",
    "        c+=1\n",
    "        \n",
    "    return sum    \n",
    "\n",
    "\n",
    "def n_stepTemporalDifference( Env , policy , n, noEpisodes , gamma=0.99 , alph=0.5 ):\n",
    "    # initializations\n",
    "    v = np.zeros(Env.size)\n",
    "    v_r = np.zeros((noEpisodes,Env.size))\n",
    "    alpha= decayAlpha(alph, 0.01, noEpisodes , 1)\n",
    "    G_t = np.zeros(noEpisodes)\n",
    "    \n",
    "    #algorithm\n",
    "    for e in range(noEpisodes):\n",
    "        \n",
    "        observation = Env.reset(seed=0)\n",
    "        s=8                                            #inital pos after reset\n",
    "        terminated = False \n",
    "        path=[]\n",
    "        \n",
    "        while not terminated  :\n",
    "            path, s_dash , terminated = generateNStepTrajectory(Env, policy , s , n)\n",
    "            target = calculateReturn(gamma , path , n)\n",
    "            if not terminated :\n",
    "                target = target + ((gamma**n) * v[s_dash])\n",
    "                \n",
    "            ntd_error=target-v[s]\n",
    "            v[s]=v[s]+(alpha[e] * ntd_error)\n",
    "            s=s_dash\n",
    "            if len(path)<=1:\n",
    "                break\n",
    "        v_r[e]=v\n",
    "    return v_r\n",
    "\n",
    "\n",
    "ev = make('RMaze' )\n",
    "observation = ev.reset(seed=0)\n",
    "n_stepTemporalDifference( ev , optimal_policy , 5 , 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3914be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TDLambdaPrediction(Env , policy , noEpisodes , lamda , gamma=0.99 , alph =1 ):\n",
    "    v = np.zeros(Env.size)\n",
    "    v_r = np.zeros((noEpisodes,Env.size))\n",
    "    E = np.zeros(Env.size)\n",
    "    y_axis= []\n",
    "    n=0\n",
    "    for e in range(noEpisodes):\n",
    "        alpha = decayAlpha(alph, 0.01, noEpisodes , 1)\n",
    "        observation = Env.reset(seed=0)\n",
    "        s=8 \n",
    "        terminated = False \n",
    "        while not terminated:\n",
    "            action = policy[s]\n",
    "            observation, reward, terminated, truncated, info = Env.step(action)\n",
    "            \n",
    "            td_target = reward \n",
    "            if not terminated :\n",
    "                td_target += (gamma * v[info['log']['next_state']])\n",
    "            \n",
    "            td_error = td_target - v[s]\n",
    "            E[s]+=1\n",
    "            \n",
    "            v = v + (alpha[e]*td_error*E)\n",
    "            E = gamma*lamda*E\n",
    "            s=info['log']['next_state']\n",
    "            if e==100 :\n",
    "                y_axis.append(E)\n",
    "                n+=1\n",
    "            \n",
    "        v_r[e]=v\n",
    "    return v_r\n",
    "            \n",
    "    \n",
    "ev = make('RMaze' )\n",
    "observation = ev.reset(seed=0)\n",
    "TDLambdaPrediction( ev , optimal_policy , 5 , 0.1 )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24020801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal and Log plot for Monte Carlo FV and EV\n",
    "#Monte Carlo First Visit -> firstVisit = True\n",
    "#Monte Carlo Every Visit -> firstVisit = False \n",
    "# For getting log plot activate the plt.xscale('log') comment\n",
    "\n",
    "arr = np.zeros((500,12))\n",
    "              \n",
    "for i in range(20): \n",
    "    environment = make('RMaze')\n",
    "    observation = environment.reset(seed=i)\n",
    "    arr = arr + MonteCarloPrediction( environment, 100,500,optimal_policy,firstVisit = False)\n",
    "arr = arr/20\n",
    "y_axis1=arr[:,0]\n",
    "y_axis2=arr[:,1]\n",
    "y_axis3=arr[:,2]\n",
    "y_axis4=arr[:,3]\n",
    "y_axis5=arr[:,4]\n",
    "y_axis6=arr[:,5]\n",
    "y_axis7=arr[:,6]\n",
    "y_axis8=arr[:,7]\n",
    "y_axis9=arr[:,8]\n",
    "y_axis10=arr[:,9]\n",
    "y_axis11=arr[:,10]\n",
    "y_axis12=arr[:,11]\n",
    "y_axis71=y_axis7\n",
    "x_axis = np.linspace(1,500,500)    \n",
    "\n",
    "# Plot the data\n",
    "plt.plot(x_axis, y_axis1, label='state-0')\n",
    "plt.plot(x_axis, y_axis2, label='state-1')\n",
    "plt.plot(x_axis, y_axis3, label='state-2')\n",
    "plt.plot(x_axis, y_axis4, label='state-3')\n",
    "plt.plot(x_axis, y_axis5, label='state-4')\n",
    "plt.plot(x_axis, y_axis6, label='state-5')\n",
    "plt.plot(x_axis, y_axis7, label='state-6')\n",
    "plt.plot(x_axis, y_axis8, label='state-7')\n",
    "plt.plot(x_axis, y_axis9, label='state-8')\n",
    "plt.plot(x_axis, y_axis10, label='state-9')\n",
    "plt.plot(x_axis, y_axis11, label='state-10')\n",
    "plt.plot(x_axis, y_axis12, label='state-11')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('Value vs. Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.25, 0.5))\n",
    "\n",
    "plt.axhline(y = -0.223, color = 'blue', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.107, color = 'orange', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.90, color = 'green', linestyle = 'dashed')\n",
    "plt.axhline(y = 0, color = 'red', linestyle = 'dashed')\n",
    "plt.axhline(y = -0.234, color = 'purple', linestyle = 'dashed')\n",
    "plt.axhline(y = 0, color = 'brown', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.318, color = 'pink', linestyle = 'dashed')\n",
    "plt.axhline(y = 0, color = 'gray', linestyle = 'dashed')\n",
    "plt.axhline(y = -0.267, color = 'lime', linestyle = 'dashed')\n",
    "plt.axhline(y = -0.256, color = 'cyan', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.140, color = 'blue', linestyle = 'dashed')\n",
    "plt.axhline(y = -0.226, color = 'orange', linestyle = 'dashed')\n",
    "\n",
    "#plt.xscale('log')\n",
    "\n",
    "#plt.savefig(\"qn315.png\", format=\"png\", dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f3f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal and Log plot for Temporal Difference \n",
    "# For getting log plot activate the plt.xscale('log') comment\n",
    "\n",
    "arr = np.zeros((500,12))\n",
    "              \n",
    "for i in range(20): \n",
    "    environment = make('RMaze')\n",
    "    observation = environment.reset(seed=i)\n",
    "    arr = arr + TemporalDifference( environment , 500 , 0.5 , optimal_policy)\n",
    "arr = arr/20\n",
    "y_axis1=arr[:,0]\n",
    "y_axis2=arr[:,1]\n",
    "y_axis3=arr[:,2]\n",
    "y_axis4=arr[:,3]\n",
    "y_axis5=arr[:,4]\n",
    "y_axis6=arr[:,5]\n",
    "y_axis7=arr[:,6]\n",
    "y_axis8=arr[:,7]\n",
    "y_axis9=arr[:,8]\n",
    "y_axis10=arr[:,9]\n",
    "y_axis11=arr[:,10]\n",
    "y_axis12=arr[:,11]\n",
    "y_axis72=y_axis7\n",
    "x_axis = np.linspace(1,500,500)    \n",
    "\n",
    "# Plot the data\n",
    "plt.plot(x_axis, y_axis1, label='state-0')\n",
    "plt.plot(x_axis, y_axis2, label='state-1')\n",
    "plt.plot(x_axis, y_axis3, label='state-2')\n",
    "plt.plot(x_axis, y_axis4, label='state-3')\n",
    "plt.plot(x_axis, y_axis5, label='state-4')\n",
    "plt.plot(x_axis, y_axis6, label='state-5')\n",
    "plt.plot(x_axis, y_axis7, label='state-6')\n",
    "plt.plot(x_axis, y_axis8, label='state-7')\n",
    "plt.plot(x_axis, y_axis9, label='state-8')\n",
    "plt.plot(x_axis, y_axis10, label='state-9')\n",
    "plt.plot(x_axis, y_axis11, label='state-10')\n",
    "plt.plot(x_axis, y_axis12, label='state-11')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('Value vs. Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.25, 0.5))\n",
    "\n",
    "plt.axhline(y = -0.223, color = 'blue', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.107, color = 'orange', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.90, color = 'green', linestyle = 'dashed')\n",
    "plt.axhline(y = 0, color = 'red', linestyle = 'dashed')\n",
    "plt.axhline(y = -0.234, color = 'purple', linestyle = 'dashed')\n",
    "plt.axhline(y = 0, color = 'brown', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.318, color = 'pink', linestyle = 'dashed')\n",
    "plt.axhline(y = 0, color = 'gray', linestyle = 'dashed')\n",
    "plt.axhline(y = -0.267, color = 'lime', linestyle = 'dashed')\n",
    "plt.axhline(y = -0.256, color = 'cyan', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.140, color = 'blue', linestyle = 'dashed')\n",
    "plt.axhline(y = -0.226, color = 'orange', linestyle = 'dashed')\n",
    "#plt.xscale('log')\n",
    "\n",
    "#plt.savefig(\"qn316.png\", format=\"png\", dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d61da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal and Log plot for n step Temporal Difference \n",
    "# For getting log plot activate the plt.xscale('log') comment\n",
    "\n",
    "arr = np.zeros((500,12))\n",
    "              \n",
    "for i in range(20): \n",
    "    environment = make('RMaze')\n",
    "    observation = environment.reset(seed=i)\n",
    "    arr = arr + n_stepTemporalDifference( ev , optimal_policy , 12 , 500 )\n",
    "arr = arr/20\n",
    "y_axis1=arr[:,0]\n",
    "y_axis2=arr[:,1]\n",
    "y_axis3=arr[:,2]\n",
    "y_axis4=arr[:,3]\n",
    "y_axis5=arr[:,4]\n",
    "y_axis6=arr[:,5]\n",
    "y_axis7=arr[:,6]\n",
    "y_axis8=arr[:,7]\n",
    "y_axis9=arr[:,8]\n",
    "y_axis10=arr[:,9]\n",
    "y_axis11=arr[:,10]\n",
    "y_axis12=arr[:,11]\n",
    "y_axis73=y_axis7\n",
    "x_axis = np.linspace(1,500,500)    \n",
    "\n",
    "# Plot the data\n",
    "plt.plot(x_axis, y_axis1, label='state-0')\n",
    "plt.plot(x_axis, y_axis2, label='state-1')\n",
    "plt.plot(x_axis, y_axis3, label='state-2')\n",
    "plt.plot(x_axis, y_axis4, label='state-3')\n",
    "plt.plot(x_axis, y_axis5, label='state-4')\n",
    "plt.plot(x_axis, y_axis6, label='state-5')\n",
    "plt.plot(x_axis, y_axis7, label='state-6')\n",
    "plt.plot(x_axis, y_axis8, label='state-7')\n",
    "plt.plot(x_axis, y_axis9, label='state-8')\n",
    "plt.plot(x_axis, y_axis10, label='state-9')\n",
    "plt.plot(x_axis, y_axis11, label='state-10')\n",
    "plt.plot(x_axis, y_axis12, label='state-11')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('Value vs. Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.25, 0.5))\n",
    "\n",
    "plt.axhline(y = -0.223, color = 'blue', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.107, color = 'orange', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.90, color = 'green', linestyle = 'dashed')\n",
    "plt.axhline(y = 0, color = 'red', linestyle = 'dashed')\n",
    "plt.axhline(y = -0.234, color = 'purple', linestyle = 'dashed')\n",
    "plt.axhline(y = 0, color = 'brown', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.318, color = 'pink', linestyle = 'dashed')\n",
    "plt.axhline(y = 0, color = 'gray', linestyle = 'dashed')\n",
    "plt.axhline(y = -0.267, color = 'lime', linestyle = 'dashed')\n",
    "plt.axhline(y = -0.256, color = 'cyan', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.140, color = 'blue', linestyle = 'dashed')\n",
    "plt.axhline(y = -0.226, color = 'orange', linestyle = 'dashed')\n",
    "#plt.xscale('log')\n",
    "\n",
    "#plt.savefig(\"qn311.png\", format=\"png\", dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ecaa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal and Log plot for Temporal Difference lambda prediction\n",
    "# For getting log plot activate the plt.xscale('log') comment\n",
    "\n",
    "arr = np.zeros((500,12))\n",
    "              \n",
    "for i in range(20): \n",
    "    environment = make('RMaze')\n",
    "    observation = environment.reset(seed=i)\n",
    "    arr = arr + TDLambdaPrediction( ev , optimal_policy , 500 , 0.1 )    \n",
    "arr = arr/20\n",
    "y_axis1=arr[:,0]\n",
    "y_axis2=arr[:,1]\n",
    "y_axis3=arr[:,2]\n",
    "y_axis4=arr[:,3]\n",
    "y_axis5=arr[:,4]\n",
    "y_axis6=arr[:,5]\n",
    "y_axis7=arr[:,6]\n",
    "y_axis8=arr[:,7]\n",
    "y_axis9=arr[:,8]\n",
    "y_axis10=arr[:,9]\n",
    "y_axis11=arr[:,10]\n",
    "y_axis12=arr[:,11]\n",
    "y_axis74=y_axis7\n",
    "x_axis = np.linspace(1,500,500)    \n",
    "\n",
    "# Plot the data\n",
    "plt.plot(x_axis, y_axis1, label='state-0')\n",
    "plt.plot(x_axis, y_axis2, label='state-1')\n",
    "plt.plot(x_axis, y_axis3, label='state-2')\n",
    "plt.plot(x_axis, y_axis4, label='state-3')\n",
    "plt.plot(x_axis, y_axis5, label='state-4')\n",
    "plt.plot(x_axis, y_axis6, label='state-5')\n",
    "plt.plot(x_axis, y_axis7, label='state-6')\n",
    "plt.plot(x_axis, y_axis8, label='state-7')\n",
    "plt.plot(x_axis, y_axis9, label='state-8')\n",
    "plt.plot(x_axis, y_axis10, label='state-9')\n",
    "plt.plot(x_axis, y_axis11, label='state-10')\n",
    "plt.plot(x_axis, y_axis12, label='state-11')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('Value vs. Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.25, 0.5))\n",
    "\n",
    "plt.axhline(y = -0.223, color = 'blue', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.107, color = 'orange', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.90, color = 'green', linestyle = 'dashed')\n",
    "plt.axhline(y = 0, color = 'red', linestyle = 'dashed')\n",
    "plt.axhline(y = -0.234, color = 'purple', linestyle = 'dashed')\n",
    "plt.axhline(y = 0, color = 'brown', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.318, color = 'pink', linestyle = 'dashed')\n",
    "plt.axhline(y = 0, color = 'gray', linestyle = 'dashed')\n",
    "plt.axhline(y = -0.267, color = 'lime', linestyle = 'dashed')\n",
    "plt.axhline(y = -0.256, color = 'cyan', linestyle = 'dashed')\n",
    "plt.axhline(y = 0.140, color = 'blue', linestyle = 'dashed')\n",
    "plt.axhline(y = -0.226, color = 'orange', linestyle = 'dashed')\n",
    "#plt.xscale('log')\n",
    "\n",
    "#plt.savefig(\"qn318.png\", format=\"png\", dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#G_Target MC EV and FV\n",
    "\n",
    "environment = make('RMaze' )\n",
    "observation = environment.reset(seed=0)\n",
    "arr = MonteCarloPrediction( environment,  100,500,optimal_policy ,firstVisit=False)\n",
    "\n",
    "y_axis=arr\n",
    "\n",
    "\n",
    "x_axis = np.linspace(1,500,500)    \n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(x_axis, y_axis, label='state')\n",
    "\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('MCEV Target vs. Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('MCEV')\n",
    "\n",
    "\n",
    "#plt.xscale('log')\n",
    "\n",
    "plt.savefig(\"qn221.png\", format=\"png\", dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b02bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = make('RMaze' )\n",
    "observation = environment.reset(seed=0)\n",
    "arr = TemporalDifference( environment , 500 , 0.5 , optimal_policy)\n",
    "\n",
    "y_axis=arr\n",
    "\n",
    "\n",
    "x_axis = np.linspace(1,500,500)    \n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(x_axis, y_axis, label='state')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('TD Target vs. Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('TD error')\n",
    "\n",
    "#plt.xscale('log')\n",
    "\n",
    "plt.savefig(\"qn322.png\", format=\"png\", dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = make('RMaze' )\n",
    "observation = ev.reset(seed=0)\n",
    "n,arr =TDLambdaPrediction( ev , optimal_policy , 500 , 0.1 )   \n",
    "arr= np.array(arr)\n",
    "\n",
    "y_axis1=arr[:,0]\n",
    "y_axis2=arr[:,1]\n",
    "y_axis3=arr[:,2]\n",
    "y_axis4=arr[:,3]\n",
    "y_axis5=arr[:,4]\n",
    "y_axis6=arr[:,5]\n",
    "y_axis7=arr[:,6]\n",
    "y_axis8=arr[:,7]\n",
    "y_axis9=arr[:,8]\n",
    "y_axis10=arr[:,9]\n",
    "y_axis11=arr[:,10]\n",
    "y_axis12=arr[:,11]\n",
    "\n",
    "\n",
    "\n",
    "x_axis = np.linspace(1,n,n)    \n",
    "\n",
    "# Plot the data\n",
    "plt.plot(x_axis, y_axis1, label='state-0')\n",
    "plt.plot(x_axis, y_axis2, label='state-1')\n",
    "plt.plot(x_axis, y_axis3, label='state-2')\n",
    "plt.plot(x_axis, y_axis4, label='state-3')\n",
    "plt.plot(x_axis, y_axis5, label='state-4')\n",
    "plt.plot(x_axis, y_axis6, label='state-5')\n",
    "plt.plot(x_axis, y_axis7, label='state-6')\n",
    "plt.plot(x_axis, y_axis8, label='state-7')\n",
    "plt.plot(x_axis, y_axis9, label='state-8')\n",
    "plt.plot(x_axis, y_axis10, label='state-9')\n",
    "plt.plot(x_axis, y_axis11, label='state-10')\n",
    "plt.plot(x_axis, y_axis12, label='state-11')\n",
    "\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('E vs. Time Step')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('E')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.25, 0.5))\n",
    "#plt.xscale('log')\n",
    "\n",
    "plt.savefig(\"qn313.png\", format=\"png\", dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(1,500,500)    \n",
    "\n",
    "\n",
    "\n",
    "plt.plot(x_axis, y_axis71, label='MC')\n",
    "plt.plot(x_axis, y_axis72, label='TD')\n",
    "plt.plot(x_axis, y_axis73, label='n-TD')\n",
    "plt.plot(x_axis, y_axis74, label='TD-l')\n",
    "\n",
    "\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "#plt.savefig(\"qn319.png\", format=\"png\", dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65f4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
