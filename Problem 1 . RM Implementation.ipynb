{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b21cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all useful functions \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces, register, make\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76a19c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomMaze(Env):\n",
    "    \n",
    "    \n",
    "    #----- 1 -----\n",
    "    #constructor for initialization and some helper functions\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #P is basically State: Action: [ Transition Probability , Next state , Reward , isTerminated?]\n",
    "        # for actions : 0 -> up 1-> right 2->down 3-> left (clockwise)\n",
    "        self.P = {\n",
    "            0: {\n",
    "                0: [(0.8, 0, -0.04, False),(0.1, 1, -0.04, False),(0.1, 0, -0.04, False)],\n",
    "                1: [(0.8, 1, -0.04, False),(0.1, 0, -0.04, False),(0.1, 4, -0.04, False)],\n",
    "                2: [(0.8, 4, -0.04, False),(0.1, 1, -0.04, False),(0.1, 0, -0.04, False)],\n",
    "                3: [(0.8, 0, -0.04, False),(0.1, 0, -0.04, False),(0.1, 4, -0.04, False)]\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.8, 1, -0.04, False),(0.1, 2, -0.04, False),(0.1, 0, -0.04, False)],\n",
    "                1: [(0.8, 2, -0.04, False),(0.1, 1, -0.04, False),(0.1, 6, -0.04, False)],\n",
    "                2: [(0.8, 1, -0.04, False),(0.1, 2, -0.04, False),(0.1, 0, -0.04, False)],\n",
    "                3: [(0.8, 0, -0.04, False),(0.1, 1, -0.04, False),(0.1, 1, -0.04, False)]\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.8, 2, -0.04, False),(0.1, 1, -0.04, False),(0.1, 3,  1.00, True)],\n",
    "                1: [(0.8, 3,  1.00, True ),(0.1, 2, -0.04, False),(0.1, 6, -0.04, False)],\n",
    "                2: [(0.8, 6, -0.04, False),(0.1, 1, -0.04, False),(0.1, 3,  1.00, True)],\n",
    "                3: [(0.8, 1, -0.04, False),(0.1, 2, -0.04, False),(0.1, 6, -0.04, False)]\n",
    "            },\n",
    "            3: {\n",
    "                0: [(1.0, 3, 0.00, True)],         # Goal\n",
    "                1: [(1.0, 3, 0.00, True)],\n",
    "                2: [(1.0, 3, 0.00, True)],\n",
    "                3: [(1.0, 3, 0.00, True)]\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.8, 0, -0.04, False),(0.1, 4, -0.04, False),(0.1, 4, -0.04, False)],\n",
    "                1: [(0.8, 4, -0.04, False),(0.1, 0, -0.04, False),(0.1, 8, -0.04, False)],\n",
    "                2: [(0.8, 8, -0.04, False),(0.1, 4, -0.04, False),(0.1, 4, -0.04, False)],\n",
    "                3: [(0.8, 4, -0.04, False),(0.1, 0, -0.04, False),(0.1, 8, -0.04, False)]\n",
    "            },\n",
    "            5: {\n",
    "                0: [(1.0, 5, 0.00, True)],\n",
    "                1: [(1.0, 5, 0.00, True)],         # wall\n",
    "                2: [(1.0, 5, 0.00, True)],\n",
    "                3: [(1.0, 5, 0.00, True)]\n",
    "            },\n",
    "            6: {\n",
    "                0: [(0.8, 2, -0.04, False),(0.1, 6, -0.04, False),(0.1, 7, -1.00,  True)],\n",
    "                1: [(0.8, 7, -1.00, True ),(0.1, 2, -0.04, False),(0.1, 10, -0.04, False)],\n",
    "                2: [(0.8, 10, -0.04,False),(0.1, 6, -0.04, False),(0.1, 7, -1.00,  True)],\n",
    "                3: [(0.8, 6, -0.04, False),(0.1, 2, -0.04, False),(0.1, 10, -0.04, False)]\n",
    "            },\n",
    "            7: {\n",
    "                0: [(1.0, 7, 0.00, True)],\n",
    "                1: [(1.0, 7, 0.00, True)],        # hole\n",
    "                2: [(1.0, 7, 0.00, True)],\n",
    "                3: [(1.0, 7, 0.00, True)]\n",
    "            },\n",
    "            8: {\n",
    "                0: [(0.8, 4, -0.04, False),(0.1, 9, -0.04, False),(0.1, 8, -0.04, False)],\n",
    "                1: [(0.8, 9, -0.04, False),(0.1, 4, -0.04, False),(0.1, 8, -0.04, False)],\n",
    "                2: [(0.8, 8, -0.04, False),(0.1, 8, -0.04, False),(0.1, 9, -0.04, False)],\n",
    "                3: [(0.8, 8, -0.04, False),(0.1, 4, -0.04, False),(0.1, 8, -0.04, False)]\n",
    "            },\n",
    "            9: {\n",
    "                0: [(0.8, 9, -0.04, False),(0.1, 10, -0.04, False),(0.1, 8, -0.04, False)],\n",
    "                1: [(0.8, 10, -0.04, False),(0.1, 9, -0.04, False),(0.1, 9, -0.04, False)],\n",
    "                2: [(0.8, 9, -0.04, False),(0.1, 8, -0.04, False),(0.1, 10, -0.04, False)],\n",
    "                3: [(0.8, 8, -0.04, False),(0.1, 9, -0.04, False),(0.1, 9, -0.04, False)]\n",
    "            },\n",
    "            10: {\n",
    "                0: [(0.8, 6, -0.04, False),(0.1, 9, -0.04, False),(0.1, 11, -0.04, False)],\n",
    "                1: [(0.8, 11, -0.04, False),(0.1, 6, -0.04, False),(0.1, 10, -0.04, False)],\n",
    "                2: [(0.8, 10, -0.04, False),(0.1, 11, -0.04, False),(0.1, 9, -0.04, False)],\n",
    "                3: [(0.8, 9, -0.04, False),(0.1, 6, -0.04, False),(0.1, 10, -0.04, False)]\n",
    "            },\n",
    "            11: {\n",
    "                0: [(0.8, 7, -1.00, True ),(0.1, 10, -0.04, False),(0.1, 11, -0.04, False)],\n",
    "                1: [(0.8, 11, -0.04, False),(0.1, 11, -0.04, False),(0.1, 7, -1.00,  True)],\n",
    "                2: [(0.8, 11, -0.04, False),(0.1, 10, -0.04, False),(0.1, 11, -0.04, False)],\n",
    "                3: [(0.8, 10, -0.04,False),(0.1, 11, -0.04, False),(0.1, 7, -1.00,  True)]\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        self.size = 12 # The size of the grid\n",
    "        #self.window_size = 512  # The size of the PyGame window\n",
    "        \n",
    "        # We have 3 observations, corresponding to each position in the 1-D grid\n",
    "        self.observation_space = spaces.Discrete(self.size)\n",
    "\n",
    "        # We have 2 actions, corresponding to \"left\" & \"right\"\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.action_space_size_ = 4 \n",
    "        \n",
    "    \n",
    "    #return the locations of agent and target\n",
    "    def _get_obs(self):\n",
    "        return {   \n",
    "            \"agent\" : self._agent_location, \n",
    "            \"target\": self._target_location  \n",
    "        }\n",
    "    \n",
    "    #returns the distance between agent and target \n",
    "    def _get_info(self):\n",
    "        return {  \n",
    "            \"distance\": abs(self._agent_location - self._target_location)   \n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #----- 2 ------\n",
    "    # The reset function to initiate \n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        self._agent_location = 8             #location of agent in the begining\n",
    "        self._target_location = 3            #location of target  \n",
    "        self._dead_state = 7                 #dead location\n",
    "        \n",
    "        \n",
    "        observation = self._get_obs()        #getting useful information\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation,info\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #------- 3 ---------\n",
    "    # The step function \n",
    "    \n",
    "    def step(self, action):  # takes action as a parameter\n",
    "\n",
    "        # gets the current location and stores the values from P set \n",
    "        prev_location = self._agent_location                                #gets location\n",
    "        transitions = self.P[prev_location][action]                         #gets the corresponding action tuple\n",
    "        probabilities, next_states, rewards, terminals = zip(*transitions)  #stores the value for use \n",
    "        \n",
    "        # Randomly select a transition based on the probabilities\n",
    "        # gives you random state based on your probabilities \n",
    "        index = random.choices(range(len(probabilities)), weights=probabilities, k=1)[0]\n",
    "        # stores the values \n",
    "        self._agent_location, reward, terminated = next_states[index], rewards[index], terminals[index]\n",
    "        \n",
    "        truncated = False\n",
    "        observation = self._get_obs()  \n",
    "        info = self._get_info()\n",
    "\n",
    "        info[\"log\"] = {\"current_state\": prev_location, \n",
    "                       \"action\":action,  \n",
    "                        \"next_state\": self._agent_location}\n",
    "\n",
    "        # Return the required 5-tuple\n",
    "        return observation, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be0a3c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kislayadityaoj/opt/anaconda3/lib/python3.9/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment RMaze already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# Register the custom environment\n",
    "register(id='RMaze', entry_point=RandomMaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7db47a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'current_state': 8, 'action': 1, 'next_state': 9}\n",
      "{'current_state': 9, 'action': 0, 'next_state': 9}\n",
      "{'current_state': 9, 'action': 3, 'next_state': 8}\n",
      "{'current_state': 8, 'action': 2, 'next_state': 8}\n",
      "{'current_state': 8, 'action': 1, 'next_state': 9}\n",
      "{'current_state': 9, 'action': 2, 'next_state': 9}\n",
      "{'current_state': 9, 'action': 1, 'next_state': 10}\n",
      "{'current_state': 10, 'action': 3, 'next_state': 9}\n",
      "{'current_state': 9, 'action': 2, 'next_state': 9}\n",
      "{'current_state': 9, 'action': 3, 'next_state': 8}\n",
      " Average Reward over 10 episode =  -0.04\n"
     ]
    }
   ],
   "source": [
    "# Create and use the environment\n",
    "environment = make('RMaze')\n",
    "\n",
    "reward_sum = 0\n",
    "observation = environment.reset(seed=0)\n",
    "for _ in range(10):\n",
    "    action = environment.action_space.sample()  # this is where you would insert your policy\n",
    "    observation, reward, terminated, truncated, info = environment.step(action)\n",
    "    reward_sum += reward\n",
    "    print(info['log'])\n",
    "\n",
    "    if terminated:\n",
    "        print(\"Terminated\", \"\\n\")\n",
    "\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation = environment.reset(seed=0)\n",
    "        \n",
    "        \n",
    "print(\" Average Reward over 10 episode = \" , (reward_sum * 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c52b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
