### Topics covered in this module-
1. Components of Reinforcement learning and Environment dynamics.
2. Custom Environment creation using OpenAI gymnasium API.
3. Registering and Running of Environment.
4. Analysis and code of Grid walk Environments ( Bandit-Walk and Random-Walk Enironments. )
5. Theroies of methods based on Markov's decision process and Bellman's Equation.
   
6. Algorithms to explore an environment - 
    <pre>
     * Greedy Approach ( Pure Exploitation )
     * Pure Exploration
     * Epsilon Greedy Approach
     * Decaying Epsilon Approach
     * Softmax Exploration Strategy
     * UCB Strategy
    </pre> 
7. Probabilistic  Prediction methods -
     <pre>
      * Monte-Carlo FV
      * Monte-Carlo EV
      * Temporal Difference 
      * n - Step TD
      * TD Lambda 
     </pre>
8. Control Algorithms  -
     <pre>
      * Monte-Carlo FV Control
      * Monte-Carlo EV Control
      * SARSA
      * Q- Learning
      * Double Q-Learning 
      * SARSA lambDa ( Accumulating and Replacing Traces )
      * Q lambDA ( Accumulating and Replacing Traces )
      * DYNA - Q ( Model Based )
      * Trajectory Sampling ( Model Based )
     </pre>     
9. Deep Reinforcement Learning  -
     <pre>
      * Neural-Fitted Q ( NFQ )
      * DQN , DDQN , D3QN
      * PER-D3QN
      * Reinforce
      * VPG
      * DDPG
      * TD3
      * PPO
     </pre>     
9. Plots and reports based on above methods.

### Notes-
1. The assignment contains all the implementation and plots. 
2. It has two solutions - the one which I wrote and the one provided by tutors.
3. All the codes in this repo may or may not be correct. Please verify once before using.
